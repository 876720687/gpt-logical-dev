#!/usr/bin/env python
# coding: utf-8

# # åŸºäºæœ¬åœ°çŸ¥è¯†çš„ ChatGLM åº”ç”¨å®ç°  
# ## ç›¸å…³é“¾æ¥  
# åŸé¡¹ç›®Githubï¼šhttps://github.com/imClumsyPanda/langchain-ChatGLM  
# 
# ChatGLM-6B åœ¨ ModelWhale å¹³å°çš„éƒ¨ç½²ä¸å¾®è°ƒæ•™ç¨‹ï¼šhttps://www.heywhale.com/mw/project/6436d82948f7da1fee2be59e  
# 
# ## ModelWhale è¿è¡Œé…ç½®  
# è®¡ç®—èµ„æºï¼šV100 Tensor Core GPU  
# é•œåƒï¼šPython3.9 Cuda11.6 Torch1.12.1 å®˜æ–¹é•œåƒ  
# 
# ## ä»‹ç»  
# 
# ğŸ¤–ï¸ ä¸€ç§åˆ©ç”¨ [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) + [langchain](https://github.com/hwchase17/langchain) å®ç°çš„åŸºäºæœ¬åœ°çŸ¥è¯†çš„ ChatGLM åº”ç”¨ã€‚  
# 
# ğŸ’¡ å— [GanymedeNil](https://github.com/GanymedeNil) çš„é¡¹ç›® [document.ai](https://github.com/GanymedeNil/document.ai) å’Œ [AlexZhangji](https://github.com/AlexZhangji) åˆ›å»ºçš„ [ChatGLM-6B Pull Request](https://github.com/THUDM/ChatGLM-6B/pull/216) å¯å‘ï¼Œå»ºç«‹äº†å…¨éƒ¨åŸºäºå¼€æºæ¨¡å‹å®ç°çš„æœ¬åœ°çŸ¥è¯†é—®ç­”åº”ç”¨ã€‚  
# 
# âœ… æœ¬é¡¹ç›®ä¸­ Embedding é€‰ç”¨çš„æ˜¯ [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese/tree/main)ï¼ŒLLM é€‰ç”¨çš„æ˜¯ [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)ã€‚ä¾æ‰˜ä¸Šè¿°æ¨¡å‹ï¼Œæœ¬é¡¹ç›®å¯å®ç°å…¨éƒ¨ä½¿ç”¨**å¼€æº**æ¨¡å‹**ç¦»çº¿ç§æœ‰éƒ¨ç½²**ã€‚  
# 
# â›“ï¸ æœ¬é¡¹ç›®å®ç°åŸç†å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¿‡ç¨‹åŒ…æ‹¬åŠ è½½æ–‡ä»¶ -> è¯»å–æ–‡æœ¬ -> æ–‡æœ¬åˆ†å‰² -> æ–‡æœ¬å‘é‡åŒ– -> é—®å¥å‘é‡åŒ– -> åœ¨æ–‡æœ¬å‘é‡ä¸­åŒ¹é…å‡ºä¸é—®å¥å‘é‡æœ€ç›¸ä¼¼çš„`top k`ä¸ª -> åŒ¹é…å‡ºçš„æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡å’Œé—®é¢˜ä¸€èµ·æ·»åŠ åˆ°`prompt`ä¸­ -> æäº¤ç»™`LLM`ç”Ÿæˆå›ç­”ã€‚  
# 
# ![å®ç°åŸç†å›¾](https://raw.githubusercontent.com/imClumsyPanda/langchain-ChatGLM/master/img/langchain%2Bchatglm.png)  
# 
# ğŸš© æœ¬é¡¹ç›®æœªæ¶‰åŠå¾®è°ƒã€è®­ç»ƒè¿‡ç¨‹ï¼Œä½†å¯åˆ©ç”¨å¾®è°ƒæˆ–è®­ç»ƒå¯¹æœ¬é¡¹ç›®æ•ˆæœè¿›è¡Œä¼˜åŒ–ã€‚  
# 
# ## æœ¬é¡¹ç›®ä½¿ç”¨æ–¹å¼  
# æ ¸å¿ƒéƒ¨åˆ†ä»£ç ä¸º  
# ```python  
# # æ‰§è¡Œåˆå§‹åŒ–  
# init_cfg(LLM_MODEL, EMBEDDING_MODEL, LLM_HISTORY_LEN)  
# # ä½¿ç”¨ ChatGLM çš„ readme è¿›è¡Œæµ‹è¯•  
# vector_store = init_knowledge_vector_store("/home/mw/project/test_chatglm_readme.md")  
# ```
# 
# å…¶ä¸­vector_storeçš„åˆå§‹åŒ–å¯ä»¥ä¼ é€’ txtã€docxã€md æ ¼å¼æ–‡ä»¶ï¼Œæˆ–è€…åŒ…å«mdæ–‡ä»¶çš„ç›®å½•ã€‚  
# 
# æ›´å¤šçŸ¥è¯†åº“åŠ è½½æ–¹å¼å¯ä»¥å‚è€ƒ[langchainæ–‡æ¡£](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/unstructured_file.html)ï¼Œ  
# é€šè¿‡ä¿®æ”¹ init_knowledge_vector_store æ–¹æ³•è¿›è¡Œå…¼å®¹ã€‚  
# 
# **å®˜æ–¹æ³¨**ï¼šModelWhale GPUæœºå‹éœ€è¦ä»äº‘å‚å•†æ‹‰å–ç®—åŠ›èµ„æºï¼Œè€—æ—¶5~10minï¼Œä¸”ä¼šé¢„æ‰£åŠå°æ—¶èµ„æºä»·æ ¼çš„é²¸å¸ã€‚å¦‚æœèµ„æºæœªå¯åŠ¨æˆåŠŸï¼Œé¢„æ‰£è´¹ç”¨ä¼šåœ¨å…³é—­ç¼–ç¨‹é¡µé¢åäº”åˆ†é’Ÿå†…é€€å›ï¼Œæ— éœ€ç´§å¼ ï¼Œå¦‚é‡é—®é¢˜æ¬¢è¿[ææŠ¥å·¥å•](https://www.heywhale.com/home/user/workorder)ï¼Œå®¢æœä¼šåŠæ—¶å¤„ç†ã€‚  
# 
# ## æ›´æ–°æ—¥å¿—  
# [2023/05/11]  
# 1. ä¿®å¤unstructuredç‰ˆæœ¬æ›´æ–°å¯¼è‡´çš„PDFè¯»å–é—®é¢˜  
# 
# [2023/05/06]  
# 1. æ›´æ–° requirementsã€‚è§£å†³ langchain ç‰ˆæœ¬å¯¼è‡´çš„ä¾èµ–å†²çª  
# 2. åŒæ­¥ä¸Šæ¸¸ä»£ç ç»“æ„ï¼Œç®€åŒ–æ¨¡å—è°ƒç”¨  
# 3. response æ”¹ä¸ºæµå¼è¾“å‡º  
# 4. çœŸå®æ¡ˆä¾‹çŸ¥è¯†åº“æ”¹ä¸ºPDFæ ¼å¼  
# 
# [2023/04/28]  
# 1. å¢åŠ ä¸€ä¸ªã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹ä¹¦ç±å¯¹è¯botçš„çœŸå®æ¡ˆä¾‹  
# 2. å¢åŠ  ChineseTextSplitter æ¥é¿å…å¬å›å†…å®¹è¿‡é•¿å¯¼è‡´çš„æ˜¾å­˜æº¢å‡ºé—®é¢˜  



# # ä½¿ç”¨ Markdown æ ¼å¼æ‰“å°æ¨¡å‹è¾“å‡º
# from IPython.display import display, Markdown, clear_output

def display_answer(agent, query, vs_path, history=[]):
    for resp, history in local_doc_qa.get_knowledge_based_answer(query=query,
                                                                 vs_path=vs_path,
                                                                 chat_history=history,
                                                                 streaming=True):
        clear_output(wait=True)
        display(Markdown(resp["result"]))
    return resp, history


# In[5]:


import torch.backends

from pkg.configs import model_config

# å…¨å±€å‚æ•°ï¼Œä¿®æ”¹åè¯·é‡æ–°åˆå§‹åŒ–
model_config.embedding_model_dict = {
    "ernie-tiny": "nghuyong/ernie-3.0-nano-zh",
    "ernie-base": "nghuyong/ernie-3.0-base-zh",
    "text2vec-base": "shibing624/text2vec-base-chinese",
    "text2vec": "/home/mw/input/text2vec2538",
}
model_config.llm_model_dict = {
    "chatyuan": "ClueAI/ChatYuan-large-v2",
    "chatglm-6b-int4-qe": "THUDM/chatglm-6b-int4-qe",
    "chatglm-6b-int4": "THUDM/chatglm-6b-int4",
    "chatglm-6b-int8": "THUDM/chatglm-6b-int8",
    "chatglm-6b": "/home/mw/input/ChatGLM6B6449",
}
model_config.VS_ROOT_PATH = "/home/mw/temp"

# from chains.local_doc_qa import LocalDocQA

EMBEDDING_MODEL = "text2vec" # embedding æ¨¡å‹ï¼Œå¯¹åº” embedding_model_dict
VECTOR_SEARCH_TOP_K = 6
LLM_MODEL = "chatglm-6b"     # LLM æ¨¡å‹åï¼Œå¯¹åº” llm_model_dict
LLM_HISTORY_LEN = 3
DEVICE = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"

local_doc_qa = LocalDocQA()

local_doc_qa.init_cfg(llm_model=LLM_MODEL,
                          embedding_model=EMBEDDING_MODEL,
                          llm_history_len=LLM_HISTORY_LEN,
                          top_k=VECTOR_SEARCH_TOP_K)




# ä½¿ç”¨ ChatGLM çš„ readme è¿›è¡Œæµ‹è¯•
vs_path, _ = local_doc_qa.init_knowledge_vector_store("/home/mw/project/test_chatglm_readme.md")
vs_path


# æµ‹è¯•æœªè¿›è¡Œæœ¬åœ°çŸ¥è¯†åº“æ¥å…¥æ—¶çš„ç»“æœ
# from IPython.display import display, Markdown, clear_output
for resp, history in local_doc_qa.llm._call("chatglm-6b çš„å±€é™æ€§å…·ä½“ä½“ç°åœ¨å“ªé‡Œï¼Œå¦‚ä½•å®ç°æ”¹è¿›"):
    clear_output(wait=True)
    display(Markdown(resp))



# æ¥å…¥çŸ¥è¯†åº“ååŒä¸€é—®é¢˜çš„è¿”å›ç»“æœ
history = []
display_answer(local_doc_qa, query="chatglm-6b çš„å±€é™æ€§å…·ä½“ä½“ç°åœ¨å“ªé‡Œï¼Œå¦‚ä½•å®ç°æ”¹è¿›", vs_path=vs_path, history=history)

