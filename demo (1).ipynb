{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D89B0EAB542642F98900CCC4D4E49360",
    "jupyter": {},
    "notebookId": "644b7c4749a32aea7f079534",
    "runtime": {
     "execution_status": null,
     "status": "default"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 初始化\n",
    "PDF位置：/home/mw/project/d2l-zh-pytorch-2.0.0.pdf  \n",
    "将其转化为词库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/autodl-tmp/langchain-ChatGLM'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "id": "AE2092CF28EB43D095027B2D69564EBC",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "644b7c4749a32aea7f079534",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cd /home/mw/project/langchain-ChatGLM\n",
    "# cd /root/autodl-tmp/langchain-ChatGLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "id": "810CF482643D4985AFA558B9F60AF78D",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "644b7c4749a32aea7f079534",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 安装依赖\n",
    "# !pip install -r requirements.txt -i https://mirror.sjtu.edu.cn/pypi/web/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "id": "4FC61B519BE24DD590C9913A15FBDA95",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "644b7c4749a32aea7f079534",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 安装nltk_data\n",
    "# 已有的情况下需要给出目录\n",
    "# !cp -r nltk_data /root/autodl-tmp/langchain-ChatGLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "id": "6314A856D10344E68E800ABBF2DBEB5F",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "644b7c4749a32aea7f079534",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用 Markdown 格式打印模型输出\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "def display_answer(agent, query, vs_path, history=[]):\n",
    "    for resp, history in local_doc_qa.get_knowledge_based_answer(query=query,\n",
    "                                                                 vs_path=vs_path,\n",
    "                                                                 chat_history=history,\n",
    "                                                                 streaming=True):\n",
    "        clear_output(wait=True)\n",
    "        # display(Markdown(resp[\"result\"]))\n",
    "    return resp, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda\n",
    "import torch.backends\n",
    "\n",
    "from configs import model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0B5EAEFEB72241DD870851D2738FE624",
    "notebookId": "644b7c4749a32aea7f079534",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 全局参数，修改后请重新初始化\n",
    "model_config.embedding_model_dict = {\n",
    "    \"ernie-tiny\": \"nghuyong/ernie-3.0-nano-zh\",\n",
    "    \"ernie-base\": \"nghuyong/ernie-3.0-base-zh\",\n",
    "    \"text2vec-base\": \"shibing624/text2vec-base-chinese\",\n",
    "    \"text2vec\": \"/root/autodl-tmp/text2vec-large-chinese\", # /home/mw/input/text2vec2538\n",
    "}\n",
    "model_config.llm_model_dict = {\n",
    "    \"chatyuan\": \"ClueAI/ChatYuan-large-v2\",\n",
    "    \"chatglm-6b-int4-qe\": \"THUDM/chatglm-6b-int4-qe\",\n",
    "    \"chatglm-6b-int4\": \"THUDM/chatglm-6b-int4\",\n",
    "    \"chatglm-6b-int8\": \"THUDM/chatglm-6b-int8\",\n",
    "    \"chatglm-6b\": \"/root/autodl-tmp/model/chatglm-6b\", # /home/mw/input/ChatGLM6B6449\n",
    "}\n",
    "\n",
    "# /root/autodl-tmp/temp\n",
    "model_config.VS_ROOT_PATH = \"/root/autodl-tmp/temp\" #/home/mw/temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0B5EAEFEB72241DD870851D2738FE624",
    "notebookId": "644b7c4749a32aea7f079534",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "2023-06-12 11:24:17.915818: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03098917007446289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 8,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcadb88e384c4b70b555896212127d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /root/autodl-tmp/text2vec-large-chinese. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "from chains.local_doc_qa import LocalDocQA\n",
    "\n",
    "EMBEDDING_MODEL = \"text2vec\" # embedding 模型，对应 embedding_model_dict\n",
    "VECTOR_SEARCH_TOP_K = 6\n",
    "LLM_MODEL = \"chatglm-6b\"     # LLM 模型名，对应 llm_model_dict\n",
    "LLM_HISTORY_LEN = 3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "local_doc_qa = LocalDocQA()\n",
    "\n",
    "local_doc_qa.init_cfg(llm_model=LLM_MODEL,\n",
    "                          embedding_model=EMBEDDING_MODEL,\n",
    "                          llm_history_len=LLM_HISTORY_LEN,\n",
    "                          top_k=VECTOR_SEARCH_TOP_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建本地文件库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "id": "37118B9733CA425881F79E7C0423A9C9",
    "jupyter": {
     "outputs_hidden": true
    },
    "notebookId": "644b7c4749a32aea7f079534",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570.pdf 已成功加载\n",
      "216.pdf 已成功加载\n",
      "202.pdf 已成功加载\n",
      "564.pdf 已成功加载\n",
      "558.pdf 已成功加载\n",
      "1027.pdf 已成功加载\n",
      "1033.pdf 已成功加载\n",
      "772.pdf 已成功加载\n",
      "766.pdf 已成功加载\n",
      "996.pdf 已成功加载\n",
      "982.pdf 已成功加载\n",
      "969.pdf 已成功加载\n",
      "1225.pdf 已成功加载\n",
      "1231.pdf 已成功加载\n",
      "955.pdf 已成功加载\n",
      "1219.pdf 已成功加载\n",
      "941.pdf 已成功加载\n",
      "799.pdf 已成功加载\n",
      "160.pdf 已成功加载\n",
      "1392.pdf 已成功加载\n",
      "606.pdf 已成功加载\n",
      "612.pdf 已成功加载\n",
      "1386.pdf 已成功加载\n",
      "174.pdf 已成功加载\n",
      "148.pdf 已成功加载\n",
      "1437.pdf 已成功加载\n",
      "1351.pdf 已成功加载\n",
      "1345.pdf 已成功加载\n",
      "49.pdf 已成功加载\n",
      "809.pdf 已成功加载\n",
      "1423.pdf 已成功加载\n",
      "61.pdf 已成功加载\n",
      "821.pdf 已成功加载\n",
      "1379.pdf 已成功加载\n",
      "75.pdf 已成功加载\n",
      "835.pdf 已成功加载\n",
      "404.pdf 已成功加载\n",
      "1190.pdf 已成功加载\n",
      "362.pdf 已成功加载\n",
      "376.pdf 已成功加载\n",
      "1184.pdf 已成功加载\n",
      "410.pdf 已成功加载\n",
      "438.pdf 已成功加载\n",
      "1153.pdf 已成功加载\n",
      "1147.pdf 已成功加载\n",
      "389.pdf 已成功加载\n",
      "388.pdf 已成功加载\n",
      "1146.pdf 已成功加载\n",
      "1152.pdf 已成功加载\n",
      "439.pdf 已成功加载\n",
      "377.pdf 已成功加载\n",
      "411.pdf 已成功加载\n",
      "1185.pdf 已成功加载\n",
      "1191.pdf 已成功加载\n",
      "405.pdf 已成功加载\n",
      "363.pdf 已成功加载\n",
      "834.pdf 已成功加载\n",
      "74.pdf 已成功加载\n",
      "1378.pdf 已成功加载\n",
      "820.pdf 已成功加载\n",
      "60.pdf 已成功加载\n",
      "808.pdf 已成功加载\n",
      "48.pdf 已成功加载\n",
      "1344.pdf 已成功加载\n",
      "1422.pdf 已成功加载\n",
      "1436.pdf 已成功加载\n",
      "1350.pdf 已成功加载\n",
      "149.pdf 已成功加载\n",
      "1387.pdf 已成功加载\n",
      "613.pdf 已成功加载\n",
      "175.pdf 已成功加载\n",
      "161.pdf 已成功加载\n",
      "607.pdf 已成功加载\n",
      "1393.pdf 已成功加载\n",
      "798.pdf 已成功加载\n",
      "940.pdf 已成功加载\n",
      "1218.pdf 已成功加载\n",
      "954.pdf 已成功加载\n",
      "1230.pdf 已成功加载\n",
      "1224.pdf 已成功加载\n",
      "968.pdf 已成功加载\n",
      "983.pdf 已成功加载\n",
      "997.pdf 已成功加载\n",
      "767.pdf 已成功加载\n",
      "773.pdf 已成功加载\n",
      "1032.pdf 已成功加载\n",
      "1026.pdf 已成功加载\n",
      "559.pdf 已成功加载\n",
      "203.pdf 已成功加载\n",
      "565.pdf 已成功加载\n",
      "571.pdf 已成功加载\n",
      "217.pdf 已成功加载\n",
      "567.pdf 已成功加载\n",
      "201.pdf 已成功加载\n",
      "215.pdf 已成功加载\n",
      "573.pdf 已成功加载\n",
      "229.pdf 已成功加载\n",
      "1030.pdf 已成功加载\n",
      "1024.pdf 已成功加载\n",
      "598.pdf 已成功加载\n",
      "1018.pdf 已成功加载\n",
      "765.pdf 已成功加载\n",
      "771.pdf 已成功加载\n",
      "759.pdf 已成功加载\n",
      "981.pdf 已成功加载\n",
      "995.pdf 已成功加载\n",
      "1232.pdf 已成功加载\n",
      "1226.pdf 已成功加载\n",
      "942.pdf 已成功加载\n",
      "956.pdf 已成功加载\n",
      "177.pdf 已成功加载\n",
      "611.pdf 已成功加载\n",
      "1385.pdf 已成功加载\n",
      "89.pdf 已成功加载\n",
      "1391.pdf 已成功加载\n",
      "605.pdf 已成功加载\n",
      "163.pdf 已成功加载\n",
      "639.pdf 已成功加载\n",
      "1420.pdf 已成功加载\n",
      "1346.pdf 已成功加载\n",
      "1352.pdf 已成功加载\n",
      "1434.pdf 已成功加载\n",
      "188.pdf 已成功加载\n",
      "76.pdf 已成功加载\n",
      "836.pdf 已成功加载\n",
      "62.pdf 已成功加载\n",
      "822.pdf 已成功加载\n",
      "1408.pdf 已成功加载\n",
      "1187.pdf 已成功加载\n",
      "413.pdf 已成功加载\n",
      "375.pdf 已成功加载\n",
      "361.pdf 已成功加载\n",
      "407.pdf 已成功加载\n",
      "1193.pdf 已成功加载\n",
      "349.pdf 已成功加载\n",
      "1144.pdf 已成功加载\n",
      "1150.pdf 已成功加载\n",
      "1178.pdf 已成功加载\n",
      "1179.pdf 已成功加载\n",
      "1151.pdf 已成功加载\n",
      "1145.pdf 已成功加载\n",
      "348.pdf 已成功加载\n",
      "360.pdf 已成功加载\n",
      "1192.pdf 已成功加载\n",
      "406.pdf 已成功加载\n",
      "412.pdf 已成功加载\n",
      "1186.pdf 已成功加载\n",
      "374.pdf 已成功加载\n",
      "823.pdf 已成功加载\n",
      "63.pdf 已成功加载\n",
      "1409.pdf 已成功加载\n",
      "189.pdf 已成功加载\n",
      "837.pdf 已成功加载\n",
      "77.pdf 已成功加载\n",
      "1353.pdf 已成功加载\n",
      "1435.pdf 已成功加载\n",
      "1421.pdf 已成功加载\n",
      "1347.pdf 已成功加载\n",
      "638.pdf 已成功加载\n",
      "604.pdf 已成功加载\n",
      "1390.pdf 已成功加载\n",
      "162.pdf 已成功加载\n",
      "176.pdf 已成功加载\n",
      "88.pdf 已成功加载\n",
      "1384.pdf 已成功加载\n",
      "610.pdf 已成功加载\n",
      "957.pdf 已成功加载\n",
      "943.pdf 已成功加载\n",
      "1227.pdf 已成功加载\n",
      "1233.pdf 已成功加载\n",
      "994.pdf 已成功加载\n",
      "980.pdf 已成功加载\n",
      "758.pdf 已成功加载\n",
      "770.pdf 已成功加载\n",
      "764.pdf 已成功加载\n",
      "1019.pdf 已成功加载\n",
      "599.pdf 已成功加载\n",
      "1025.pdf 已成功加载\n",
      "1031.pdf 已成功加载\n",
      "228.pdf 已成功加载\n",
      "214.pdf 已成功加载\n",
      "572.pdf 已成功加载\n",
      "566.pdf 已成功加载\n",
      "200.pdf 已成功加载\n",
      "238.pdf 已成功加载\n",
      "204.pdf 已成功加载\n",
      "562.pdf 已成功加载\n",
      "576.pdf 已成功加载\n",
      "210.pdf 已成功加载\n",
      "1009.pdf 已成功加载\n",
      "589.pdf 已成功加载\n",
      "1035.pdf 已成功加载\n",
      "1021.pdf 已成功加载\n",
      "984.pdf 已成功加载\n",
      "748.pdf 已成功加载\n",
      "990.pdf 已成功加载\n",
      "760.pdf 已成功加载\n",
      "774.pdf 已成功加载\n",
      "947.pdf 已成功加载\n",
      "953.pdf 已成功加载\n",
      "1237.pdf 已成功加载\n",
      "1223.pdf 已成功加载\n",
      "628.pdf 已成功加载\n",
      "614.pdf 已成功加载\n",
      "1380.pdf 已成功加载\n",
      "172.pdf 已成功加载\n",
      "166.pdf 已成功加载\n",
      "98.pdf 已成功加载\n",
      "1394.pdf 已成功加载\n",
      "600.pdf 已成功加载\n",
      "73.pdf 已成功加载\n",
      "833.pdf 已成功加载\n",
      "1419.pdf 已成功加载\n",
      "199.pdf 已成功加载\n",
      "67.pdf 已成功加载\n",
      "827.pdf 已成功加载\n",
      "1343.pdf 已成功加载\n",
      "1425.pdf 已成功加载\n",
      "1431.pdf 已成功加载\n",
      "9.pdf 已成功加载\n",
      "1357.pdf 已成功加载\n",
      "358.pdf 已成功加载\n",
      "370.pdf 已成功加载\n",
      "1182.pdf 已成功加载\n",
      "416.pdf 已成功加载\n",
      "402.pdf 已成功加载\n",
      "1196.pdf 已成功加载\n",
      "364.pdf 已成功加载\n",
      "1169.pdf 已成功加载\n",
      "1141.pdf 已成功加载\n",
      "1155.pdf 已成功加载\n",
      "1154.pdf 已成功加载\n",
      "1140.pdf 已成功加载\n",
      "1168.pdf 已成功加载\n",
      "1197.pdf 已成功加载\n",
      "403.pdf 已成功加载\n",
      "365.pdf 已成功加载\n",
      "371.pdf 已成功加载\n",
      "417.pdf 已成功加载\n",
      "1183.pdf 已成功加载\n",
      "359.pdf 已成功加载\n",
      "8.pdf 已成功加载\n",
      "1430.pdf 已成功加载\n",
      "1356.pdf 已成功加载\n",
      "1342.pdf 已成功加载\n",
      "1424.pdf 已成功加载\n",
      "198.pdf 已成功加载\n",
      "826.pdf 已成功加载\n",
      "66.pdf 已成功加载\n",
      "832.pdf 已成功加载\n",
      "72.pdf 已成功加载\n",
      "1418.pdf 已成功加载\n",
      "167.pdf 已成功加载\n",
      "601.pdf 已成功加载\n",
      "1395.pdf 已成功加载\n",
      "99.pdf 已成功加载\n",
      "1381.pdf 已成功加载\n",
      "615.pdf 已成功加载\n",
      "173.pdf 已成功加载\n",
      "629.pdf 已成功加载\n",
      "1222.pdf 已成功加载\n",
      "1236.pdf 已成功加载\n",
      "952.pdf 已成功加载\n",
      "946.pdf 已成功加载\n",
      "775.pdf 已成功加载\n",
      "761.pdf 已成功加载\n",
      "991.pdf 已成功加载\n",
      "749.pdf 已成功加载\n",
      "985.pdf 已成功加载\n",
      "1020.pdf 已成功加载\n",
      "1034.pdf 已成功加载\n",
      "588.pdf 已成功加载\n",
      "1008.pdf 已成功加载\n",
      "577.pdf 已成功加载\n",
      "211.pdf 已成功加载\n",
      "205.pdf 已成功加载\n",
      "563.pdf 已成功加载\n",
      "239.pdf 已成功加载\n",
      "549.pdf 已成功加载\n",
      "213.pdf 已成功加载\n",
      "575.pdf 已成功加载\n",
      "561.pdf 已成功加载\n",
      "207.pdf 已成功加载\n",
      "1022.pdf 已成功加载\n",
      "1036.pdf 已成功加载\n",
      "993.pdf 已成功加载\n",
      "987.pdf 已成功加载\n",
      "777.pdf 已成功加载\n",
      "763.pdf 已成功加载\n",
      "950.pdf 已成功加载\n",
      "788.pdf 已成功加载\n",
      "944.pdf 已成功加载\n",
      "1208.pdf 已成功加载\n",
      "1220.pdf 已成功加载\n",
      "978.pdf 已成功加载\n",
      "1234.pdf 已成功加载\n",
      "159.pdf 已成功加载\n",
      "1397.pdf 已成功加载\n",
      "603.pdf 已成功加载\n",
      "165.pdf 已成功加载\n",
      "171.pdf 已成功加载\n",
      "617.pdf 已成功加载\n",
      "1383.pdf 已成功加载\n",
      "64.pdf 已成功加载\n",
      "1368.pdf 已成功加载\n",
      "824.pdf 已成功加载\n",
      "70.pdf 已成功加载\n",
      "830.pdf 已成功加载\n",
      "58.pdf 已成功加载\n",
      "1354.pdf 已成功加载\n",
      "818.pdf 已成功加载\n",
      "1432.pdf 已成功加载\n",
      "1426.pdf 已成功加载\n",
      "1340.pdf 已成功加载\n",
      "429.pdf 已成功加载\n",
      "367.pdf 已成功加载\n",
      "401.pdf 已成功加载\n",
      "1195.pdf 已成功加载\n",
      "1181.pdf 已成功加载\n",
      "415.pdf 已成功加载\n",
      "373.pdf 已成功加载\n",
      "398.pdf 已成功加载\n",
      "1156.pdf 已成功加载\n",
      "1142.pdf 已成功加载\n",
      "1143.pdf 已成功加载\n",
      "1157.pdf 已成功加载\n",
      "399.pdf 已成功加载\n",
      "414.pdf 已成功加载\n",
      "1180.pdf 已成功加载\n",
      "372.pdf 已成功加载\n",
      "366.pdf 已成功加载\n",
      "1194.pdf 已成功加载\n",
      "400.pdf 已成功加载\n",
      "428.pdf 已成功加载\n",
      "1427.pdf 已成功加载\n",
      "1341.pdf 已成功加载\n",
      "819.pdf 已成功加载\n",
      "1355.pdf 已成功加载\n",
      "59.pdf 已成功加载\n",
      "1433.pdf 已成功加载\n",
      "831.pdf 已成功加载\n",
      "71.pdf 已成功加载\n",
      "825.pdf 已成功加载\n",
      "1369.pdf 已成功加载\n",
      "65.pdf 已成功加载\n",
      "170.pdf 已成功加载\n",
      "1382.pdf 已成功加载\n",
      "616.pdf 已成功加载\n",
      "602.pdf 已成功加载\n",
      "1396.pdf 已成功加载\n",
      "164.pdf 已成功加载\n",
      "158.pdf 已成功加载\n",
      "1235.pdf 已成功加载\n",
      "979.pdf 已成功加载\n",
      "1221.pdf 已成功加载\n",
      "1209.pdf 已成功加载\n",
      "945.pdf 已成功加载\n",
      "789.pdf 已成功加载\n",
      "951.pdf 已成功加载\n",
      "762.pdf 已成功加载\n",
      "776.pdf 已成功加载\n",
      "986.pdf 已成功加载\n",
      "992.pdf 已成功加载\n",
      "1037.pdf 已成功加载\n",
      "1023.pdf 已成功加载\n",
      "560.pdf 已成功加载\n",
      "206.pdf 已成功加载\n",
      "212.pdf 已成功加载\n",
      "574.pdf 已成功加载\n",
      "548.pdf 已成功加载\n",
      "275.pdf 已成功加载\n",
      "513.pdf 已成功加载\n",
      "1087.pdf 已成功加载\n",
      "1093.pdf 已成功加载\n",
      "507.pdf 已成功加载\n",
      "261.pdf 已成功加载\n",
      "249.pdf 已成功加载\n",
      "1044.pdf 已成功加载\n",
      "1050.pdf 已成功加载\n",
      "1078.pdf 已成功加载\n",
      "1285.pdf 已成功加载\n",
      "711.pdf 已成功加载\n",
      "705.pdf 已成功加载\n",
      "1291.pdf 已成功加载\n",
      "739.pdf 已成功加载\n",
      "1246.pdf 已成功加载\n",
      "1252.pdf 已成功加载\n",
      "936.pdf 已成功加载\n",
      "922.pdf 已成功加载\n",
      "665.pdf 已成功加载\n",
      "103.pdf 已成功加载\n",
      "117.pdf 已成功加载\n",
      "671.pdf 已成功加载\n",
      "881.pdf 已成功加载\n",
      "659.pdf 已成功加载\n",
      "895.pdf 已成功加载\n",
      "1332.pdf 已成功加载\n",
      "1326.pdf 已成功加载\n",
      "842.pdf 已成功加载\n",
      "16.pdf 已成功加载\n",
      "856.pdf 已成功加载\n",
      "301.pdf 已成功加载\n",
      "467.pdf 已成功加载\n",
      "473.pdf 已成功加载\n",
      "315.pdf 已成功加载\n",
      "329.pdf 已成功加载\n",
      "1130.pdf 已成功加载\n",
      "1124.pdf 已成功加载\n",
      "498.pdf 已成功加载\n",
      "1118.pdf 已成功加载\n",
      "1119.pdf 已成功加载\n",
      "499.pdf 已成功加载\n",
      "1125.pdf 已成功加载\n",
      "1131.pdf 已成功加载\n",
      "328.pdf 已成功加载\n",
      "472.pdf 已成功加载\n",
      "314.pdf 已成功加载\n",
      "300.pdf 已成功加载\n",
      "466.pdf 已成功加载\n",
      "857.pdf 已成功加载\n",
      "17.pdf 已成功加载\n",
      "843.pdf 已成功加载\n",
      "1327.pdf 已成功加载\n",
      "1333.pdf 已成功加载\n",
      "894.pdf 已成功加载\n",
      "658.pdf 已成功加载\n",
      "880.pdf 已成功加载\n",
      "116.pdf 已成功加载\n",
      "670.pdf 已成功加载\n",
      "664.pdf 已成功加载\n",
      "102.pdf 已成功加载\n",
      "923.pdf 已成功加载\n",
      "937.pdf 已成功加载\n",
      "1253.pdf 已成功加载\n",
      "1247.pdf 已成功加载\n",
      "738.pdf 已成功加载\n",
      "1290.pdf 已成功加载\n",
      "704.pdf 已成功加载\n",
      "710.pdf 已成功加载\n",
      "1284.pdf 已成功加载\n",
      "1079.pdf 已成功加载\n",
      "1051.pdf 已成功加载\n",
      "1045.pdf 已成功加载\n",
      "248.pdf 已成功加载\n",
      "506.pdf 已成功加载\n",
      "1092.pdf 已成功加载\n",
      "260.pdf 已成功加载\n",
      "274.pdf 已成功加载\n",
      "1086.pdf 已成功加载\n",
      "512.pdf 已成功加载\n",
      "262.pdf 已成功加载\n",
      "1090.pdf 已成功加载\n",
      "504.pdf 已成功加载\n",
      "510.pdf 已成功加载\n",
      "1084.pdf 已成功加载\n",
      "276.pdf 已成功加载\n",
      "538.pdf 已成功加载\n",
      "1053.pdf 已成功加载\n",
      "1047.pdf 已成功加载\n",
      "289.pdf 已成功加载\n",
      "706.pdf 已成功加载\n",
      "1292.pdf 已成功加载\n",
      "1286.pdf 已成功加载\n",
      "712.pdf 已成功加载\n",
      "1251.pdf 已成功加载\n",
      "909.pdf 已成功加载\n",
      "1245.pdf 已成功加载\n",
      "921.pdf 已成功加载\n",
      "935.pdf 已成功加载\n",
      "1279.pdf 已成功加载\n",
      "672.pdf 已成功加载\n",
      "114.pdf 已成功加载\n",
      "100.pdf 已成功加载\n",
      "666.pdf 已成功加载\n",
      "896.pdf 已成功加载\n",
      "128.pdf 已成功加载\n",
      "882.pdf 已成功加载\n",
      "1325.pdf 已成功加载\n",
      "29.pdf 已成功加载\n",
      "869.pdf 已成功加载\n",
      "1331.pdf 已成功加载\n",
      "1319.pdf 已成功加载\n",
      "15.pdf 已成功加载\n",
      "855.pdf 已成功加载\n",
      "699.pdf 已成功加载\n",
      "841.pdf 已成功加载\n",
      "316.pdf 已成功加载\n",
      "470.pdf 已成功加载\n",
      "464.pdf 已成功加载\n",
      "302.pdf 已成功加载\n",
      "458.pdf 已成功加载\n",
      "1127.pdf 已成功加载\n",
      "1133.pdf 已成功加载\n",
      "1132.pdf 已成功加载\n",
      "1126.pdf 已成功加载\n",
      "459.pdf 已成功加载\n",
      "465.pdf 已成功加载\n",
      "303.pdf 已成功加载\n",
      "317.pdf 已成功加载\n",
      "471.pdf 已成功加载\n",
      "840.pdf 已成功加载\n",
      "698.pdf 已成功加载\n",
      "854.pdf 已成功加载\n",
      "14.pdf 已成功加载\n",
      "1318.pdf 已成功加载\n",
      "1330.pdf 已成功加载\n",
      "868.pdf 已成功加载\n",
      "28.pdf 已成功加载\n",
      "1324.pdf 已成功加载\n",
      "883.pdf 已成功加载\n",
      "897.pdf 已成功加载\n",
      "129.pdf 已成功加载\n",
      "101.pdf 已成功加载\n",
      "667.pdf 已成功加载\n",
      "673.pdf 已成功加载\n",
      "115.pdf 已成功加载\n",
      "1278.pdf 已成功加载\n",
      "934.pdf 已成功加载\n",
      "920.pdf 已成功加载\n",
      "1244.pdf 已成功加载\n",
      "908.pdf 已成功加载\n",
      "1250.pdf 已成功加载\n",
      "713.pdf 已成功加载\n",
      "1287.pdf 已成功加载\n",
      "1293.pdf 已成功加载\n",
      "707.pdf 已成功加载\n",
      "288.pdf 已成功加载\n",
      "1046.pdf 已成功加载\n",
      "1052.pdf 已成功加载\n",
      "539.pdf 已成功加载\n",
      "1085.pdf 已成功加载\n",
      "511.pdf 已成功加载\n",
      "277.pdf 已成功加载\n",
      "263.pdf 已成功加载\n",
      "505.pdf 已成功加载\n",
      "1091.pdf 已成功加载\n",
      "529.pdf 已成功加载\n",
      "1095.pdf 已成功加载\n",
      "501.pdf 已成功加载\n",
      "267.pdf 已成功加载\n",
      "273.pdf 已成功加载\n",
      "515.pdf 已成功加载\n",
      "1081.pdf 已成功加载\n",
      "298.pdf 已成功加载\n",
      "1056.pdf 已成功加载\n",
      "1042.pdf 已成功加载\n",
      "703.pdf 已成功加载\n",
      "1297.pdf 已成功加载\n",
      "1283.pdf 已成功加载\n",
      "717.pdf 已成功加载\n",
      "924.pdf 已成功加载\n",
      "1268.pdf 已成功加载\n",
      "930.pdf 已成功加载\n",
      "918.pdf 已成功加载\n",
      "1254.pdf 已成功加载\n",
      "1240.pdf 已成功加载\n",
      "893.pdf 已成功加载\n",
      "887.pdf 已成功加载\n",
      "139.pdf 已成功加载\n",
      "111.pdf 已成功加载\n",
      "677.pdf 已成功加载\n",
      "663.pdf 已成功加载\n",
      "105.pdf 已成功加载\n",
      "10.pdf 已成功加载\n",
      "688.pdf 已成功加载\n",
      "850.pdf 已成功加载\n",
      "1308.pdf 已成功加载\n",
      "844.pdf 已成功加载\n",
      "1320.pdf 已成功加载\n",
      "38.pdf 已成功加载\n",
      "1334.pdf 已成功加载\n",
      "878.pdf 已成功加载\n",
      "449.pdf 已成功加载\n",
      "475.pdf 已成功加载\n",
      "313.pdf 已成功加载\n",
      "307.pdf 已成功加载\n",
      "461.pdf 已成功加载\n",
      "1122.pdf 已成功加载\n",
      "1136.pdf 已成功加载\n",
      "1137.pdf 已成功加载\n",
      "1123.pdf 已成功加载\n",
      "306.pdf 已成功加载\n",
      "460.pdf 已成功加载\n",
      "474.pdf 已成功加载\n",
      "312.pdf 已成功加载\n",
      "448.pdf 已成功加载\n",
      "879.pdf 已成功加载\n",
      "1335.pdf 已成功加载\n",
      "39.pdf 已成功加载\n",
      "1321.pdf 已成功加载\n",
      "845.pdf 已成功加载\n",
      "1309.pdf 已成功加载\n",
      "851.pdf 已成功加载\n",
      "689.pdf 已成功加载\n",
      "11.pdf 已成功加载\n",
      "662.pdf 已成功加载\n",
      "104.pdf 已成功加载\n",
      "110.pdf 已成功加载\n",
      "676.pdf 已成功加载\n",
      "886.pdf 已成功加载\n",
      "138.pdf 已成功加载\n",
      "892.pdf 已成功加载\n",
      "1241.pdf 已成功加载\n",
      "1255.pdf 已成功加载\n",
      "919.pdf 已成功加载\n",
      "931.pdf 已成功加载\n",
      "1269.pdf 已成功加载\n",
      "925.pdf 已成功加载\n",
      "716.pdf 已成功加载\n",
      "1282.pdf 已成功加载\n",
      "1296.pdf 已成功加载\n",
      "702.pdf 已成功加载\n",
      "1043.pdf 已成功加载\n",
      "1057.pdf 已成功加载\n",
      "299.pdf 已成功加载\n",
      "272.pdf 已成功加载\n",
      "1080.pdf 已成功加载\n",
      "514.pdf 已成功加载\n",
      "500.pdf 已成功加载\n",
      "1094.pdf 已成功加载\n",
      "266.pdf 已成功加载\n",
      "528.pdf 已成功加载\n",
      "258.pdf 已成功加载\n",
      "516.pdf 已成功加载\n",
      "1082.pdf 已成功加载\n",
      "270.pdf 已成功加载\n",
      "264.pdf 已成功加载\n",
      "1096.pdf 已成功加载\n",
      "502.pdf 已成功加载\n",
      "1069.pdf 已成功加载\n",
      "1041.pdf 已成功加载\n",
      "1055.pdf 已成功加载\n",
      "728.pdf 已成功加载\n",
      "1280.pdf 已成功加载\n",
      "714.pdf 已成功加载\n",
      "700.pdf 已成功加载\n",
      "1294.pdf 已成功加载\n",
      "933.pdf 已成功加载\n",
      "927.pdf 已成功加载\n",
      "1243.pdf 已成功加载\n",
      "1257.pdf 已成功加载\n",
      "884.pdf 已成功加载\n",
      "890.pdf 已成功加载\n",
      "648.pdf 已成功加载\n",
      "106.pdf 已成功加载\n",
      "660.pdf 已成功加载\n",
      "674.pdf 已成功加载\n",
      "112.pdf 已成功加载\n",
      "847.pdf 已成功加载\n",
      "13.pdf 已成功加载\n",
      "853.pdf 已成功加载\n",
      "1337.pdf 已成功加载\n",
      "1323.pdf 已成功加载\n",
      "338.pdf 已成功加载\n",
      "462.pdf 已成功加载\n",
      "304.pdf 已成功加载\n",
      "310.pdf 已成功加载\n",
      "476.pdf 已成功加载\n",
      "1109.pdf 已成功加载\n",
      "489.pdf 已成功加载\n",
      "1135.pdf 已成功加载\n",
      "1121.pdf 已成功加载\n",
      "1120.pdf 已成功加载\n",
      "1134.pdf 已成功加载\n",
      "488.pdf 已成功加载\n",
      "1108.pdf 已成功加载\n",
      "311.pdf 已成功加载\n",
      "477.pdf 已成功加载\n",
      "463.pdf 已成功加载\n",
      "305.pdf 已成功加载\n",
      "339.pdf 已成功加载\n",
      "1322.pdf 已成功加载\n",
      "1336.pdf 已成功加载\n",
      "852.pdf 已成功加载\n",
      "12.pdf 已成功加载\n",
      "846.pdf 已成功加载\n",
      "675.pdf 已成功加载\n",
      "113.pdf 已成功加载\n",
      "107.pdf 已成功加载\n",
      "661.pdf 已成功加载\n",
      "649.pdf 已成功加载\n",
      "891.pdf 已成功加载\n",
      "885.pdf 已成功加载\n",
      "1256.pdf 已成功加载\n",
      "1242.pdf 已成功加载\n",
      "926.pdf 已成功加载\n",
      "932.pdf 已成功加载\n",
      "1295.pdf 已成功加载\n",
      "701.pdf 已成功加载\n",
      "715.pdf 已成功加载\n",
      "1281.pdf 已成功加载\n",
      "729.pdf 已成功加载\n",
      "1054.pdf 已成功加载\n",
      "1040.pdf 已成功加载\n",
      "1068.pdf 已成功加载\n",
      "265.pdf 已成功加载\n",
      "503.pdf 已成功加载\n",
      "1097.pdf 已成功加载\n",
      "1083.pdf 已成功加载\n",
      "517.pdf 已成功加载\n",
      "271.pdf 已成功加载\n",
      "259.pdf 已成功加载\n",
      "254.pdf 已成功加载\n",
      "532.pdf 已成功加载\n",
      "526.pdf 已成功加载\n",
      "240.pdf 已成功加载\n",
      "268.pdf 已成功加载\n",
      "297.pdf 已成功加载\n",
      "1065.pdf 已成功加载\n",
      "1071.pdf 已成功加载\n",
      "283.pdf 已成功加载\n",
      "1059.pdf 已成功加载\n",
      "730.pdf 已成功加载\n",
      "724.pdf 已成功加载\n",
      "1298.pdf 已成功加载\n",
      "718.pdf 已成功加载\n",
      "1267.pdf 已成功加载\n",
      "1273.pdf 已成功加载\n",
      "917.pdf 已成功加载\n",
      "903.pdf 已成功加载\n",
      "644.pdf 已成功加载\n",
      "122.pdf 已成功加载\n",
      "136.pdf 已成功加载\n",
      "888.pdf 已成功加载\n",
      "650.pdf 已成功加载\n",
      "678.pdf 已成功加载\n",
      "687.pdf 已成功加载\n",
      "1313.pdf 已成功加载\n",
      "1307.pdf 已成功加载\n",
      "693.pdf 已成功加载\n",
      "23.pdf 已成功加载\n",
      "863.pdf 已成功加载\n",
      "37.pdf 已成功加载\n",
      "877.pdf 已成功加载\n",
      "320.pdf 已成功加载\n",
      "446.pdf 已成功加载\n",
      "452.pdf 已成功加载\n",
      "334.pdf 已成功加载\n",
      "308.pdf 已成功加载\n",
      "1111.pdf 已成功加载\n",
      "485.pdf 已成功加载\n",
      "491.pdf 已成功加载\n",
      "1105.pdf 已成功加载\n",
      "1139.pdf 已成功加载\n",
      "1138.pdf 已成功加载\n",
      "1104.pdf 已成功加载\n",
      "490.pdf 已成功加载\n",
      "484.pdf 已成功加载\n",
      "1110.pdf 已成功加载\n",
      "309.pdf 已成功加载\n",
      "453.pdf 已成功加载\n",
      "335.pdf 已成功加载\n",
      "321.pdf 已成功加载\n",
      "447.pdf 已成功加载\n",
      "876.pdf 已成功加载\n",
      "36.pdf 已成功加载\n",
      "862.pdf 已成功加载\n",
      "22.pdf 已成功加载\n",
      "692.pdf 已成功加载\n",
      "1306.pdf 已成功加载\n",
      "1312.pdf 已成功加载\n",
      "686.pdf 已成功加载\n",
      "679.pdf 已成功加载\n",
      "137.pdf 已成功加载\n",
      "651.pdf 已成功加载\n",
      "889.pdf 已成功加载\n",
      "645.pdf 已成功加载\n",
      "123.pdf 已成功加载\n",
      "902.pdf 已成功加载\n",
      "916.pdf 已成功加载\n",
      "1272.pdf 已成功加载\n",
      "1266.pdf 已成功加载\n",
      "719.pdf 已成功加载\n",
      "1299.pdf 已成功加载\n",
      "725.pdf 已成功加载\n",
      "731.pdf 已成功加载\n",
      "1058.pdf 已成功加载\n",
      "1070.pdf 已成功加载\n",
      "282.pdf 已成功加载\n",
      "296.pdf 已成功加载\n",
      "1064.pdf 已成功加载\n",
      "269.pdf 已成功加载\n",
      "527.pdf 已成功加载\n",
      "241.pdf 已成功加载\n",
      "255.pdf 已成功加载\n",
      "533.pdf 已成功加载\n",
      "243.pdf 已成功加载\n",
      "525.pdf 已成功加载\n",
      "531.pdf 已成功加载\n",
      "257.pdf 已成功加载\n",
      "519.pdf 已成功加载\n",
      "1099.pdf 已成功加载\n",
      "280.pdf 已成功加载\n",
      "1072.pdf 已成功加载\n",
      "1066.pdf 已成功加载\n",
      "294.pdf 已成功加载\n",
      "727.pdf 已成功加载\n",
      "733.pdf 已成功加载\n",
      "1270.pdf 已成功加载\n",
      "928.pdf 已成功加载\n",
      "1264.pdf 已成功加载\n",
      "900.pdf 已成功加载\n",
      "914.pdf 已成功加载\n",
      "1258.pdf 已成功加载\n",
      "653.pdf 已成功加载\n",
      "135.pdf 已成功加载\n",
      "121.pdf 已成功加载\n",
      "647.pdf 已成功加载\n",
      "109.pdf 已成功加载\n",
      "1304.pdf 已成功加载\n",
      "690.pdf 已成功加载\n",
      "848.pdf 已成功加载\n",
      "684.pdf 已成功加载\n",
      "1310.pdf 已成功加载\n",
      "1338.pdf 已成功加载\n",
      "34.pdf 已成功加载\n",
      "874.pdf 已成功加载\n",
      "20.pdf 已成功加载\n",
      "860.pdf 已成功加载\n",
      "337.pdf 已成功加载\n",
      "451.pdf 已成功加载\n",
      "445.pdf 已成功加载\n",
      "323.pdf 已成功加载\n",
      "479.pdf 已成功加载\n",
      "492.pdf 已成功加载\n",
      "1106.pdf 已成功加载\n",
      "1112.pdf 已成功加载\n",
      "486.pdf 已成功加载\n",
      "487.pdf 已成功加载\n",
      "1113.pdf 已成功加载\n",
      "1107.pdf 已成功加载\n",
      "493.pdf 已成功加载\n",
      "478.pdf 已成功加载\n",
      "444.pdf 已成功加载\n",
      "322.pdf 已成功加载\n",
      "336.pdf 已成功加载\n",
      "450.pdf 已成功加载\n",
      "861.pdf 已成功加载\n",
      "21.pdf 已成功加载\n",
      "875.pdf 已成功加载\n",
      "35.pdf 已成功加载\n",
      "1339.pdf 已成功加载\n",
      "1311.pdf 已成功加载\n",
      "685.pdf 已成功加载\n",
      "849.pdf 已成功加载\n",
      "691.pdf 已成功加载\n",
      "1305.pdf 已成功加载\n",
      "108.pdf 已成功加载\n",
      "120.pdf 已成功加载\n",
      "646.pdf 已成功加载\n",
      "652.pdf 已成功加载\n",
      "134.pdf 已成功加载\n",
      "1259.pdf 已成功加载\n",
      "915.pdf 已成功加载\n",
      "901.pdf 已成功加载\n",
      "1265.pdf 已成功加载\n",
      "929.pdf 已成功加载\n",
      "1271.pdf 已成功加载\n",
      "732.pdf 已成功加载\n",
      "726.pdf 已成功加载\n",
      "1067.pdf 已成功加载\n",
      "295.pdf 已成功加载\n",
      "281.pdf 已成功加载\n",
      "1073.pdf 已成功加载\n",
      "1098.pdf 已成功加载\n",
      "518.pdf 已成功加载\n",
      "530.pdf 已成功加载\n",
      "256.pdf 已成功加载\n",
      "242.pdf 已成功加载\n",
      "524.pdf 已成功加载\n",
      "1088.pdf 已成功加载\n",
      "508.pdf 已成功加载\n",
      "520.pdf 已成功加载\n",
      "246.pdf 已成功加载\n",
      "252.pdf 已成功加载\n",
      "534.pdf 已成功加载\n",
      "1077.pdf 已成功加载\n",
      "285.pdf 已成功加载\n",
      "291.pdf 已成功加载\n",
      "1063.pdf 已成功加载\n",
      "722.pdf 已成功加载\n",
      "736.pdf 已成功加载\n",
      "905.pdf 已成功加载\n",
      "1249.pdf 已成功加载\n",
      "911.pdf 已成功加载\n",
      "939.pdf 已成功加载\n",
      "1275.pdf 已成功加载\n",
      "1261.pdf 已成功加载\n",
      "118.pdf 已成功加载\n",
      "130.pdf 已成功加载\n",
      "656.pdf 已成功加载\n",
      "642.pdf 已成功加载\n",
      "124.pdf 已成功加载\n",
      "31.pdf 已成功加载\n",
      "871.pdf 已成功加载\n",
      "25.pdf 已成功加载\n",
      "1329.pdf 已成功加载\n",
      "865.pdf 已成功加载\n",
      "1301.pdf 已成功加载\n",
      "695.pdf 已成功加载\n",
      "19.pdf 已成功加载\n",
      "681.pdf 已成功加载\n",
      "1315.pdf 已成功加载\n",
      "859.pdf 已成功加载\n",
      "468.pdf 已成功加载\n",
      "454.pdf 已成功加载\n",
      "332.pdf 已成功加载\n",
      "326.pdf 已成功加载\n",
      "440.pdf 已成功加载\n",
      "497.pdf 已成功加载\n",
      "1103.pdf 已成功加载\n",
      "1117.pdf 已成功加载\n",
      "483.pdf 已成功加载\n",
      "482.pdf 已成功加载\n",
      "1116.pdf 已成功加载\n",
      "1102.pdf 已成功加载\n",
      "496.pdf 已成功加载\n",
      "327.pdf 已成功加载\n",
      "441.pdf 已成功加载\n",
      "455.pdf 已成功加载\n",
      "333.pdf 已成功加载\n",
      "469.pdf 已成功加载\n",
      "858.pdf 已成功加载\n",
      "1314.pdf 已成功加载\n",
      "680.pdf 已成功加载\n",
      "18.pdf 已成功加载\n",
      "694.pdf 已成功加载\n",
      "1300.pdf 已成功加载\n",
      "864.pdf 已成功加载\n",
      "1328.pdf 已成功加载\n",
      "24.pdf 已成功加载\n",
      "870.pdf 已成功加载\n",
      "30.pdf 已成功加载\n",
      "643.pdf 已成功加载\n",
      "125.pdf 已成功加载\n",
      "131.pdf 已成功加载\n",
      "657.pdf 已成功加载\n",
      "119.pdf 已成功加载\n",
      "1260.pdf 已成功加载\n",
      "1274.pdf 已成功加载\n",
      "938.pdf 已成功加载\n",
      "910.pdf 已成功加载\n",
      "1248.pdf 已成功加载\n",
      "904.pdf 已成功加载\n",
      "737.pdf 已成功加载\n",
      "723.pdf 已成功加载\n",
      "290.pdf 已成功加载\n",
      "1062.pdf 已成功加载\n",
      "1076.pdf 已成功加载\n",
      "284.pdf 已成功加载\n",
      "253.pdf 已成功加载\n",
      "535.pdf 已成功加载\n",
      "521.pdf 已成功加载\n",
      "247.pdf 已成功加载\n",
      "509.pdf 已成功加载\n",
      "1089.pdf 已成功加载\n",
      "279.pdf 已成功加载\n",
      "537.pdf 已成功加载\n",
      "251.pdf 已成功加载\n",
      "245.pdf 已成功加载\n",
      "523.pdf 已成功加载\n",
      "1048.pdf 已成功加载\n",
      "1060.pdf 已成功加载\n",
      "292.pdf 已成功加载\n",
      "286.pdf 已成功加载\n",
      "1074.pdf 已成功加载\n",
      "709.pdf 已成功加载\n",
      "1289.pdf 已成功加载\n",
      "735.pdf 已成功加载\n",
      "721.pdf 已成功加载\n",
      "912.pdf 已成功加载\n",
      "906.pdf 已成功加载\n",
      "1262.pdf 已成功加载\n",
      "1276.pdf 已成功加载\n",
      "669.pdf 已成功加载\n",
      "127.pdf 已成功加载\n",
      "899.pdf 已成功加载\n",
      "641.pdf 已成功加载\n",
      "655.pdf 已成功加载\n",
      "133.pdf 已成功加载\n",
      "26.pdf 已成功加载\n",
      "866.pdf 已成功加载\n",
      "32.pdf 已成功加载\n",
      "872.pdf 已成功加载\n",
      "682.pdf 已成功加载\n",
      "1316.pdf 已成功加载\n",
      "1302.pdf 已成功加载\n",
      "696.pdf 已成功加载\n",
      "319.pdf 已成功加载\n",
      "443.pdf 已成功加载\n",
      "325.pdf 已成功加载\n",
      "331.pdf 已成功加载\n",
      "457.pdf 已成功加载\n",
      "1128.pdf 已成功加载\n",
      "1114.pdf 已成功加载\n",
      "480.pdf 已成功加载\n",
      "494.pdf 已成功加载\n",
      "1100.pdf 已成功加载\n",
      "1101.pdf 已成功加载\n",
      "495.pdf 已成功加载\n",
      "481.pdf 已成功加载\n",
      "1115.pdf 已成功加载\n",
      "1129.pdf 已成功加载\n",
      "330.pdf 已成功加载\n",
      "456.pdf 已成功加载\n",
      "442.pdf 已成功加载\n",
      "324.pdf 已成功加载\n",
      "318.pdf 已成功加载\n",
      "697.pdf 已成功加载\n",
      "1303.pdf 已成功加载\n",
      "1317.pdf 已成功加载\n",
      "683.pdf 已成功加载\n",
      "873.pdf 已成功加载\n",
      "33.pdf 已成功加载\n",
      "867.pdf 已成功加载\n",
      "27.pdf 已成功加载\n",
      "654.pdf 已成功加载\n",
      "132.pdf 已成功加载\n",
      "126.pdf 已成功加载\n",
      "640.pdf 已成功加载\n",
      "898.pdf 已成功加载\n",
      "668.pdf 已成功加载\n",
      "1277.pdf 已成功加载\n",
      "1263.pdf 已成功加载\n",
      "907.pdf 已成功加载\n",
      "913.pdf 已成功加载\n",
      "720.pdf 已成功加载\n",
      "734.pdf 已成功加载\n",
      "1288.pdf 已成功加载\n",
      "708.pdf 已成功加载\n",
      "287.pdf 已成功加载\n",
      "1075.pdf 已成功加载\n",
      "1061.pdf 已成功加载\n",
      "293.pdf 已成功加载\n",
      "1049.pdf 已成功加载\n",
      "244.pdf 已成功加载\n",
      "522.pdf 已成功加载\n",
      "536.pdf 已成功加载\n",
      "250.pdf 已成功加载\n",
      "278.pdf 已成功加载\n",
      "551.pdf 已成功加载\n",
      "237.pdf 已成功加载\n",
      "223.pdf 已成功加载\n",
      "545.pdf 已成功加载\n",
      "579.pdf 已成功加载\n",
      "1006.pdf 已成功加载\n",
      "592.pdf 已成功加载\n",
      "586.pdf 已成功加载\n",
      "1012.pdf 已成功加载\n",
      "753.pdf 已成功加载\n",
      "747.pdf 已成功加载\n",
      "948.pdf 已成功加载\n",
      "790.pdf 已成功加载\n",
      "1204.pdf 已成功加载\n",
      "1210.pdf 已成功加载\n",
      "784.pdf 已成功加载\n",
      "974.pdf 已成功加载\n",
      "1238.pdf 已成功加载\n",
      "960.pdf 已成功加载\n",
      "141.pdf 已成功加载\n",
      "627.pdf 已成功加载\n",
      "633.pdf 已成功加载\n",
      "155.pdf 已成功加载\n",
      "83.pdf 已成功加载\n",
      "97.pdf 已成功加载\n",
      "169.pdf 已成功加载\n",
      "182.pdf 已成功加载\n",
      "1416.pdf 已成功加载\n",
      "1370.pdf 已成功加载\n",
      "1364.pdf 已成功加载\n",
      "68.pdf 已成功加载\n",
      "828.pdf 已成功加载\n",
      "1402.pdf 已成功加载\n",
      "196.pdf 已成功加载\n",
      "40.pdf 已成功加载\n",
      "800.pdf 已成功加载\n",
      "1358.pdf 已成功加载\n",
      "54.pdf 已成功加载\n",
      "814.pdf 已成功加载\n",
      "6.pdf 已成功加载\n",
      "425.pdf 已成功加载\n",
      "343.pdf 已成功加载\n",
      "357.pdf 已成功加载\n",
      "431.pdf 已成功加载\n",
      "419.pdf 已成功加载\n",
      "1199.pdf 已成功加载\n",
      "1172.pdf 已成功加载\n",
      "380.pdf 已成功加载\n",
      "394.pdf 已成功加载\n",
      "1166.pdf 已成功加载\n",
      "395.pdf 已成功加载\n",
      "1167.pdf 已成功加载\n",
      "1173.pdf 已成功加载\n",
      "381.pdf 已成功加载\n",
      "1198.pdf 已成功加载\n",
      "418.pdf 已成功加载\n",
      "356.pdf 已成功加载\n",
      "430.pdf 已成功加载\n",
      "424.pdf 已成功加载\n",
      "342.pdf 已成功加载\n",
      "815.pdf 已成功加载\n",
      "55.pdf 已成功加载\n",
      "1359.pdf 已成功加载\n",
      "7.pdf 已成功加载\n",
      "801.pdf 已成功加载\n",
      "41.pdf 已成功加载\n",
      "829.pdf 已成功加载\n",
      "69.pdf 已成功加载\n",
      "1365.pdf 已成功加载\n",
      "197.pdf 已成功加载\n",
      "1403.pdf 已成功加载\n",
      "1417.pdf 已成功加载\n",
      "183.pdf 已成功加载\n",
      "1371.pdf 已成功加载\n",
      "96.pdf 已成功加载\n",
      "168.pdf 已成功加载\n",
      "82.pdf 已成功加载\n",
      "632.pdf 已成功加载\n",
      "154.pdf 已成功加载\n",
      "140.pdf 已成功加载\n",
      "626.pdf 已成功加载\n",
      "961.pdf 已成功加载\n",
      "1239.pdf 已成功加载\n",
      "975.pdf 已成功加载\n",
      "785.pdf 已成功加载\n",
      "1211.pdf 已成功加载\n",
      "1205.pdf 已成功加载\n",
      "791.pdf 已成功加载\n",
      "949.pdf 已成功加载\n",
      "746.pdf 已成功加载\n",
      "752.pdf 已成功加载\n",
      "1013.pdf 已成功加载\n",
      "587.pdf 已成功加载\n",
      "593.pdf 已成功加载\n",
      "1007.pdf 已成功加载\n",
      "578.pdf 已成功加载\n",
      "222.pdf 已成功加载\n",
      "544.pdf 已成功加载\n",
      "550.pdf 已成功加载\n",
      "236.pdf 已成功加载\n",
      "546.pdf 已成功加载\n",
      "220.pdf 已成功加载\n",
      "234.pdf 已成功加载\n",
      "552.pdf 已成功加载\n",
      "208.pdf 已成功加载\n",
      "585.pdf 已成功加载\n",
      "1011.pdf 已成功加载\n",
      "1005.pdf 已成功加载\n",
      "591.pdf 已成功加载\n",
      "1039.pdf 已成功加载\n",
      "744.pdf 已成功加载\n",
      "750.pdf 已成功加载\n",
      "988.pdf 已成功加载\n",
      "778.pdf 已成功加载\n",
      "1213.pdf 已成功加载\n",
      "787.pdf 已成功加载\n",
      "793.pdf 已成功加载\n",
      "1207.pdf 已成功加载\n",
      "963.pdf 已成功加载\n",
      "977.pdf 已成功加载\n",
      "156.pdf 已成功加载\n",
      "630.pdf 已成功加载\n",
      "624.pdf 已成功加载\n",
      "142.pdf 已成功加载\n",
      "1398.pdf 已成功加载\n",
      "94.pdf 已成功加载\n",
      "618.pdf 已成功加载\n",
      "80.pdf 已成功加载\n",
      "1401.pdf 已成功加载\n",
      "195.pdf 已成功加载\n",
      "1367.pdf 已成功加载\n",
      "1373.pdf 已成功加载\n",
      "181.pdf 已成功加载\n",
      "1415.pdf 已成功加载\n",
      "5.pdf 已成功加载\n",
      "57.pdf 已成功加载\n",
      "817.pdf 已成功加载\n",
      "43.pdf 已成功加载\n",
      "803.pdf 已成功加载\n",
      "1429.pdf 已成功加载\n",
      "432.pdf 已成功加载\n",
      "354.pdf 已成功加载\n",
      "340.pdf 已成功加载\n",
      "426.pdf 已成功加载\n",
      "368.pdf 已成功加载\n",
      "1165.pdf 已成功加载\n",
      "397.pdf 已成功加载\n",
      "383.pdf 已成功加载\n",
      "1171.pdf 已成功加载\n",
      "1159.pdf 已成功加载\n",
      "1158.pdf 已成功加载\n",
      "382.pdf 已成功加载\n",
      "1170.pdf 已成功加载\n",
      "1164.pdf 已成功加载\n",
      "396.pdf 已成功加载\n",
      "369.pdf 已成功加载\n",
      "341.pdf 已成功加载\n",
      "427.pdf 已成功加载\n",
      "433.pdf 已成功加载\n",
      "355.pdf 已成功加载\n",
      "802.pdf 已成功加载\n",
      "42.pdf 已成功加载\n",
      "1428.pdf 已成功加载\n",
      "4.pdf 已成功加载\n",
      "816.pdf 已成功加载\n",
      "56.pdf 已成功加载\n",
      "1372.pdf 已成功加载\n",
      "1414.pdf 已成功加载\n",
      "180.pdf 已成功加载\n",
      "194.pdf 已成功加载\n",
      "1400.pdf 已成功加载\n",
      "1366.pdf 已成功加载\n",
      "81.pdf 已成功加载\n",
      "619.pdf 已成功加载\n",
      "95.pdf 已成功加载\n",
      "1399.pdf 已成功加载\n",
      "625.pdf 已成功加载\n",
      "143.pdf 已成功加载\n",
      "157.pdf 已成功加载\n",
      "631.pdf 已成功加载\n",
      "976.pdf 已成功加载\n",
      "962.pdf 已成功加载\n",
      "1206.pdf 已成功加载\n",
      "792.pdf 已成功加载\n",
      "786.pdf 已成功加载\n",
      "1212.pdf 已成功加载\n",
      "779.pdf 已成功加载\n",
      "989.pdf 已成功加载\n",
      "751.pdf 已成功加载\n",
      "745.pdf 已成功加载\n",
      "1038.pdf 已成功加载\n",
      "590.pdf 已成功加载\n",
      "1004.pdf 已成功加载\n",
      "1010.pdf 已成功加载\n",
      "584.pdf 已成功加载\n",
      "209.pdf 已成功加载\n",
      "235.pdf 已成功加载\n",
      "553.pdf 已成功加载\n",
      "547.pdf 已成功加载\n",
      "221.pdf 已成功加载\n",
      "219.pdf 已成功加载\n",
      "225.pdf 已成功加载\n",
      "543.pdf 已成功加载\n",
      "557.pdf 已成功加载\n",
      "231.pdf 已成功加载\n",
      "1028.pdf 已成功加载\n",
      "580.pdf 已成功加载\n",
      "1014.pdf 已成功加载\n",
      "1000.pdf 已成功加载\n",
      "594.pdf 已成功加载\n",
      "769.pdf 已成功加载\n",
      "741.pdf 已成功加载\n",
      "999.pdf 已成功加载\n",
      "755.pdf 已成功加载\n",
      "966.pdf 已成功加载\n",
      "972.pdf 已成功加载\n",
      "1216.pdf 已成功加载\n",
      "782.pdf 已成功加载\n",
      "796.pdf 已成功加载\n",
      "1202.pdf 已成功加载\n",
      "91.pdf 已成功加载\n",
      "609.pdf 已成功加载\n",
      "85.pdf 已成功加载\n",
      "1389.pdf 已成功加载\n",
      "635.pdf 已成功加载\n",
      "153.pdf 已成功加载\n",
      "147.pdf 已成功加载\n",
      "621.pdf 已成功加载\n",
      "52.pdf 已成功加载\n",
      "812.pdf 已成功加载\n",
      "1438.pdf 已成功加载\n",
      "46.pdf 已成功加载\n",
      "806.pdf 已成功加载\n",
      "1362.pdf 已成功加载\n",
      "1404.pdf 已成功加载\n",
      "190.pdf 已成功加载\n",
      "184.pdf 已成功加载\n",
      "1410.pdf 已成功加载\n",
      "1376.pdf 已成功加载\n",
      "379.pdf 已成功加载\n",
      "351.pdf 已成功加载\n",
      "437.pdf 已成功加载\n",
      "423.pdf 已成功加载\n",
      "345.pdf 已成功加载\n",
      "1148.pdf 已成功加载\n",
      "392.pdf 已成功加载\n",
      "1160.pdf 已成功加载\n",
      "1174.pdf 已成功加载\n",
      "386.pdf 已成功加载\n",
      "1175.pdf 已成功加载\n",
      "387.pdf 已成功加载\n",
      "393.pdf 已成功加载\n",
      "1161.pdf 已成功加载\n",
      "1149.pdf 已成功加载\n",
      "422.pdf 已成功加载\n",
      "344.pdf 已成功加载\n",
      "350.pdf 已成功加载\n",
      "436.pdf 已成功加载\n",
      "378.pdf 已成功加载\n",
      "1411.pdf 已成功加载\n",
      "185.pdf 已成功加载\n",
      "1377.pdf 已成功加载\n",
      "1363.pdf 已成功加载\n",
      "191.pdf 已成功加载\n",
      "1405.pdf 已成功加载\n",
      "807.pdf 已成功加载\n",
      "47.pdf 已成功加载\n",
      "813.pdf 已成功加载\n",
      "53.pdf 已成功加载\n",
      "1.pdf 已成功加载\n",
      "146.pdf 已成功加载\n",
      "620.pdf 已成功加载\n",
      "634.pdf 已成功加载\n",
      "152.pdf 已成功加载\n",
      "1388.pdf 已成功加载\n",
      "84.pdf 已成功加载\n",
      "608.pdf 已成功加载\n",
      "90.pdf 已成功加载\n",
      "1203.pdf 已成功加载\n",
      "797.pdf 已成功加载\n",
      "783.pdf 已成功加载\n",
      "1217.pdf 已成功加载\n",
      "973.pdf 已成功加载\n",
      "967.pdf 已成功加载\n",
      "754.pdf 已成功加载\n",
      "998.pdf 已成功加载\n",
      "740.pdf 已成功加载\n",
      "768.pdf 已成功加载\n",
      "595.pdf 已成功加载\n",
      "1001.pdf 已成功加载\n",
      "1015.pdf 已成功加载\n",
      "581.pdf 已成功加载\n",
      "1029.pdf 已成功加载\n",
      "556.pdf 已成功加载\n",
      "230.pdf 已成功加载\n",
      "224.pdf 已成功加载\n",
      "542.pdf 已成功加载\n",
      "218.pdf 已成功加载\n",
      "568.pdf 已成功加载\n",
      "232.pdf 已成功加载\n",
      "554.pdf 已成功加载\n",
      "540.pdf 已成功加载\n",
      "226.pdf 已成功加载\n",
      "1003.pdf 已成功加载\n",
      "597.pdf 已成功加载\n",
      "583.pdf 已成功加载\n",
      "1017.pdf 已成功加载\n",
      "756.pdf 已成功加载\n",
      "742.pdf 已成功加载\n",
      "971.pdf 已成功加载\n",
      "965.pdf 已成功加载\n",
      "1229.pdf 已成功加载\n",
      "795.pdf 已成功加载\n",
      "1201.pdf 已成功加载\n",
      "959.pdf 已成功加载\n",
      "1215.pdf 已成功加载\n",
      "781.pdf 已成功加载\n",
      "86.pdf 已成功加载\n",
      "178.pdf 已成功加载\n",
      "92.pdf 已成功加载\n",
      "622.pdf 已成功加载\n",
      "144.pdf 已成功加载\n",
      "150.pdf 已成功加载\n",
      "636.pdf 已成功加载\n",
      "45.pdf 已成功加载\n",
      "1349.pdf 已成功加载\n",
      "805.pdf 已成功加载\n",
      "3.pdf 已成功加载\n",
      "51.pdf 已成功加载\n",
      "811.pdf 已成功加载\n",
      "79.pdf 已成功加载\n",
      "1375.pdf 已成功加载\n",
      "839.pdf 已成功加载\n",
      "187.pdf 已成功加载\n",
      "1413.pdf 已成功加载\n",
      "1407.pdf 已成功加载\n",
      "193.pdf 已成功加载\n",
      "1361.pdf 已成功加载\n",
      "1188.pdf 已成功加载\n",
      "408.pdf 已成功加载\n",
      "346.pdf 已成功加载\n",
      "420.pdf 已成功加载\n",
      "434.pdf 已成功加载\n",
      "352.pdf 已成功加载\n",
      "385.pdf 已成功加载\n",
      "1177.pdf 已成功加载\n",
      "1163.pdf 已成功加载\n",
      "391.pdf 已成功加载\n",
      "1162.pdf 已成功加载\n",
      "390.pdf 已成功加载\n",
      "384.pdf 已成功加载\n",
      "1176.pdf 已成功加载\n",
      "435.pdf 已成功加载\n",
      "353.pdf 已成功加载\n",
      "347.pdf 已成功加载\n",
      "421.pdf 已成功加载\n",
      "409.pdf 已成功加载\n",
      "1189.pdf 已成功加载\n",
      "192.pdf 已成功加载\n",
      "1406.pdf 已成功加载\n",
      "1360.pdf 已成功加载\n",
      "838.pdf 已成功加载\n",
      "1374.pdf 已成功加载\n",
      "78.pdf 已成功加载\n",
      "1412.pdf 已成功加载\n",
      "186.pdf 已成功加载\n",
      "2.pdf 已成功加载\n",
      "810.pdf 已成功加载\n",
      "50.pdf 已成功加载\n",
      "804.pdf 已成功加载\n",
      "1348.pdf 已成功加载\n",
      "44.pdf 已成功加载\n",
      "151.pdf 已成功加载\n",
      "637.pdf 已成功加载\n",
      "623.pdf 已成功加载\n",
      "145.pdf 已成功加载\n",
      "93.pdf 已成功加载\n",
      "87.pdf 已成功加载\n",
      "179.pdf 已成功加载\n",
      "780.pdf 已成功加载\n",
      "1214.pdf 已成功加载\n",
      "958.pdf 已成功加载\n",
      "1200.pdf 已成功加载\n",
      "794.pdf 已成功加载\n",
      "1228.pdf 已成功加载\n",
      "964.pdf 已成功加载\n",
      "970.pdf 已成功加载\n",
      "743.pdf 已成功加载\n",
      "757.pdf 已成功加载\n",
      "1016.pdf 已成功加载\n",
      "582.pdf 已成功加载\n",
      "596.pdf 已成功加载\n",
      "1002.pdf 已成功加载\n",
      "541.pdf 已成功加载\n",
      "227.pdf 已成功加载\n",
      "233.pdf 已成功加载\n",
      "555.pdf 已成功加载\n",
      "569.pdf 已成功加载\n",
      "Json schema does not match the Unstructured schema\n",
      "sampled_papers.json 未能成功加载\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/root/autodl-tmp/tempsampled_papers_FAISS_20230612_150827'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vs_path, _ = local_doc_qa.init_knowledge_vector_store(\"/root/autodl-tmp/test\") # /home/mw/project/d2l-zh-pytorch-2.0.0.pdf\n",
    "\n",
    "# 构建全量模型\n",
    "vs_path, _ = local_doc_qa.init_knowledge_vector_store(\"/root/autodl-tmp/track2-问答式科研知识库\") # /home/mw/project/d2l-zh-pytorch-2.0.0.pdf\n",
    "\n",
    "vs_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取本地知识库并对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 知识库过大会导致分配空间失败"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_path  = '/root/autodl-tmp/papers_FAISS' # 完成知识库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cuda.max_split_size_mb = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "id": "BD931141B1F548C48AF461A8653E3D85",
    "jupyter": {
     "outputs_hidden": true
    },
    "notebookId": "644b7c4749a32aea7f079534",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于以下已知信息，简洁和专业的来回答用户的问题，问题是\"什么是 DenseNet 模型\"。如果无法从中得到答案，请说 \"根据已知信息无法回答该问题\" 或 \"没有提供足够的相关信息\"，不允许在答案中添加编造成分，答案请使用中文。已知内容如下: \n",
      "THEORETICAL ANALYSIS OF DEEP NEURAL NETWORKS FOR TEMPORALLY DEPENDENT OBSERVATIONS  2 2 0 2  t c O 0 2  ] L M  . t a t s [  1 v 0 3 5 1 1 . 0 1 2 2 : v i X r a  Mingliang Ma Department of Statistics University of Florida Gainesville, FL 32611 maminglian@ufl.edu  Abolfazl Saﬁkhani Department of Statistics George Mason University Fairfax, VA 22030 asafikha@gmu.edu  ABSTRACT  Deep neural networks are powerful tools to model observations over time with non-linear patterns. Despite the widespread use of neural networks in such settings, most theoretical developments of deep neural networks are under the assumption of independent observations, and theoretical results for temporally dependent observations are scarce. To bridge this gap, we study theoretical properties of deep neural networks on modeling non-linear time series data. Speciﬁcally, non-asymptotic bounds for prediction error of (sparse) feed-forward neural network with ReLU activation function is established under mixing-type assumptions. These assumptions are mild such that they include a wide range of time series models including auto-regressive models. Compared to independent observations, established convergence rates have additional logarithmic factors to compensate for additional complexity due to dependence among data points. The theoretical results are supported via various numerical simulation settings as well as an application to a macroeconomic data set.  1  Introduction  Neural networks have the ability to model highly complex relationship among data. If input data are observed data in past with future observations as response, neural networks can be utilized to perform time series forecasting. Examples of application of neural networks in forecasting include biotechnology [1], ﬁnance [2], health sciences [3], and business [4], just to name a selected few. Compared to more traditional time series forecasting methods such as ARIMA models [5], neural networks have the ability to detect highly non-linear trend and seasonality. In this work, we analyse the prediction error consistency of (deep) feed-forward neural networks to ﬁt stationary (non-linear) time series models.  The property of the single hidden layer neural network is well studied in the past few decades. For example, [6] use single hidden layer neural network with a transformed cosine activation function to show that a sufﬁciently complex single hidden layer feed-forward network can approximate any member of a speciﬁc class of functions to any desired degree of accuracy. Such an approximation property for neural networks with sigmoidal activation function was also analyzed in [7, 8]. Further, [9] use monotonic homogeneous activation function (general version of ReLU activation function) and show that both the input dimension and number of hidden units have an effect on the convergence rate when using single layer neural networks.  There are many recent works which shed some light on the reasoning behind the good performance of multi-layer (or deep) neural networks. The performance is evaluated via computing mean squared predictive error which is also called the statistical risk. For example, [10] show that the statistical risk of a multi-layer neural network depends on the number of layers and the input dimension of each layer. The problem of applying deep neural network in high-dimensional settings is that the high-dimensional input vector in nonparametric regression leads to a slow convergence rate [11] while the complexity often scales exponentially with the depth or number of units per layer [12, 13, 14]. Further, convergence rate for prediction error is also related to the regression function and fast rate can be obtained in special classes of regression functions such as additive and/or composition functions [11, 15, 16]. To avoid the curse of dimensionality and achieving faster rates, [17] work under hierarchical composition assumption with a sparse neural network. It is shown that under the independence assumption over input vectors, the estimator utilizing a sparse network achieves  nearly optimal convergence rates for prediction error. Finally, [18] use neural network as a classiﬁer for temporally dependent observations which are based on Markov processes. We refer to [19] for an overview of deep learning methods.  Most of theoretical developments related to prediction error consistency of neural networks are under the assumption that either the input variables are independent or they are independent with the error (noise) term, or both. However, these assumptions are restrictive and may not hold in time series models. To bridge this gap, the main goal of this paper is to establish consistency rates for prediction error of deep feed-forward neural networks for temporally dependent observations. To that end, we focus on multivariate nonparametric regression model with bounded and composite regression functions and apply sparse neural networks with ReLU activation functions for estimation (see more details in Section 2). The modeling framework is similar to [17] while the independence assumption is relaxed. Speciﬁcally, we show that given temporally dependent observations, under certain mixing condition, the statistical risk coincides with the result of independent observations setting with an additional log4(n) factor where n is the sample size (Theorem 1). Moreover, utilizing the Wold decomposition, this result is extended to a general family of stationary time series models in which it is shown that the decay rate of AR(∞) representation coefﬁcients plays an important role on the consistency rate for prediction error of neural networks (Theorem 2). These results give some insights on the effect of temporal dependence on the performance of neural networks by speciﬁcally quantifying the prediction error in such settings. Finally, the prediction performance of neural networks in time series settings is investigated empirically via several simulation scenarios and a real data application (Sections 4 and 5). Notation. For two random variables X and Y , X D= Y implies that X and Y have the same distribution. For a matrix W , (cid:107)W (cid:107)∞ := maxi,j|Wij|, (cid:107)W (cid:107)0 is the number of nonzero entries of W , and (cid:107)W (cid:107)1 := (cid:80) i,j |Wij|. For a vector v, |v|0, |v|1 and |v|∞ are deﬁned by the same way. We write (cid:98)x(cid:99) for the smallest integer ≥ x. For two sequences (at)t≥1 and (bt)t≥1, we write at (cid:46) bt if there exists a constant c ≥ 1 such that at ≤ cbt for all t. If both at (cid:46) bt and bt (cid:38) at, we write at (cid:16) bt. Also, at = o(bt) implies at/bt → 0 as t → ∞. For a multi-dimension random variable X, X ∼ N (µ, Σ) implies that X has multivariate Gaussian distribution with mean µ and covariance matrix Σ. For two functions f, g, we use f ◦ g(x) to denote f (g(x)). Also, (cid:107)f (cid:107)∞ := supx|f (x)| is the sup-norm of f , and (a)+ = max(a, 0) for a ∈ R.  2 Setup  In this section, a brief presentation of feed-forward neural networks is provided in Section 2.1 followed by discussing the modeling framework under consideration in Section 2.2.  2.1 Background about multi-layer neural networks  Multi-layer neural network is composed of three parts: input layer, hidden layers and output layer. We denote the depth of a multi-layer neural network by L, which implies that there are L + 1 layers in total consisting of L − 1 hidden layers, one input layer and one output layer. We refer to the input layer as the 0-th layer and the output layer as the L-th layer. The multi-layer neural network can be written as  f (x) : Rd −→ R = WLσvL(WL−1σvL−1(· · · W1σv0 (W0(x))))),  (1)  where Wi is the matrix of weights between (i − 1)-th and i-th layers of the network (i = 1, . . . , L) and σv is a modiﬁed ReLU activation function in each layer. Speciﬁcally, for the shift parameter v = (v1, · · · , vr) ∈ Rr, the activation function σv : Rr −→ R is deﬁned as           =       (x1 − v1)+ ... (xr − vr)+      .  x1 ... xr  σv  Let pi denote the number of units in the i-th layer (note that p0 = d and pL = 1). For a fully connected multi-layer neural network, the total number of parameters is (cid:80)L−1 i=0 pipi+1 that is deﬁned as the size of a multi-layer neural network [20]. Similar to [17], the entries of all weight matrices {Wi}i=1,··· ,L and shift parameters {vi}i=1,··· ,L are assumed to be uniformly bounded. The sparsity level s is deﬁned as the number of all non-zero parameters in {Wi}i=1,··· ,L and {vi}i=1,··· ,L. We also assume that the value of |f | is bounded by some constant F . In summary, we focus on the collection of s-sparse multi-layer neural networks with bounded parameters which is denoted by F(L, p, s, F ) and  2  deﬁned as  F(L, p, s, F ) := {f ∈ F(L, p) :  L (cid:88)  i=0  (cid:107)Wi(cid:107)0 + |vi|0 ≤ s, (cid:107)f (cid:107)∞ ≤ F },  where F(L, p) := {f of form (1) : max  i=0,1,··· ,L  (cid:107)Wi(cid:107)∞ ∨ |vi|∞ ≤ 1}.  This restriction of neural networks to the ones with sparse connections and bounded parameters is common on deep learning (see e.g. [17] and references therein) since neural networks are typically trained using certain penalization methods and dropouts.  2.2 Model  The modeling framework considered is similar to [17] while allowing for temporal dependence among observations. Let ((cid:15)t)t≥1 be a sequence of independent random variables with E[(cid:15)t] = 0. Let {Xt}t≥1 be a p-dimensional stationary process with Xt := (Xt,1, · · · , Xt,p). We assume Yt is generated as  Yt = f0(Xt) + (cid:15)t, with a measurable function f0 : Rp −→ R. We assume that the regression function f0 is a composition of several functions, speciﬁcally  (2)  f0 = gq ◦ gq−1 ◦ · · · ◦ g1 ◦ g0, (3) with gi : [ai, bi]di −→ [ai+1, bi+1]di+1, where d0 = p, dq+1 = 1. Each gi has a di+1-dimensional vector output gi = (gi,1, · · · , gi,di+1). We assume that the multivariate function gi,j depends on at most ti variables while ti is far less than di, i.e. ti (cid:28) di. As mentioned in [17], for a β-smooth function f0, the minimax estimation rate for the prediction error is n−2β/(2β+d0). Since the dimensionality d0 can be large in applications, the rate can be slow. To mitigate this issue, the sparse structure avoids the effect of input dimension on the convergence rate and improves the rate. Let T be a region in Rr. Let β and L be two positive numbers. The Hölder class Σ(β, L) is deﬁned as the set of α = (cid:98)β(cid:99) times differentiable functions f : T −→ Rr whose derivative ∂αf (x) satisﬁes  |∂αf (x) − ∂αf (y)| |x − y|β−(cid:98)β(cid:99)  ∞  ≤ L,  where we used the notation ∂α = ∂α1 · · · ∂αr with α = (α1, · · · , αr) and |α| := |α|1. Further, we deﬁne the ball of β-Hölder functions with radius K as  r (D, K) = {f :D ⊂ Rr −→ R : Cβ  (cid:88)  (cid:107)∂αf (cid:107)∞ +  (cid:88)  α:|α|<β  α:|α|=(cid:98)β(cid:99)  sup x,y∈D  |∂αf (x) − ∂αf (y)| |x − y|β−(cid:98)β(cid:99)  ∞  ≤ K}.  We assume that all functions gij for i = 0, · · · , q and j = 1, · · · , di+1 belong to βi-Hölder class Cβi the model (3), we know that Dij = [ai, bi]ti. Hence, the class of f0 we focus belongs to  ti (Dij, K). From  G(q, d, t, β, K) := {f0 = gq ◦ · · · ◦ g0 : gi = (gij)j : [ai, bi]di −→ [ai+1, bi+1]di+1, ti ([ai, bi]ti, K), for some |ai|, |bi| ≤ K},  gij ∈ Cβi  (4)  with d := (d0, · · · , dq+1), t := (t0, · · · , tq) and β := (β0, · · · , βq).  3 Main result  In this section, we present consistency results for prediction error of deep neural networks applied to model (2) followed by providing time series model examples satisfying the assumptions in Section 3.1. First, we need to introduce some notations and state the main assumptions under which the theoretical developments are established. For any estimator  3  (cid:98)fn in the class F(L, p, s, F ), we deﬁne (similar to [17]) ∆n( (cid:98)fn, f0) to measure the difference between the expected empirical risk of (cid:98)fn and the global minimum over all networks in the class F(L, p, s, F ) as  ∆n( (cid:98)fn, f0) (cid:34)  := Ef0  1 n  n (cid:88)  i=1  (Yi − (cid:98)fn(Xi))2 −  inf f ∈F (L,p,s,F )  1 n  n (cid:88)  (Yi − f (Xi))2  (cid:35)  .  i=1  The quantity ∆n( (cid:98)fn, f0) plays a pivotal role in consistency properties of neural networks. The performance of (cid:98)fn is evaluated by the prediction error deﬁned as  R( (cid:98)fn, f0) := Ef0  ( (cid:98)fn(X) − f0(X))2(cid:105) (cid:104)  ,  (5)  with X D= Xt and X is independent with {Xt}t≥0. Recall from Section 2.2, the regression function f0 is in the class G(q, d, t, β, K). To simplify notations, we deﬁne β∗ following assumptions are needed to present the ﬁrst theorem. Assumption 1. For all i = 1, 2, . . ., E[(cid:15)i] = 0, E[(cid:15)2 E[|(cid:15)i|m] ≤ σ2m!cm−2, m = 3, 4, · · · . Assumption 2. {Xt} is a strictly stationary and exponentially α-mixing process. Recall that the α-mixing coefﬁcient of a stationary process {Xt} is deﬁne as  i ] = σ2, and there exists some positive constant c such that  (cid:96)=i+1(β(cid:96) ∧ 1), φn := maxi=0,··· ,q n  2β∗ i +ti . The  i := βi  (cid:81)q  2β∗ i  −  α(s) = sup{|P(A ∩ B) − P(A)P(B)| : −∞ < t < ∞, A ∈ σ(X−  t ), B ∈ σ(X+  t+s)},  t consists of the entire past of the process including Xt, and X+  where X− {Xt} is said to be exponentially α-mixing if there exists some constant ˜c > 0 such that log(α(t)) ≤ −˜ct, t ≥ 1. Assumption 3. The error term (cid:15)t is independent with {Xs, s ≤ t}.  t consists of its entire future. The process  Assumption 1 is known as the Bernstein condition and implies that (cid:15)t is a sub-exponential variable. This assumption is often used when we cannot assume (cid:15)t is bounded. Assumption 2 is to control the dependence among input variables and holds for a wide range of time series models, see e.g. auto-regressive models in [21]. Assumption 3 controls dependence between input variables and error terms. A more stringent condition is to assume the whole error process {(cid:15)t}t≥0 is independent with {Xs}s≥0. However, this assumption is restrictive since it excludes auto-regressive model which is an important family of time series models. To avoid this, we only assume the current error term is independent of current and past input variables. Further, this assumption ensures that (cid:80) t (cid:15)tf0(Xt) is a martingale which helps in verifying certain concentration inequalities needed in the proof of main results. All three assumptions are common in non-linear time series analysis, see e.g. [22]. Now, we are ready to state the main result. Theorem 1. Consider the d-variate nonparametric regression model (2) for a composite regression function (3) in the class G(q, d, t, β, K). Suppose Assumptions 1-3 hold. Let (cid:98)fn be an estimator taking values in the network class F(L, (pi)i=0,··· ,L+1, s, F ) satisfying (i) F ≥ max(K, 1); (ii) (cid:80)q i=0 log2(4ti ∨ 4βi)log2n ≤ L (cid:46) nφn; (iii) nφn (cid:46) mini=1,··· ,Lpi; and (iv) s (cid:16) nφnlogn. Then, there exist positive constants C, C depending only on q, d, t, β, F , such that if ∆n( (cid:98)f , f0) ≤ CφnLlog6n, then  (cid:48)  R( (cid:98)fn, f0) ≤ C  (cid:48)  φnLlog6n,  and if ∆n( (cid:98)f , f0) ≥ CφnLlog6n, then 1 C (cid:48) ∆n( (cid:98)f , f0) ≤ R( (cid:98)fn, f0) ≤ C  (cid:48)  ∆n( (cid:98)fn, f0).  (6)  (7)  Based on Theorem 1, the prediction error deﬁned in (5) is controlled by φnLlog6n. From condition (ii), L is at least of the order of log2n. Thus, the rate in Theorem 1 becomes φnlog7n. This rate for the case of independent observations is of order φnlog3n based on Theorem 1 in [17]. Compared to latter, our rate has an extra log4n factor, which compensates for additional complexity in verifying the prediction error consistency in the presence of temporal dependence among input variables. Further, note that for a fully connected neural network, the number of parameters is (cid:80)L−1 nL. We can see that this number is greater than the sparsity level s which is of order nφnlogn based on condition (iv) in Theorem 1. Thus, it can be seen that condition (iv) restricts the neural network class to the ones with sparse architecture. In other words, at least (cid:80)L−1 i=1 pipi+1 − s units of the neural network is completely inactive.  i=1 pipi+1 (cid:38) n2φ2  4  3.1 Time series model examples  In this section, we introduce some (well-known) examples of time series models that satisfy the assumptions of Theorem 1. The ﬁrst example is to let {(cid:15)t}t∈Z and {Xt}t∈Z be two independent processes. This independence assumption implies that the input variables Xt are exogenous, thus Assumption 3 is automatically satisﬁed. Further, assume (cid:15)t satisfy the moment conditions in Assumption 1 (for example, they have normal distribution) and Xt is a stationary and geometrically α-mixing process. There are many examples of such processes including certain ﬁnite-order auto-regressive processes, see e.g. [23, 21]. The second example is to consider non-linear auto-regressive models, i.e. assume  Xt = g(Xt−1, · · · , Xt−d) + (cid:15)t.  (8)  This is a special case of model (2) by setting Xt = (Xt−d, · · · , Xt−1) and Yt = Xt. Assuming (cid:15)t’s are i.i.d. random variables with positive density in the real line and the boundedness of the function g, it can be shown that there exists a stationary solution to equation (8) while the solution is exponentially α-mixing as well [24, 21]. In both examples, assumptions of Theorem 1 are satisﬁed, thus the results of this Theorem are applicable.  Now, we consider a more general time series model. Recall that by the well-known Wold representation, every purely nondeterministic stationary and zero-mean stochastic process Xt can be expressed as Xt = (cid:80)∞ i=0 ai(cid:15)t−i where (cid:15)t is a mean-zero white noise. Further, if Xt has a non-vanishing spectral density and absolute summable auto-regressive coefﬁcients, i.e. (cid:80)∞ i=1 |φi| < ∞, it has the AR(∞) representation Xt = (cid:80)∞ i=1 φiXt−i + (cid:15)t (see e.g. [25]). Motivated by this discussion, we consider a general family of times series models satisfying  Xt =  ∞ (cid:88)  i=1  φiXt−i + (cid:15)t,  (9)  where (cid:15)t’s are i.i.d. errors. Independence among (cid:15)t’s is a strong assumption compared to only assuming that they are uncorrelated, but this is required for our theoretical analysis. The interesting fact about model (9) is that it is a linear model. However, since there are inﬁnite covariates in this AR(∞) representation, training neural networks directly is impossible. The common solution is to truncate the covariates and only consider the ﬁrst few, i.e. approximate model (9) by an AR(d) model for some d. This approximation can successfully estimate second order structures of the original model (i.e. spectral density or auto-correlation function) if d is selected carefully and under certain assumptions on the AR coefﬁcients φi’s (see e.g. [25]). Therefore, we follow this path and ﬁt a neural network to the d-dimensional input variables (Xt−1, . . . , Xt−d) with a proper choice of d while keeping in mind that the true regression function is in fact f0(Xt) = (cid:80)∞ i=1 φiXt−i. To establish the prediction error consistency of neural networks on truncated input variables, we need two additional assumptions. Assumption 4. There exists α > 0, M > 0 such that (cid:80)∞ Assumption 5. For some constant K > 0, |Xt| ≤ K for all t ≥ 0.  i=1(1 + i)α|φi| ≤ M < ∞.  Assumption 4 controls the decay rate of the AR(∞) coefﬁcients in the true model and α can be treated as a decreasing rate of φi’s. This assumption is needed to compensate for approximating a general time series of form (9) with a ﬁnite lag AR process. Further, it plays an important role in restricting the ﬁrst derivative of f0, which corresponds to the βi-smoothness assumption on gij in (4). Note that Assumption 4 is satisﬁed if the spectral density function is strictly positive and continuous, and the auto-covariance function of Xt has some bounded property [26]. Moreover, since the model is linear (i.e. the regression function is unbounded), Assumption 5 becomes necessary to make f0(Xt) bounded, a property needed for Theorem 1 as well. Theorem 2. Consider model (9) with f0(Xt) = (cid:80)∞ i=1 φiXt−i. Let (cid:98)fn be an estimator taking values in the network class F(L, (pi)i=0,··· ,L+1, s, F ) satisfying (i) F ≥ KM ; (ii) L ≥ 4; (iii) s (cid:16) Ld, and (iv) d (cid:46) mini=1,··· ,Lpi. Assume that α+1 Llog5n, d (cid:16) n then  α+1 . Under the Assumptions 1-5, there exist positive constants C, C  such that if ∆n( (cid:98)f , f0) ≤ Cn− α  1  (cid:48)  and if ∆n( (cid:98)f , f0) > Cn− α  α+1 Llog5n, then  R( (cid:98)fn, f0) ≤ C  (cid:48)  n− α  α+1 Llog5n,  1 C (cid:48) ∆n( (cid:98)f , f0) ≤ R( (cid:98)fn, f0) ≤ C  (cid:48)  ∆n( (cid:98)f , f0).  5  (10)  (11)  Based on Theorem 2, the best convergence rate of R( (cid:98)fn, f0) for model (9) is n− α α+1 log5n. Compared with the convergence rate in Theorem 1, we can see that the convergence rate in model (9) depends on the decreasing rate of coefﬁcients φi’s instead of the smoothness of regression function. Also the logarithmic factor changes from log7n to log5n. Such a subtle decrease is due to the fact that since the truncated model is linear, a shallower and sparser neural network can be used in the proof of Theorem 2 (as seen from conditions (ii) and (iii) in the statement of Theorem 2). Also, note that for a simple linear AR(d) model where φj = 0 for j > d, Assumption 4 is satisﬁed for any large α. Thus, in this case, the best convergence rate of prediction error becomes n−1log5n. Remark 1. To ﬁt the neural network to a truncated model (9), shallow neural networks are sufﬁcient. In fact, L = 4 is enough based on Theorem 2. This is because model (9) has a simple linear structure compared to composition functions (3). Remark 2. For a general selection of input vector d, the result of Theorem 2 becomes R( (cid:98)f , f0) ≤ C 1 n + 1 4∆n( (cid:98)f , f0). With the choice of d (cid:16) n α+1 , we balance the ﬁrst two terms and establish a proper upper bound for R( (cid:98)fn, f0). Thus, this selection can be regarded as the “optimal selection\" of lag d when applying neural networks for estimation in model (9).  dα + C  (cid:48) dLlog5n  4 Simulation experiments  In this section, we conduct two simulation settings to illustrate the performance of neural networks applied to temporally dependent data (see also Section C in the supplementary materials for numerical comparisons between feed-forward neural networks and LSTM). In the ﬁrst simulation in Section 4.1, we aim to compare the convergence rate in dependent observations setting with the rate in independent observations setting. The convergence rate is explained as how fast the mean squared predictive error decreases as the sample size grows. In the second simulation (Section 4.2), motivated by time series model examples introduced in Section 3.1, we use neural networks to ﬁt linear and non-linear auto-regressive models and compare the performance with the result of linear regression method (least squares method). All simulations are repeated 200 times.  To train the neural network, we split the data into three parts: training set T1, validation set T2, and testing set T3. We set T1 = {1, · · · , n/2}, T2 = {n/2, · · · , 3n/4}, and T3 = {3n/4, · · · , n}. We penalize parameters of weight matrix of the neural network and use mean square error as our loss. To be more speciﬁc, the loss function is deﬁned as  loss :=  (cid:88)  t∈T1  (Yt − (cid:98)Yt)2/(n/2) + λ  L (cid:88)  i=1  (cid:107)Wi(cid:107)1,  (12)  where λ is the sparsity tuning parameter. The value of λ does not have signiﬁcant effect on simulation results. In this section, we set λ = 0.1. We apply gradient descent method to update parameters of neural network and stop the iteration when the neural network gives the minimum mean square error for T2. The prediction error R( (cid:98)fn, f0) deﬁned in (5) is empirically estimated by (cid:98)R = (cid:80) (f0(Xt) − (cid:98)Yt)2/(n/4). We use (cid:98)R to evaluate the performance of the trained neural network.  t∈T3  4.1 Dependent vs. independent observations  In this experiment, we consider a nonlinear additive model f0(Xt) = 2 (cid:80)4 model  i=1 cos(Xt,i). Thus, Yt is from the following  Yt = 2  cos(Xt,i) + ηt,  t = 1, 2, · · · , n,  (13)  4 (cid:88)  i=1  with ηt’s as independent standard normal random variables, and Xt := (Xt,1, · · · , Xt,4) is generated from an AR(4) model. Speciﬁcally,  Xt =     0 ρ 0 0   0 0  0 0 ρ 0 0 ρ 0 0 0 0    Xt−1 + (cid:15)t,  (cid:15)t ∼ N (0, I4),  where ρ takes two value, ρ = 0.2, 0.6. The sample size is n = 100, 400, 1600, 6400. A three layer neural network is selected to ﬁt model (13) while ReLU is used as the activation function. The network requires a 4- dimensional input vector and has 20 units in each hidden layer. To make a fair comparison with the performance of neural network  6  in independent observations setting, we still use model (13) but with independent observations, i.e. we generate D= X1. X  n ∼ N (0, Σ) i.i.d. and Σ derives from X  1, · · · , X  (cid:48) 1  (cid:48)  (cid:48)  (a) ρ = 0.2  (b) ρ = 0.6  Figure 1: Box plots of logarithm of the mean square error on the testing set as a function of sample size.  (cid:16)  (cid:17)  (cid:98)R  Figure 1 illustrates log against various sample sizes for both temporally dependent case and independent case by running 200 replications for each. As can be seen, the prediction performance of neural network improves as sample size increases. Further, we observe that the prediction error for the neural network with temporally dependent data and independent one are similar. This may imply that the additional logarithmic terms appearing in Theorem 1 might be an artifact of the proof.  4.2 Auto-regressive examples  In this experiment, we test the performance of the neural network and compare its result with simple linear regression (least squares method) for several linear and non-linear auto-regressive models. Speciﬁcally, we consider the following four models: (1) Xt = 0.6Xt−1 + (cid:15)t; (2) Xt = 0.6Xt−1 − 0.4Xt−2 + 0.2Xt−3 + (cid:15)t; (3) Xt = 0.5(cid:112)|Xt−1| + (cid:15)t; (4) Xt = 0.5|Xt−1| + (cid:15)t. The error term is generated as (cid:15)1, · · · , (cid:15)n ∼ N (0, 1) i.i.d. in all models. The sample size is n = 100, 400, 1600, 6400.  We again use a four layer neural network with 20 units in each hidden layer. This time, we have no prior knowledge on the input dimension of the neural network, since lags of these four time series models are unknown. To determine the input dimension (lag of time series), we apply linear regression with AIC criterion. The AIC is deﬁned as AIC = nlog(SSE) + 2d, in which d is the input dimension and SSE is the summation of squared errors.  Figure 2 displays the logarithm of (cid:98)R( (cid:98)f , f0) as sample size increases. Linear regression method has a fast convergence rate in AR(1) and AR(3) models, i.e. models (1) and (2). For non-linear auto-regressive models (models (3) and (4)), the convergence rate of linear regression method becomes slow while neural network outperforms the linear method in these cases. In summary, neural network has the advantage to deal with estimation in both linear and non-linear auto-regressive models.  5 Real data analysis  In this section, we apply neural network on a macroeconomic data1 to predict monthly inﬂation rate. The data consist of 132 monthly macroeconomic variables from January 1960 to December 2011, a total of 624 time periods. The inﬂation rate is measured by the percentage changes of the Consumer Price Index (CPI). To be more speciﬁc, we are interested in forecasting  πt = 1200 × log  .  (14)  (cid:19)  (cid:18) CPIt+1 CPIt  Similar to [27, 28], we use the AR(4) model as the benchmark model, i.e. πt+1 = α0 + (cid:80)3 model can be written as  i=0 αiπt−i. Thus, the overall  πt+1 = f (πt.πt−1, πt−2, πt−3, xt) + vt+1,  (15)  1link : https://www.sydneyludvigson.com/data-and-appendixes  7  (a) model (1)  (b) model (2)  (c) model (3)  (d) model (4)  Figure 2: Box plots of logarithm of the mean square error on the testing set as a function of sample size.  where xt consists of other 131 predictors in the data set along with three of their lags. Therefore, xt is a 524-dimensional vector. We consider one step ahead forecasts computed in a rolling window scheme with 453 observations. Since there are some missing values in the ﬁrst four years, we start from 1964 to ﬁt model and predict the inﬂation rate. We use the observations between May 1964 and Jan 2002 to ﬁt (15) and predict the monthly inﬂation rate starting at time Feb 2002.  We apply the feature screening method Sure Independent Screening (SIS) proposed in [29] to reduce the input dimension and select only the top γ% important variables from xt for γ = 0, 5, 7.5, 10. The case of γ = 0 implies that we do not select any covariates from xt and only use four lags of πt as predictors. Then we use the selected covariates and four lags of πt as predictors or the input vector of our neural network. The neural network is selected to have ReLU as the activation function and 100 units in each hidden layers. Two different depths of neural network are selected, one with 3 layers (1 hidden layers) and the other with 6 layers (4 hidden layers). To train the neural network, we use dropout method for hidden layers and gradient descent to update parameters. In each rolling window, 453 observations are divided into two parts: the ﬁrst 300 observations is treated as training set while the last 153 observations is treated as validation set. The loss function is again (12) with λ = 0.1. We stop training when MSE over the validation set reaches its minimum. The performance is evaluated by MSE of one step ahead forecast. We also show the performance of linear regression (i.e. least squares method) with the same input as neural network.  The results are summarized in Table 1 while Figure 3 in the supplementary materials plots the predicted values against the truth. As seen from Table 1, neural network has a better performance compared to linear regression method. In other words, the overall prediction error of neural network with three layers is small compared to the ones for linear regression (except for γ = 7.5). Speciﬁcally, in the case of γ = 10 (i.e. including 52 covariates from vector xt), the performance of neural network remains satisfactory while linear regression method suffers from large input vector dimension. Finally, note that three layers seems to be enough for this data as increasing the number of layers does not reduce the prediction error.  6 Conclusion  Considering nonparametric regression model with the regression function belonging to a speciﬁc family of bounded composite functions, we analyzed the performance of deep feed-forward neural networks with ReLU activation function on estimating such functions. Consistency of prediction error is established under mild conditions on the input data which can include temporal dependence among observations. Interestingly, the consistency rate matches the one for  8  γ% = 0% γ% = 5% γ% = 7.5% γ% = 10%  neural network (3 layers) 15.71 15.65 16.40 16.51  neural network (6 layers) 17.44 18.37 19.67 20.70  linear regression 18.05∗ 17.05 15.42 172.49  Table 1: Prediction error of inﬂation rate, i.e. the mean square of the forecasting error. Since the result of neural network depends on initialization, we averaged results across 10 replicates. The entry with asterisk corresponds to the benchmark model (i.e. AR(4) model).  independent data with additional logarithmic factors with respect to sample size. This result is applicable to a wide range of linear and non-linear time series models including ﬁnite lag non-linear auto-regressive models. Then, the result is extended to include a general family of stationary time series models utilizing the Wold decomposition while the consistency rate depends on the decay rate of AR(∞) representation coefﬁcients. Relaxing some assumptions in the theoretical analysis including the mixing condition and boundedness of observations in the case of general time series models are interesting future directions. Another limitation of the work is considering only the ReLU as the activation function while extending the results to a more general family of activation functions is a fruitful research direction.  References  [1] Jonas S Almeida. Predictive non-linear modeling of complex data by artiﬁcial neural networks. Current opinion  in biotechnology, 13(1):72–76, 2002.  [2] Marcus D Odom and Ramesh Sharda. A neural network model for bankruptcy prediction. In 1990 IJCNN  International Joint Conference on neural networks, pages 163–168. IEEE, 1990.  [3] Paulo JG Lisboa. A review of evidence of health beneﬁt from artiﬁcial neural networks in medical intervention.  Neural networks, 15(1):11–39, 2002.  [4] G Peter Zhang. Neural networks in business forecasting. IGI global, 2004. [5] Peter J Brockwell and Richard A Davis. Introduction to time series and forecasting. Springer, 2002.  [6] Daniel F McCaffrey and A Ronald Gallant. Convergence rates for single hidden layer feedforward networks.  Neural Networks, 7(1):147–158, 1994.  [7] Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions  on Information theory, 39(3):930–945, 1993.  [8] Andrew R Barron. Approximation and estimation bounds for artiﬁcial neural networks. Machine learning,  14(1):115–133, 1994.  [9] Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of Machine  Learning Research, 18(1):629–681, 2017.  [10] Andrew R Barron and Jason M Klusowski. Approximation and estimation for high-dimensional deep learning  networks. arXiv preprint arXiv:1809.03090, 2018.  [11] Benedikt Bauer and Michael Kohler. On deep learning as a remedy for the curse of dimensionality in nonparametric  regression. The Annals of Statistics, 47(4):2261–2285, 2019.  [12] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks.  Advances in neural information processing systems, 30, 2017.  [13] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks.  In Conference On Learning Theory, pages 297–299. PMLR, 2018.  [14] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In  Conference on Learning Theory, pages 1376–1401. PMLR, 2015.  [15] Anatoli B Juditsky, Oleg V Lepski, and Alexandre B Tsybakov. Nonparametric estimation of composite functions.  The Annals of Statistics, 37(3):1360–1404, 2009.  [16] Michael Kohler and Adam Krzy˙zak. Adaptive regression estimation with multilayer feedforward neural networks.  Nonparametric Statistics, 17(8):891–913, 2005.  [17] Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation function.  The Annals of Statistics, 48(4):1875–1897, 2020.  9  [18] Lan V Truong. Generalization error bounds on deep learning with markov datasets.  arXiv preprint  arXiv:2201.11059, 2021.  [19] Jianqing Fan, Cong Ma, and Yiqiao Zhong. A selective overview of deep learning.  arXiv preprint  arXiv:1904.05526, 2019.  [20] Anna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous, and Yann LeCun. The loss surfaces of  multilayer networks. In Artiﬁcial intelligence and statistics, pages 192–204. PMLR, 2015.  [21] Paul Doukhan. Mixing: properties and examples, volume 85. Springer Science & Business Media, 2012. [22] Richard A Davis and Mikkel S Nielsen. Modeling of time series using random forests: Theoretical developments.  Electronic Journal of Statistics, 14(2):3644–3671, 2020.  [23] Jianqing Fan and Qiwei Yao. Nonlinear time series: nonparametric and parametric methods. Springer Science &  Business Media, 2008.  [24] HZ An and FC Huang. The geometrical ergodicity of nonlinear autoregressive models. Statistica Sinica, pages  943–956, 1996.  [25] Jiang Wang and Dimitris N Politis. Consistent autoregressive spectral estimates: Nonlinear time series and large  autocovariance matrices. Journal of Time Series Analysis, 42(5-6):580–596, 2021.  [26] Jens-Peter Kreiss, Efstathios Paparoditis, and Dimitris N Politis. On the range of validity of the autoregressive  sieve bootstrap. The Annals of Statistics, 39(4):2103–2130, 2011.  [27] Marcelo C Medeiros and Eduardo F Mendes. (cid:96)1-regularization of high-dimensional time-series models with  non-gaussian and heteroskedastic errors. Journal of Econometrics, 191(1):255–271, 2016.  [28] Wei-Biao Wu and Ying Nian Wu. Performance bounds for parameter estimates of high-dimensional linear models  with correlated errors. Electronic Journal of Statistics, 10(1):352–379, 2016.  [29] Jianqing Fan and Jinchi Lv. Sure independence screening for ultrahigh dimensional feature space. Journal of the  Royal Statistical Society: Series B (Statistical Methodology), 70(5):849–911, 2008.  [30] Florence Merlevède, Magda Peligrad, and Emmanuel Rio. Bernstein inequality and moderate deviations under strong mixing conditions. In High dimensional probability V: the Luminy volume, pages 273–292. Institute of Mathematical Statistics, 2009.  [31] VH De La Pena. A general class of exponential inequalities for martingales and ratios. 1999. Ann. Probab,  36:1902–1938.  10  Appendix  We provide proofs of main theorems in Section A.2 while some useful Lemmas are stated and proved in Section A.1. In Section B, additional details related to the numerical experiments are provided. We also provide a numerical comparison between feed-forward deep neural networks (DNN) and Long Short-Term Memory networks (LSTM) in Section C.  A Proofs in Section 3  A.1 Useful Lemmas and their proofs  lemma 1. Assume that {Xt}t∈Z satisﬁes Assumption 2, that is, {Xt}t∈Z is a stationary and exponential α−mixing process. Further, assume f is a measurable function satisfying (cid:107)f (·)(cid:107)∞ < M . Then |Cov(f (Xi), f (Xj))| ≤ 11M 2exp(−c|i − j|/3),  for some positive constant c depending on ˜c.  Proof. Consider the set of grid points  D(m) := {(ak, bl), k, l = 1, 2, · · · , 2m + 1} ak = M (k − m − 1)/m, bl = M (l − m − 1)/m,  where m is some positive integer. Then we have  (cid:12) (cid:12) (cid:12) (cid:12)  E(f (Xi)f (Xj)) −  2m (cid:88)  2m (cid:88) (  k=1  l=1  ak + ak+1 2  )(  bl + bl+1 2  (cid:12) (cid:12) )P(ak ≤ f (Xi) < ak+1, bl ≤ f (Xj) < bl+1) (cid:12) (cid:12)  P(ak ≤ f (Xi) < ak+1, bl ≤ f (Xj) < bl+1)  M 2 m  ≤  =  2m (cid:88)  2m (cid:88)  l=1  .  k=1 M 2 m  Similar to (16) we can prove that  (cid:12) (cid:12) (cid:12) (cid:12)  E[f (Xi)] −  2m (cid:88)  k=1  ak + ak+1 2  (cid:12) (cid:12) P(ak ≤ f (Xi) < ak+1) (cid:12) (cid:12)  ≤  M 2m  .  To simplify the notation, we let Ai := (cid:80)2m  k=1  ak+ak+1 2  P(ak ≤ f (Xi) < ak+1). Then, we have  (cid:12) (cid:12) (cid:12) (cid:12)  E[f (Xi)]E[f (Xj)] − AiAj  (cid:12) (cid:12) (cid:12) (cid:12)  ≤  ≤  (cid:12) (cid:12) (cid:12) (cid:12) M 2 2m  (cid:12) (cid:12) E[f (Xi)](E[f (Xj)] − Aj) (cid:12) (cid:12) M 2 m  M 2m  M 2m  (M +  ) =  +  +  (cid:12) (cid:12) (E[f (Xj)] − Aj)Ai (cid:12) (cid:12)  (cid:12) (cid:12) (cid:12) (cid:12)  +  M 2 4m2 .  (16)  (17)  Since {Xt}t∈Z is an exponential α−mixing sequence, we know that there exists some positive constant c such that  P(ak ≤ f (Xi) < ak+1, bl ≤ f (Xj) < bl+1) ≤ exp(−c|i − j|) + P(ak ≤ f (Xi) < ak+1)P(bl ≤ f (Xj) < bl+1).  Therefore  (cid:12) (cid:12) (cid:12) (cid:12)  2m (cid:88)  2m (cid:88)  k=1  l=1  (  ak + ak+1 2  )(  bl + bl+1 2  )P(ak ≤ f (Xi) < ak+1, bl ≤ f (Xj) < bl+1) − AiAj  2m (cid:88)  2m (cid:88)  ≤  k=1  l=1  |(  ak + ak+1 2  )(  bl + bl+1 2  ≤ 4m2M 2exp(−c|i − j|).  )|exp(−c|i − j|)  11  (cid:12) (cid:12) (cid:12) (cid:12)  (18)  From (16),(17) and (18), we have that  (cid:12) (cid:12) (cid:12) (cid:12)  (cid:12) (cid:12) E[f (Xi)f (Xj)] − E[f (Xi)]E[f (Xj)] (cid:12) (cid:12)  ≤  2M 2 m  +  M 2 4m2 + 4m2M 2exp(−c|i − j|).  With the choice of m = (cid:98)exp(c|i − j|/3)(cid:99),  (cid:12) (cid:12) (cid:12) (cid:12)  (cid:12) (cid:12) E[f (Xi)f (Xj)] − E[f (Xi)]E[f (Xj)] (cid:12) (cid:12)  ≤ 10M 2exp(−c|i − j|/3) +  ≤ 11M 2exp(−c|i − j|/3).  M 2 4  exp(−2c|i − j|/3)  lemma 2. Let {Xt}t∈Z and f be as in lemma 1 while {an}n∈Z is a sequence of real numbers such that an ≤ nα for some positive α. Let Yni = anf (Xi), then  Var(Yn0) + 2  (cid:88)  i>0  |Cov(Yn0, Yni)| ≤ ([24αlog(n)/c] + 3)Var(Yn0) + 22M 2  1  n2α(exp( c  3 ) − 1)  ,  where c is the same constant as in lemma 1 depending on ˜c (the α-mixing exponent).  Proof. Let k = [ 12α  2  Therefore  (cid:88)  c log(n)] + 2. Notice that Yni < nαM for all i ∈ Z. From lemma 1, we have that 3 (k − 1)) 3 − 1 1  (nαM )2exp(−ci/3) = 22n2αM 2 exp(− c exp c  |Cov(Yn0, Ynk)| ≤ 22  (cid:88)  i≥k  i≥k  ≤ 22M 2  n2αexp( c  3 − 1)  .  Var(Yn0) + 2  (cid:88)  i>0  |Cov(Yn0, Ynk)| ≤ (2k − 1)Var(Yn0) + 22M 2  1  n2α(exp( c  3 ) − 1)  .  Let F be a class of function. We deﬁne N (δ, F, (cid:107) · (cid:107)∞) to be the covering number, that is, the minimal number of (cid:107) · (cid:107)∞-balls with radius δ that covers F. lemma 3. Consider the d-variate nonparametric regression model with unknown regression function f0, Yi = f0(Xi) + (cid:15)i, satisfying Assumptions 1-3. Let (cid:98)f be any estimator taking values in F. Deﬁne  ∆n := ∆n( (cid:98)f , f0, F) := Ef0  (cid:34)  1 n  n (cid:88)  i=1  (Yi − (cid:98)f (Xi))2 − inf f ∈F  1 n  n (cid:88)  (Yi − f (Xi))2  (cid:35)  i=1  and assume {f0} ∪ F ⊂ {f : [0, 1]d → [−F, F ]} for some F ≥ 1. If Nn := N (δ, F, (cid:107) · (cid:107)∞) ≥ 3, then,  (1 − (cid:15))2∆n − C(F, σ2, c)  log4nlogNn n(cid:15)  − δC(F, σ2, c)log2n  ≤ R( (cid:98)f , f0) ≤  (1 + (cid:15))2  (cid:18)  inf f ∗∈F  (cid:107)f ∗ − f0(cid:107)2  ∞ + ∆n( (cid:98)f , f ) + C(F, σ2, c)δlog2n  (cid:19)  +  (1 + (cid:15))3 (cid:15)  C(F, σ2, c)  log4nlogNn n  ,  where C(F, σ2, c) is deﬁned as a constant depending only on F, σ2, and c (the same constant in lemma 1).  12  i=1 g(Xi)2. For any estimator ˜f , we introduce Proof. Throughout the proof we write E = Ef0. Deﬁne (cid:107)g(cid:107)2 (cid:98)Rn( ˜f , f0) := E[(cid:107) ˜f − f0(cid:107)2 n] for the empirical risk. In the ﬁrst step, we show that we can restrict ourselves to the case logNn ≤ n. Since R( (cid:98)f , f0) ≤ 4F 2, the upper bound trivially holds if logNn ≥ n. To see that also the lower bound is trivial in this case, let ˜f ∈ argminf ∈F  i=1(Yi − f (Xi))2 be a (global) empirical risk minimizer. Observe that  n := 1 n  (cid:80)n  (cid:80)n  (cid:98)Rn( (cid:98)f , f0) − (cid:98)Rn( ˜f , f0) = ∆n + E  (cid:34)  2 n  n (cid:88)  i=1  (cid:35)  (cid:34)  (cid:15)i (cid:98)f (Xi)  − E  (cid:35) ˜f (Xi)  .  (cid:15)i  2 n  n (cid:88)  i=1  From this equation, it follows that ∆n ≤ 8F 2 and this implies the lower bound in the statement of the lemma for logNn ≥ n. We may therefore assume logNn ≤ n. The proof is divided into four parts which are denoted by (I∼IV) (I): We relate the risk R( (cid:98)f , f0) = E[( (cid:98)f (X) − f0(X))2] to its empirical counterpart (cid:98)Rn( (cid:98)f , f0) via the inequalities  (1 − (cid:15)) (cid:98)Rn( (cid:98)f , f0) − CF  ≤ R( (cid:98)f , f0) ≤  log2nlogNn n  − CF δlog2n −  1 (cid:15)  CF F 2log4nlogNn n  (1 + (cid:15)) (cid:98)Rn( (cid:98)f , f0) + (1 + (cid:15))  CF  (cid:18)  log2nlogNn n  (cid:19)  + CF δlog2n  (1 + (cid:15))2 (cid:15) where CF is some constant which depends on F and c.  CF F 2log4nlogNn n  +  ,  (II): For any estimator ˜f taking values in F, (cid:35) (cid:12) (cid:12) ˜f (Xi) (cid:12) (cid:12)  (cid:34)(cid:12) (cid:12) (cid:12) (cid:12)  n (cid:88)  2 n  (cid:15)i  E  i=1  ≤ δC(F, σ2, c)log2n + C(F, σ2, c)log4n  logNn n  (cid:115)  + C(F, σ2, c)  log4nlogNn n  (cid:98)R1/2  n ( ˜f , f0).  (III): We have  (IV): We have  (cid:98)Rn( (cid:98)f , f0) ≤ (1 + (cid:15))  (cid:20) inf f ∈F E[(f (X) − f0(X))2] + ∆n + δC(F, σ2, c)log2n  + C(F, σ2, c)log4n  (cid:21)  +  logNn n  (1 + (cid:15))2 (cid:15)  C 2(F, σ2, c)log4n  logNn n  .  (cid:98)Rn( (cid:98)f , f0) ≥ (1 − (cid:15))(∆n − C(F, σ2, c)  log4nlogNn n(cid:15)  − 2δC(F, σ2, c)log2n).  Combining (I) and (IV) gives the lower bound of the assertion. The upper bound follows from (I) and (III).  (I): Given a minimal δ-covering of F, denote the centers of the balls by fj . By construction there exists a (random) j∗ such that (cid:107) (cid:98)f − fj∗ (cid:107)∞ ≤ δ. Without loss of generality, we can assume that (cid:107)fj(cid:107)∞ ≤ F . Generate i.i.d. random i, i = 1, · · · , n} with the same distribution as X (X D= Xi) and independent of {Xi, i = 1, · · · , n}. Using variables {X that (cid:107)fj(cid:107)∞, (cid:107)f0(cid:107)∞, δ ≤ F ,  (cid:48)  (cid:48)  i))2 −  1 n  n (cid:88)  i=1  ( (cid:98)f (Xi) − f0(Xi))2  (cid:35) (cid:12) (cid:12) (cid:12) (cid:12)  |R( (cid:98)f , f0) − (cid:98)Rn( (cid:98)f , f0)| n (cid:88)  (cid:34)  (cid:48)  ( (cid:98)f (X  i) − f0(X  =  E  (cid:12) (cid:12) (cid:12) (cid:12)  1 n  ≤ E  (cid:34)(cid:12) (cid:12) (cid:12) (cid:12)  1 n  i=1 n (cid:88)  i=1  gj∗ (Xi, X  (cid:35)  (cid:48)  (cid:12) (cid:12) i) (cid:12) (cid:12)  + 9δF,  13  (cid:48)  (cid:48)  (cid:48)  with gj∗ (Xi, X fj. Similarly, set γj := (cid:112)n−1logNn ∨ E1/2[(fj(X) − f0(X))2] and deﬁne γ∗ as  i) := (fj∗ (X  i))2 − (fj∗ (Xi) − f0(Xi))2. Deﬁne gj in the same way with f ∗  i) − f0(X  j replaced by  (cid:112)  (cid:112)  γ∗ =  ≤  n−1logNn ∨ E1/2[(fj∗ (X) − f0(X))2|{(Xi, Yi)}∞ n−1logNn + E1/2[( (cid:98)f (X) − f0(X))2|{(Xi, Yi)}∞  i=1] i=1] + δ,  (19)  where the last part follows from triangle inequality and fj∗ − (cid:98)f ≤ δ. For random variables U1, T1, Cauchy-Schwarz inequality gives E[U1T1] ≤ E1/2[U 2 E1/2[( (cid:98)f (X) − f0(X))2|{(Xi, Yi)}∞  1 ]E1/2[T 2 i=1 gj(Xi, Xi(cid:48) )/γjF |. Using that E[U 2  i=1] and T1 = maxj | (cid:80)n  1 ]. Choose U1 = 1 ] = R( (cid:98)f , f0),  |R( (cid:98)f , f0) − (cid:98)Rn( (cid:98)f , f0)|  ≤  F n  R( (cid:98)f , f0)  1  2 E 1  2 [T 2  1 ] +  (cid:114)  F n  (  logNn n  + δ)E[T1] + 9δF.  (20)  Next, we need to estimate the upper bound for E[T ] and E[T 2]. To simplify the notation, we let v2  j =  Var(gj(Xi, X  |gj(Xi, X  (cid:48)  i)/γjF ) and ˜v2  k>i Cov i)/F | ≤ 4F and 1/γj ≤ n1/2. From lemma 2, we can derive  j = Var  gj (Xi,X γj F  + 2 (cid:80)  (cid:48)  (cid:18)  (cid:19)  (cid:48) i)  (cid:18)  (cid:48) k)  gj (Xk,X γj F  , gj (Xi,X γj F  (cid:48) i)  (cid:19)  . We know that  j ≤ ([12log(n)/c] + 3)v2 ˜v2  j + 22(4F )2  1 n(exp(c/3) − 1)  .  j = 2Var((fj(Xi) − f0(Xi))2/γjF ) ≤ 2E[(fj(Xi) − f0(Xi))4]/(γ2  Since v2 C1log(n) + C2F 2/n, where C1 and C2 are constants which only depend on c. Observe that E[gj(Xi, X | gj (Xi,X γj F By Bernstein inequality (Theorem 2, [30]) with a union bound over j, we have  j ≤ i)] = 0, | ≤ 4F/γj. Also, we know that {gj(Xi, Xi(cid:48) )/γjF, i = 1, · · · , n} is an exponentially α-mixing process.  j F 2) ≤ 8, we conclude that ˜v2  (cid:48) i )  (cid:48)      P(T1 ≥ t) ≤ 1 ∧  2Nn max  j  exp  −  ˜ct2 + t 4F γj  (logn)2          n˜v2  j + 16F 2 γ2 j      ≤ 1 ∧  2Nn max  j  exp  −      ≤ 1 ∧  2Nn max  j  exp  −  ˜ct2 log(Nn) + 4tF (logn)2(cid:113) n n log(Nn)            C1nlog(n) + C2F 2 + 16F 2  ˜ct2 n(C1log(n) + (16 + C2)F 2) + 4tF (logn)2(cid:113) n  log(Nn)     .  Therefore we can estimate the upper of E[T1] by P(T1 > t)  E[T1] =  (cid:90) ∞  0  P(T1 > t)dt  ≤ θ(cid:112)nlog(Nn) + 2Nn  (cid:90) ∞ √  θ  nlog(Nn)    exp   −  ˜ct  ((16+C2)F 2+C1log(n))n  √  θ  nlog(Nn)  + 4  = θ(cid:112)nlog(Nn) + 2Nn  1 ˜c   exp   −  (cid:32)  ((16 + C2)F 2 + C1log(n))n θ(cid:112)nlog(Nn)  + 4  (cid:114) n  log(Nn)  ˜cθ(cid:112)nlog(Nn) + 4  (cid:113) n  ((16+C2)F 2+C1log(n))n  √  θ  nlog(Nn)  log(Nn) (logn)2F      .  14       (cid:113) n  log(Nn) (logn)2F (cid:33)  (logn)2F  Let θ =  (cid:113) (32+2C2)F 2+2C1log(n) ˜c  ∨ 8F (logn)2 ˜c  , we have  E[T ] ≤ θ(cid:112)nlog(Nn) +  (cid:18) (32 + 2C2)F 2 + 2C1log(n) θ˜c  +  8F (logn)2 ˜c  (cid:19) (cid:114) n  log(Nn)  = θ(cid:112)nlog(Nn) + A1  (cid:114) n  log(Nn)  .  Next we estimate the upper bound of E[T 2 (cid:90) ∞  √  E[T 2  1 ] =  P(T1 >  t)dt  0  1 ] in a similar way.  ≤ θ2nlog(Nn) + 2Nn  (cid:90) ∞  θ2nlog(Nn)    exp   −  √ ˜c ((16+C2)F 2+C1log(n))n  t  √  + 4  θ  nlog(Nn)      dt.  (cid:113) n  log(Nn) (logn)2F  Using the fact that (cid:82) ∞ b2 exp(− the upper bound of E[T1]. Also, we can prove that  √  ua)du = (2ab + 1)exp(−ab)/a2 and with the same choice of θ as above, we estimate  E[T 2  1 ] ≤ θ2nlog(Nn) + 6n  (cid:18) (16 + C2)F 2 + C1log(n) θ˜c2  +  4log2(n)F ˜c2  (cid:19)  = θ2nlog(Nn) + 3nA1/˜c  ≤ (θ2 +  3A1 ˜c  )nlogNn.  With eq(20) and the upper bound for ET1 and ET 2  1 , we have  |R( (cid:98)f , f0) − (cid:98)Rn( (cid:98)f , f0)| ≤  F n  R( (cid:98)f , f0) (cid:114) (  F n  R( (cid:98)f , f0) (cid:114)  F δ(θ  =  F n  1  2 (cid:112)θ2nlog(Nn) + 3nA1/˜c+  (cid:18)  + δ)  θ(cid:112)nlogNn + A1  logNn n 2 (cid:112)θ2nlog(Nn) + 3nA1/˜c + (cid:114) 1  1  + A1  + 9)  nlogNn  logNn n (cid:114)  ≤  1 2  R( (cid:98)f , f0)  F n (θ + A1 + 9)F δ.  (θ2 +  3A1 ˜c  )nlogNn +  (θ + A1)F logNn n  +  (cid:114) n  (cid:19)  logNn θF logNn n  + 9δF  +  A1F n  +  (21)  Since there exists some constant such that (θ+A1+9)F be simpliﬁed as  log2n  ∨ θ2+3A1/˜c  log4n ≤ CF , where CF depends on F and c, eq(21) can  |R( (cid:98)f , f0) − (cid:98)Rn( (cid:98)f , f0)| ≤  (cid:113)  1 2  R( (cid:98)f , f0)  F n  CF nlog4nlogNn + CF  log2nlogNn n  +  CF δlog2n.  From eq(43) in [17], we know that for positive real numbers a, b, c, d being such that |a − b| ≤ 2 0 < (cid:15) < 1,  √  ac + d, then for any  (1 − (cid:15))b − d −  c2 (cid:15)  ≤ a ≤ (1 + (cid:15))(b + d) +  (1 + (cid:15))2 (cid:15)  c2.  (22)  Using (22) with a = R( (cid:98)f , f0) and b = (cid:98)R( (cid:98)f , f0), we can derive the following bounds for R( (cid:98)f , f0) from (21):  15  (1 − (cid:15)) (cid:98)Rn( (cid:98)f , f0) − CF  ≤ R( (cid:98)f , f0) ≤  log2nlogNn n  − CF δlog2n −  1 (cid:15)  CF F 2log4nlogNn n  (1 + (cid:15)) (cid:98)Rn( (cid:98)f , f0) + (1 + (cid:15))  CF  (cid:18)  log2nlogNn n  (cid:19)  + CF δlog2n  +  (1 + (cid:15))2 (cid:15)  CF F 2log4nlogNn n  .  (II): Similar to the proof of (I), there exists a random j∗ such that (cid:107)fj∗ − ˜f (cid:107)∞ ≤ δ. We have |E[(cid:80)n fj∗ (Xi))]| ≤ δE[(cid:80)n  i=1 |(cid:15)i|] ≤ nδ. Since E[(cid:15)if0(Xi)] = E[E[(cid:15)if0(Xi)|Xi]] = 0, we also ﬁnd  i=1 (cid:15)i( ˜f (Xi) −  (23)  | and U2 =  (cid:12) (cid:12) (cid:12) (cid:12)  E[  2 n  n (cid:88)  i=1  (cid:12) (cid:12) ˜f (Xi)] (cid:12) (cid:12)  (cid:15)i  =  (cid:12) (cid:12) (cid:12) (cid:12)  E[  2 n  ≤ 2δ +  n (cid:88)  (cid:15)i( ˜f (Xi) − f0(Xi))]  (cid:12) (cid:12) (cid:12) (cid:12)  i=1  (cid:12) (cid:12) E (cid:12) (cid:12)  2 n  n (cid:88)  i=1  (cid:15)i(fj∗ (Xi) − f0(Xi))  (cid:12) (cid:12) (cid:12) (cid:12)  .  Recall that γj := (cid:112)n−1logNn ∨ E1/2[(fj(X) − f (X))2] and the deﬁnition of γ∗ is  (cid:112)  (cid:112)  γ∗ =  ≤  n−1logNn ∨ E1/2[(fj∗ (X) − f0(X))2|{(Xi, Yi)}∞ n−1logNn + E1/2[( ˜f (X) − f0(X))2|{(Xi, Yi)}∞  i=1] i=1] + δ.  Using Cauchy-Schwarz inequality E[U2T2] ≤ E1/2[U 2 E1/2[( ˜f (X) − f0(X))2|{(Xi, Yi)}∞  i=1], we have that  2 ]E1/2[T 2  2 ] with T2 = maxj| (cid:15)j (fj (Xi)−f0(Xi))  γj  (cid:12) (cid:12) E (cid:12) (cid:12)  n (cid:88)  i=1  (cid:15)i(fj∗ (Xi) − f0(Xi))  (cid:12) (cid:12) (cid:12) (cid:12)  (cid:12) (cid:12) = E (cid:12) (cid:12)  (cid:80)n  i=1 (cid:15)i(fj∗ (Xi) − f0(Xi)) γ∗  γ∗  (cid:12) (cid:12) (cid:12) (cid:12)  (cid:112)  ≤ E[|T2( ≤ E[T2](δ +  n−1logNn + E1/2[( ˜f (X) − f0(X))2|{(Xi, Yi)}∞  i=1] + δ)|]  (cid:112)  n−1logNn) + E1/2[T 2  2 ]E1/2[( ˜f (X) − f0(X))2].  Notice that E1/2[( ˜f (X) − f0(X))2] = R1/2( (cid:98)f , f0). Now, using eq(23), eq(24),  (cid:12) (cid:12) E (cid:12) (cid:12)  2 n  n (cid:88)  i=1  ˜f (Xi)  (cid:15)i  (cid:12) (cid:12) (cid:12) (cid:12)  ≤ 2δ +  2 n  E[T2](δ +  (cid:112)  n−1logNn) +  2 n  E1/2[T 2  2 ]R1/2( ˜f , f0).  (24)  (25)  γj  . From γj ≥ (cid:112)n−1logNn and γj ≥ E1/2[(fj(X) − f (X))2], we know that (1) |Zij| ≤ Let Zij := fj (Xi)−f0(Xi) 2F/(cid:112)n−1logNn; (2) EZ 2 ij ≤ 1. Observe that {(cid:15)iZij}i≥1 is a martingale difference sequence with respect to the ﬁltration Ft = σ((Xi), i ≤ t). Then we check the moment conditions in theorem 1.2B in [31]. Since (cid:15)i and Fi are independent, E((cid:15)2  ijσ2. For the m-th moment of |(cid:15)iZij| given Fi, we have that  ij|Fi) = Z 2  i Z 2  E[|(cid:15)m  ij |(cid:12) i Z m  (cid:12)Fi] ≤ σ2m!cm−2  0  |Zij|m ≤ σ2Z 2  ijm!  (cid:32)  2c0F (cid:112)n−1logNn  (cid:33)m−2  .  16  We split P(| (cid:80)n  i=1 (cid:15)iZij| ≥ x) into two parts:  P(|  n (cid:88)  i=1  (cid:15)iZij| ≥ x) ≤ P(|  (cid:124)  n (cid:88)  i=1  (cid:15)iZij| ≥ x, σ2  n (cid:88)  Z 2  ij ≤  i=1 (cid:123)(cid:122) a(x)  (cid:114) n  logNn  x)  +  (cid:125)  Z 2  ij ≥  n (cid:88)  i=1  P(  (cid:124)  (cid:114) n  logNn  x).  (cid:125)  1 σ2 (cid:123)(cid:122) b(x)  Using theorem 1.2B in [31] with c = 2c0F/(cid:112)(n−1)logNn, Vn = σ2 (cid:80)n  i=1 Z 2  ij, y = (cid:112)n−1logNnx, we obtain    a(x) ≤ exp  −  x  (2 + 2c0F )  (cid:113) n  logNn     .  (26)  (27)  To simplify the notation, we let v2 (fj(Xi) − f0(Xi))2 ≤ 4F 2 and 1/γ2  j = Var(Z 2 ij) and ˜v2 j ≤ n. From lemma 2, we have  j = Var (cid:0)Z 2  ij  (cid:1) + 2 (cid:80)  k>i Cov  (cid:16)  Z 2  ij, Z 2 kj  (cid:17)  . We know that  j ≤ ([24log(n)/c] + 3)v2 ˜v2  j + 22(16F 4)  1 n2(exp(c/3) − 1)  .  j ≤ EZ 4  Since v2 j ≤ C1F 2nlog(n)/logNn + C2F 4/n2, where C1 and C2 only depend on c. Since EZ 2 ˜v2 P((cid:80)n x − n). Observe that |Z 2 α-mixing process, using Bernstein inequality (Theorem 2, [30]), again we have that  ij ≤ E(fj(Xi) − f0(Xi))4/(n−1logNnE(fj(Xi) − f0(Xi))2) ≤ 4F 2n/logNn, we derive that ij ≤ 1, we have b(x) ≤ ij}i is an exponentially  ij| ≤ 4F 2n/logNn and {Z 2  ij − EZ 2  ij − EZ 2  ij) ≥ 1 σ2  i=1(Z 2  (cid:113) n  logNn    b(x) ≤ exp  −    ≤ exp  −    ≤ exp  −  C3( 1 σ2  (cid:113) n  logNn  x − n)2      n˜v2  j + 16n2F 4 logNn  + ( 1 σ2  x − n) nlog2n logNn  (cid:113) n  logNn (cid:113) n  C3( 1 σ2 n + 16n2F 4 logNn (cid:113) n  C3( 1 σ2  logNn  C1F 2n2logn logNn  + C2F 4  (C1F 2+C2F 4+16F 4)n2logn logNn  + ( 1 σ2  x − n)2 (cid:113) n  logNn  x − n) nlog2n logNn   .  x − n)2  logNn  + ( 1 σ2  (cid:113) n  logNn      x − n) nlog2n logNn   (28)  The last inequality uses the assumption that n ≥ logNn. Next, we use eq(26), eq(27), eq(28) to estimate the upper bound for ET2 and ET 2 over all j = 1, · · · , Nn, we have  2 in eq(25). Taking the union bound  Therefore we ﬁnd that for all θ  P(T2 ≥ x) ≤ 1 ∧ 2Nn(a(x) + b(x)).  ET2 ≤ θ(cid:112)nlogNn + 2Nn  (cid:90) ∞  √  θ  nlogNn  17  a(x)dx + 2Nn  (cid:90) ∞  √  θ  nlogNn  b(x)dx.  (29)  From eq(27), we have  2Nn  (cid:90) ∞  √  θ  nlogNn  a(x)dx ≤ 2Nn  When θ ≥ 2 + 2c0F , it follows that  (cid:18)  (2 + 2c0F )  (cid:114) n  logNn  (cid:19)  (cid:18)  exp  −  θlogNn 2 + 2c0F  (cid:19)  .  (30)  2Nn  (cid:90) ∞  √  θ  nlogNn  (cid:18)  a(x)dx ≤ 2  (2 + 2c0F )  (cid:114) n  logNn  (cid:19)  .  From eq(28), we have that for θ > σ2  (cid:90) ∞  √  θ  nlogNn  b(x)dx  ≤  ≤  (cid:90) ∞  √  θ  nlogNn  (cid:90) ∞  √  θ  nlogNn    exp  −    exp  −  C3( 1 σ2  (cid:113) n  logNn  x − n)2 (cid:113) n     dx  x − n) nlog2n logNn  (C1F 2+C2F 4+16F 4)n2logn logNn  + ( 1 σ2  C3( 1 σ2  (cid:113) n  logNn  x − n)  (C1F 2+C2F 4+16F 4)nlogn (θ/σ2−1)logNn  logNn    dx  + nlog2n logNn (cid:32)  exp  −  ≤  (cid:18) (C1F 2 + C2F 4 + 16F 4) (θ/σ2 − 1)  + 1  (cid:19) σ2√ √ C3  nlog2n logNn  C3(θ/σ2 − 1)logNn  C1F 2+C2F 4+16F 4 θ/σ2−1  logn + log2n  (cid:33)  .  When θ ≥ σ2(1 + 2log2n/C3 ∨ (cid:112)(2C1F 2 + 2C2F 4 + 32F 4)logn/C3),  (cid:32)  exp  −  C3(θ/σ2 − 1)logNn  C1F 2+C2F 4+16F 4 θ/σ2−1  logn + log2n  (cid:33)  . ≤  1 Nn  Therefore,  2Nn  (cid:90) ∞  √  θ  nlogNn  b(x)dx ≤ (C1F 2 + C2F 4 + 16F 4)σ2  (cid:114) n  logNn  +  2σ2log2n C3  (cid:114) n  logNn  .  (31)  We choose θ = (2 + 2c0F ) ∨ σ2(1 + 2log2n/C3 ∨ (cid:112)(2C1F 2 + 2C2F 4 + 32F 4)logn/C3). Combining eq(29), eq(30) and eq(31) gives  ET ≤ C(F, σ2, c)(cid:112)nlogNnlog2n,  where C(F, σ2, c) is a constant depending on F, σ2, c.  Similar to eq(29), we can prove that  ET 2  2 ≤ θ2nlogNn + 2Nn  (cid:90) ∞  √  a(  x)dx + 2Nn  (cid:90) ∞  √  b(  x)dx.  θ2nlogNn  θ2nlogNn  (32)  (33)  We still choose θ = (2 + 2c0F ) ∨ σ2(1 + 2log2n/C3 ∨ (cid:112)(2C1F 2 + 2C2F 4 + 32F 4)logn/C3). Using the fact that u − c)a)du = 2(b/a + 1/a2)exp(−a(b − c)), we can estimate the upper for both (cid:82) ∞ (cid:82) ∞ x)dx b2 exp(−( and (cid:82) ∞ b(  x)dx. From eq(27), we have  θ2nlogNn  a(  √  √  √  θ2nlogNn  18  (cid:90) ∞  √  a(  x)dx ≤  (cid:90) ∞  θ2nlogNn  θ2nlogNn    exp  −  √  x (cid:113) n  logNn  (2 + 2c0F )     dx  (cid:18)  = 2  θn(2 + 2c0F ) + (2 + 2c0F )2 n  logNn  (cid:19)  (cid:32)  exp  −  ≤ C(F, c)  nlog2n Nn  .  √  θ  nlogNn (2 + 2c0F )(cid:112)n/logNn  (cid:33)  From eq(28), we have  (cid:90) ∞  √  b(  x)dx ≤  (cid:90) ∞  θ2nlogNn  θ2nlogNn  (cid:90) ∞  ≤  θ2nlogNn    exp  −    exp  −  C3( 1 σ2  (cid:113) n  logNn  √  x − n)2 (cid:113) n     dx  √  x − n) nlog2n logNn  (C1F 2+C2F 4+16F 4)n2logn logNn  + ( 1 σ2  C3( 1 σ2  (cid:113) n  √  logNn  x − n)  (C1F 2+C2F 4+16F 4)nlogn (θ/σ2−1)logNn  logNn    dx  + nlog2n logNn C1F 2 + C2F 4 + 16F 4 θ/σ2 − 1  (cid:33)  (cid:21)  + 1)2 nlog4n logNn  (cid:20)  ≤ 2  θ(  C1F 2 + C2F 4 + 16F 4 θ/σ2 − 1 (cid:32)  + 1)nlog2n + (  exp  −  C3(θ/σ2 − 1)logNn  C1F 2+C2F 4+16F 4 θ/σ2−1  logn + log2n  ≤ C(F, σ2, c)nlog4n/Nn.  Therefore, eq(33) gives  With eq(25) and upper bound for ET2, ET 2 2 ,  ET 2  2 ≤ C(F, σ2, c)nlog4nlogNn.  (cid:12) (cid:12) E (cid:12) (cid:12)  2 n  n (cid:88)  i=1  ˜f (X)  (cid:15)i  (cid:12) (cid:12) (cid:12) (cid:12)  ≤ 2δ +  2 n  C(F, σ2, c)(cid:112)nlogNnlog2n(δ +  (cid:112)  n−1logNn)  (cid:113)  +  2 n  C(F, σ2, c)nlog4nlogNnR1/2( ˜f , f0)  (34)  ≤ δC(F, σ2, c)log2n + C(F, σ2, c)  logNn n  log2n  (cid:113)  +  2 n  C(F, σ2, c)nlog4nlogNnR1/2( ˜f , f0).  From the upper bound of inequality in (I), setting (cid:15) = 1 gives  (cid:113)  R( ˜f , f0) ≤  (cid:113)  (cid:115)  2 (cid:98)Rn( ˜f , f0) +  2CF  log2nlogNn n  +  (cid:113)  2CF δlog2n  (cid:115)  +  4CF F 2log4nlogNn n  (cid:113)  ≤  (cid:115)  2 (cid:98)Rn( ˜f , f0) +  CF  log4nlogNn n  (cid:113)  +  CF δlog2n.  19  Therefore,  (cid:113)  2 n  C(F, σ2, c)nlog4nlogNnR1/2( ˜f , f0) ≤  (cid:115)  C(F, σ2, c)log4nlogNn n  + C(F, σ2, c)  log4nlogNn n  (cid:98)R1/2  n ( ˜f , f0) (cid:115)  + 2  δCF log6nlogNn n  (35)  .  (cid:113)  Using the fact that 2 eq(35).  δCF log6nlogNn/n ≤ δlog2n + CF log4nlogNn/n, then inequality (II) follows from eq(34),  (III): For any ﬁxed f ∈ F, E[ 1 n being deterministic, we have E[(cid:107)f − f0(cid:107)2  (cid:80)n  i=1(Yi − (cid:98)f (Xi))2] ≤ E[ 1  n  (cid:80)n  i=1(Yi − f (Xi))2] + ∆n. Because of Xi  D= X and f  n] = E(f (X) − f0(X))2. Since also E[(cid:15)if (Xi)] = 0,  (cid:98)Rn( (cid:98)f , f0) ≤ E[(cid:107)f − f0(cid:107)2  n] + E[  2 n  n (cid:88)  i=1  (cid:15)i (cid:98)f (Xi)] + ∆n  ≤ E[(cid:107)f − f0(cid:107)2  n] + ∆n + δC(F, σ2, c)log2n + C(F, σ2, c)log4n  + 2C(F, σ2, c)  (cid:115)  log4nlogNn n  (cid:98)R1/2( (cid:98)f , f0).  logNn n  (36)  Observe that  (cid:115)  2C(F, σ2, c)  log4nlogNn n  (cid:98)R1/2( (cid:98)f , f0) ≤  1 + (cid:15) (cid:15)  C 2(F, σ2, c)log4nlogNn/n  +  (cid:15) 1 + (cid:15)  (cid:98)R( (cid:98)f , f0).  Combining eq(36) with eq(37) and rearranging (cid:98)R( (cid:98)f , f0) to one side give inequality (III).  (IV): Let ˜f ∈ argminf ∈F  (cid:80)n  i=1(Yi − f (Xi))2 be an empirical risk minimizer. We have  (cid:98)Rn( (cid:98)f , f0) − (cid:98)Rn( ˜f , f0)  = ∆n + 2E  (cid:34)  1 n  n (cid:88)  i=1  (cid:35)  (cid:15)i (cid:98)f (Xi)  − 2E  (cid:34)  1 n  n (cid:88)  i=1  (cid:35) ˜f (Xi)  (cid:15)i  ≥ ∆n − 2δC(F, σ2, c)log2n − 2C(F, σ2, c)log4n  logNn n  (cid:115)  (cid:115)  − 2C(F, σ2, c)  log4nlogNn n  Observe that  (cid:98)R1/2  n ( (cid:98)f , f0) − 2C(F, σ2, c)  log4nlogNn n  (cid:98)R1/2  n ( ˜f , f0).  (cid:115)  2C(F, σ2, c)  (cid:115)  2C(F, σ2, c)  log4nlogNn n  log4nlogNn n  (cid:98)R1/2( (cid:98)f , f0) ≤ C 2(F, σ2, c)  log4nlogNn n  + (cid:98)R( (cid:98)f , f0)  (cid:98)R1/2  n ( ˜f , f0) ≤  1 − (cid:15) (cid:15)  C 2(F, σ2, c)  log4nlogNn n  +  (cid:15) 1 − (cid:15)  (cid:98)Rn( ˜f , f0).  Inequality (IV) follows from eq(38), eq(39).  20  (37)  (38)  (39)  lemma 4. Consider the d-variate nonparametric regression model Yi = f0(Xi) + (cid:15)i with unknown regression function f0, satisfying (cid:107)f0(cid:107)∞ ≤ F for some F ≥ 1. Let (cid:98)fn be any estimator taking values in the class F(L, p, s, F ) and ∆n is deﬁned in lemma 3. Under Assumptions 1-3, for any (cid:15) ∈ (0, 1], there exists a constant C(cid:15), only depending on (cid:15), such that with  γ(cid:15),n := C(cid:15)F 2 (s + 1)log(n(s + 1)L)p0pL+1log4n  n  ,  (1 − (cid:15))2∆n( (cid:98)fn, f0) − γ(cid:15),n (cid:18)  ≤ R( (cid:98)fn, f0) ≤ (1 + (cid:15))2  inf f ∈F (L,p,s,F )  (cid:107)f − f0(cid:107)2  ∞ + ∆n( (cid:98)fn, f0)  (cid:19)  + γ(cid:15),n  Proof. Lemma 4 follows from Lemma 3 with the choice of δ = 1/n, F = F(L, p, s, ∞) and Remark 1 in [17]  logN (δ, F(L, p, s, ∞), (cid:107) · (cid:107)∞) ≤ (s + 1)log (cid:0)22L+5δ−1(L + 1)p2  0p2  L+1s2L(cid:1)  A.2 Proof of Theorem 1  Proof of theorem 1. Combining lemma 4 with the assumed bounds on L and s, it follows that  1 4 4  ∆n( (cid:98)fn, f0) − C  (cid:48)  φnLlog6n ≤ R( (cid:98)f , f0)  inf f ∗∈F (L,p,s,F )  (cid:107)f ∗ − f0(cid:107)2  ∞ + 4∆n( (cid:98)fn, f0) + C  (cid:48)  φnLlog6n,  (40)  where we used (cid:15) = 1/2 for the lower bound and (cid:15) = 1 for the upper bound. Let C = 8C whenever ∆n( (cid:98)f , f0) ≥ CφnLlog6n. The lower bound on (7) is proved.  (cid:48)  , then C  (cid:48)  φnLlog6n ≤ (cid:98)R( (cid:98)f , f0)  To get the upper bound, we need to control inf f ∗∈F (L,p,s,F ) (cid:107)f ∗ − f0(cid:107)2  ∞. From eq(26) in [17], we know that  inf f ∗∈F (L,p,s)  (cid:107)f ∗ − f0(cid:107)2  ∞ ≤ C  (cid:48)  max i=0,··· ,q  N −  2β∗ i ti ≤ C  (cid:48)  max i=0,··· ,q  2β∗ i ti  β∗ i 2β∗ +ti . i  −  n  − 0  C  Hence, inf f ∗∈F (L,p,s,F ) (cid:107)f ∗−f0(cid:107)2 ∞ ≤ C1φn. Deﬁne f ∗ = ˜f ((cid:107)f0(cid:107)∞/(cid:107) ˜f (cid:107)∞). Then (cid:107)f ∗(cid:107)∞ ≤ (cid:107)f0(cid:107)∞ = (cid:107)gq(cid:107)∞ ≤ K ≤ F , which implies that ˜f ∈ F(L, p, s, F ). Writing f ∗ − f0 = (f ∗ − ˜f ) + ( ˜fn − f0), we obtain (cid:107)f ∗ − f0(cid:107)∞ ≤ 2(cid:107) ˜f − f0(cid:107)∞ ≤ 2C1φ0. From eq(40), we obtain that  ∞ ≤ C1φn. Therefore, there exists ˜f ∈ F(L, p, s, F ) such that (cid:107)f ∗−f0(cid:107)2  (cid:48)  φnLlog6n.  (41)  R( (cid:98)f , f0) ≤ 8C1φ0 + 4∆n( (cid:98)fn, f0) + C  The upper bounds on eq(6) and eq(7) follow from eq(41).  A.3 Proof of Theorem 2  Proof of Theorem 2. From lemma 3, we know that  (1 − (cid:15))2∆n − C(F, σ2, c)  log4nlogNn n(cid:15)  − δC(F, σ2, c)log2n  ≤ R( (cid:98)f , f0) ≤  (1 + (cid:15))2  (cid:18)  inf f ∗∈F  (cid:107)f ∗ − f0(cid:107)2  ∞ + ∆n( (cid:98)f , f ) + C(F, σ2, c)δlog2n  (cid:19)  +  (1 + (cid:15))3 (cid:15)  C(F, σ2, c)  log4nlogNn n  .  21  Let F := F(L, p, s, ∞). From remark 1 in [17],  logN (δ, F(L, p, s, ∞), (cid:107) · (cid:107)∞) ≤ (s + 1)log (cid:0)22L+5δ−1(L + 1)p2  0p2  L+1s2L(cid:1) .  According to the assumption s (cid:16) dL and d (cid:46) p with the choice of δ = 1 using the result of lemma 3 with (cid:15) = 1  n , we have that logNn (cid:16) dLlogn. Therefore,  2 , we have that (cid:48) dLlog5n n  ∆n − C  1 4  ≤ R( (cid:98)f , f0) ≤  4 inf f ∗∈F  (cid:107)f ∗ − f0(cid:107)2  ∞ + 4∆n( (cid:98)f , f ) + C  (cid:48) dLlog5n n  .  (42)  1 The lower bound of eq(11) can be derived from the left side of eq(42) with the assumption d (cid:16) n α+1 . To derive the upper bound of eq(10) and eq(11), we need the upper bound of inf f ∗∈F (L,p,s,F )(cid:107)f ∗ − f0(cid:107)∞. Let ˜f = (cid:80)d i=1 φiYt−i ∈ F(3, (d, 2Kd, 1), s), s = 4Kd (For each Yt−i in input layer, it maps to 2(cid:100)φi(cid:101) units in hidden layer. (cid:100)φi(cid:101) units are φi(Yt−i)+/(cid:100)φi(cid:101) and other (cid:100)φi(cid:101) units are −φi(−Yt−i)+/(cid:100)φi(cid:101). Assumption φi < K implies that there are at most 2Kd units in hidden layer. The output ˜f equals to the summation of all hidden units). We have that  (cid:107) ˜f − f0(cid:107)∞ = (cid:107)  ∞ (cid:88)  i=d+1  φiYt−i(cid:107)∞ ≤ K  ∞ (cid:88)  d+1  |φi| (cid:46)  1 (d + 2)α .  (43)  Deﬁne f ∗ = ˜f ((cid:107)f0(cid:107)∞/(cid:107) ˜f (cid:107)∞). Then (cid:107)f ∗(cid:107)∞ ≤ (cid:107)f0(cid:107)∞ ≤ K (cid:80)∞ i=1 |φi| ≤ KM/2α ≤ F . Therefore, f ∗ ∈ F(4, p∗, 4Kd + 1, F ). To extend the layer of neural network from 4 to L, we can add (L − 4) additional identical layers before F(4, p, (6K + 2)d, F ). Note that the dimension of input vector is d, the deepened network belongs to F(L, (d, · · · , d, p∗), (4K + L)d + 1, F ).  Therefore, the neural network f ∗ ⊂ F(L, p, s, F ). Writing f ∗ − f0 = (f ∗ − ˜f ) + ( ˜fn − f0), we obtain (cid:107)f ∗ − f0(cid:107)∞ ≤ 2(cid:107) ˜f − f0(cid:107)∞. With eq(42) and eq(43), we have that  R( (cid:98)f , f0) ≤ C  1 dα + 4∆n( (cid:98)f , f ) + C  (cid:48) dLlog5n n  .  (44)  Using the assumption d (cid:16) n  1 α+1 again, we can derive the upper bound of eq(10) and eq(11).  B Additional details on numerical experiments  Computer information: Processor: Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz 2.59 GHz Installed RAM: 16.0 GB (15.9 GB usable) System type: Windows 10 Home 64-bit operating system, x64-based processor Disk: Samsung mzvlb512hbjq-000h1 GPU: NVIDIA GeForce GTX 1660 Ti  Additional ﬁgure in Section 5: Figure 3.  C Numerical comparison between DNN and LSTM  In this section, we compare the predicting performance of DNN with LSTM. We consider the same set of models as in section 4.2. The input dimension of both DNN and LSTM are determined by AIC and the number of hidden layer units are set to be 10. The results are shown in ﬁgure 4. As we can see from this ﬁgure, DNN has a slightly faster convergence rate than LSTM in linear AR models while for non-linear AR models, LSTM performs slightly better. In fact, the difference between the results of DNN and LSTM in these simulation settings are not statistically signiﬁcant.  22  (a) γ = 0  (b) γ = 5  (c) γ = 7.5  (d) γ = 10  Figure 3: Estimation for monthly inﬂation rate. The blue line reﬂects the true change of inﬂation rate from 2002 Feb to 2011 Nov. Other two lines correspond to two estimators.  (a) model (1)  (b) model (2)  (c) model (3)  (d) model (4)  Figure 4: : Box plots of logarithm of the mean square error on the testing set as a function of sample size.  23\n",
      "2 2 0 2  t c O 3 1  ]  G L . s c [  1 v 9 9 6 6 0 . 0 1 2 2 : v i X r a  Parameter-Efﬁcient Masking Networks  Yue Bai1,∗ Huan Wang1,4 Xu Ma1 Yitian Zhang1 Zhiqiang Tao3 Yun Fu1,2,4 1Department of Electrical and Computer Engineering, Northeastern University 2Khoury College of Computer Science, Northeastern University 3School of Information, Rochester Institute of Technology 4AInnovation Labs, Inc. Project Homepage: https://yueb17.github.io/PEMN  Abstract  A deeper network structure generally handles more complicated non-linearity and performs more competitively. Nowadays, advanced network designs often contain a large number of repetitive structures (e.g., Transformer). They empower the network capacity to a new level but also increase the model size inevitably, which is unfriendly to either model restoring or transferring. In this study, we are the ﬁrst to investigate the representative potential of ﬁxed random weights with limited unique values by learning diverse masks and introduce the Parameter-Efﬁcient Masking Networks (PEMN). It also naturally leads to a new paradigm for model compres- sion to diminish the model size. Concretely, motivated by the repetitive structures in modern neural networks, we utilize one random initialized layer, accompanied with different masks, to convey different feature mappings and represent repetitive network modules. Therefore, the model can be expressed as one-layer with a bunch of masks, which signiﬁcantly reduce the model storage cost. Furthermore, we enhance our strategy by learning masks for a model ﬁlled by padding a given random weights vector. In this way, our method can further lower the space com- plexity, especially for models without many repetitive architectures. We validate the potential of PEMN learning masks on random weights with limited unique values and test its effectiveness for a new compression paradigm based on different network architectures. Code is available at https://github.com/yueb17/PEMN.  1  Introduction  Deep neural networks have emerged in several application ﬁelds and achieved state-of-the-art perfor- mances [9, 18, 34]. Along with the data explosion in this era, huge amount of data gathered to build network models with higher capacity [4, 8, 30]. In addition, researchers also pursue a uniﬁed network framework to deal with multi-modal and multi-task problems as a powerful intelligent model [30, 42]. All these trending topics inevitably require even larger and deeper network models to tackle diverse data ﬂows, arising new challenges to compress and transmit models, especially for mobile systems.  Despite the success of recent years with promising task performances, advanced neural networks suffer from their growing size, which causes inconvenience for both model storage and transferring. To reduce the model size of a given network architecture, neural network pruning is a typical technique [26, 24, 13]. Pruning approaches remove redundant weights using designed criteria and the pruning operation can be conducted for both pretrained model (conventional pruning: [14, 13]) and randomly initialized model (pruning at initialization: [25, 37]). Another promising direction is to obtain sparse network by dynamic sparse training [10, 29]. They jointly optimize network architectures and weights to ﬁnd good sparse networks. Basically, these methods commonly demand regular training, and the ﬁnal weights are updated by optimization algorithms like SGD automatically.  Corresponding author: bai.yue@northeastern.edu  36th Conference on Neural Information Processing Systems (NeurIPS 2022).  Now that the trained weights have such a great representative capacity, one may wonder what is the potential of random and ﬁxed weights or is it possible to achieve the same performance on random weights? If we consider a whole network, the answer is obviously negative as a random network cannot provide informative and distinguishable outputs. However, picking a subnetwork from a random dense network make it possible as feature mapping varies with changes of subnetwork structures. Then, the question has been updated as what is the representative potential of random and ﬁxed weights with selecting subnetwork structures? Pioneer work LTH [11] shows the winning ticket exists in random network with good trainability but cannot be used directly without further training. Supermasks [44] enhances the winning ticket and enable it being usable directly. Recent work Popup [31] signiﬁcantly improves subnetwork capacity from its dense counterpart by learning the masks using backpropagation. Following this insightful perspective, we further ask a question – what is the maximum representative potential of a set of random weights? In our work, we ﬁrst make a thorough exploration of this scientiﬁc question to propose our Parameter-Efﬁcient Masking Networks (PEMN). Then, leveraging on the PEMN, we naturally introduce a new network compression paradigm by combining a set of ﬁxed random weights with a corresponding learned mask to represent the whole network.  We start with network architectures which recent popular design style, i.e., building a small-scale encoding module and stacking it to obtain a deep neural network [9, 33, 32]. Based on this point, we naturally propose the One-layer strategy by using one module as a prototype and copy its parameter into other repetitive structures. More generally, we further provide two versions: max-layer padding (MP) and random weight padding (RP) to handle diverse network structures. Speciﬁcally, MP chooses the layer with the most number of parameters as the prototype and uses ﬁrst certain parameters of prototype to ﬁll in other layers. RP even breaks the constraint of network architecture. It samples a random vector with certain length as the prototype which is copied several times to ﬁll in all layers based on their different lengths. RP is architecture-agnostic and can be seen as a most general strategy in our work. Three strategies are from speciﬁc to general manner and reduce the number of unique parameters gradually. We ﬁrst employ these strategies to randomly initialize network. Then, we learn different masks to explore the random weights potential and positively answer the scientiﬁc question above. Leveraging on it, we propose a new network compression paradigm by using a set of random weights with a bunch of masks to represent a network model instead of restoring sparse weights for all layers (see Fig. 1). We conduct comprehensive experiments to explore the random weights representative potential and test the model compression performance to validate our paradigm. We summarize our contributions as below:  We scientiﬁcally explore the representative potential of ﬁxed random weights with lim- ited unique values and introduce our Parameter-Efﬁcient Masking Networks (PEMN). It leverages on learning different masks to represent different feature mappings.  A novel network compression paradigm is naturally proposed by fully utilizing the repre- sentative capacity of random weights. We represent and restore a network based on a given random vector with a bunch of masks instead of retaining all the sparse weights.  Extensive experimental results explore the random weights potential by using our PEMN and test the compression performance of our new paradigm. We expect our work can inspire more interesting explorations in this direction.  2 Related Works  2.1 Sparse Network Training  Our work is related to sparse network training. Conventional pruning techniques ﬁnetune the pruned network from pretrained models [13, 14] with various pruning criteria for different applications [26, 15, 16, 38, 36, 40, 41]. Instead of pruning a pretrained model, pruning at initialization [37] approaches attempt to ﬁnd winning ticket from the random weight. Gradient information is considered to build pruning criteria in [25, 35]. Different from pruning methods above, sparse network training also can be conducted in a dynamic fashion. To name a few, Rigging the Lottery [10] edits the network connections and jointly updates the learnable weights. Dynamic Sparse Reparameterization [29] modiﬁes the parameter budget among the whole network dynamically. Sparse Networks from Scratch [7] proposes a momentum based approach to adaptively grow weights and empirically  2  Figure 1: Comparison of different ways to represent a neural network. Different features mappings are shown as blue rectangles. Squares with different color patches inside serve as parameters of different layers. Left is the conventional fashion where weights are optimized and sparse structures are decided by certain criteria. Right is our PEMN to represent a network where the prototype weights are ﬁxed and repetitively used to ﬁll in the whole network and different masks are learned to deliver different feature mappings. Following this line, we explore the representative potential of random weights and propose a novel paradigm to achieve model compression by combining a set of random weights and a bunch of masks.  veriﬁes its effectiveness. Most of the sparse network training achieve the network sparsity by keeping necessary weights and removing others, which reduces the cost of model storage and transferring. In our work, we propose a novel model compression paradigm by leveraging the representative potential of random weights accompanied with subnetwork selection.  2.2 Random Network Selection  Our work inherits the research line of exploring the representative capacity of random network. The potential of randomly initialized network is pioneeringly explored by the Lottery Ticket Hypoth- esis [11], and further investigated by [28, 2, 39]. It articulates that there exists a winning ticket subnetwork in a random dense network. This subnetwork can be trained in isolation and achieves comparable results with its dense counterpart. Moreover, the potential of the winning ticket is further explored in Supermasks [44]. It surprisingly discovers the subnetwork can be identiﬁed from dense network to obtain reasonable performance without training. It extends and proves the potential of subnetwork from good trainability to being used directly. More recently, the representative capacity of subnetworks is enhanced by Popup algorithm proposed by [31]. Based on random dense initial- ization, the learnable mask is optimized to obtain subnetwork with promising results. Instead of considering network with random weights, the network with the same shared parameters can also delivery representative capacity to some extent, which is investigated by Weight Agnostic Neural Network [12] and also inspires this research direction. We are highly motivated by these researches to validate how is the representative potential of random weights with limited unique values by learning various masks.  2.3 Weight Sharing  Our study is also related to several recent works about weight sharing. This strategy has been explored and analyzed in convolutional neural networks for their efﬁciency [19, 43]. In addition, several works are also proposed for efﬁcient transformer architecture using weight sharing strategy [22, 5, 1]. There are two main differences between these works and our study: 1) They follow the regular optimization strategy to learn the weight in a recurrent fashion, which is closer to the recurrent neural network. Our work follows a different setting. We use ﬁxed repetitive random weights to ﬁll in the whole network and employs different masks to represent different feature mappings; 2) They mainly conduct cross-layer weight sharing for repetitive transformer structure. In our work, we explore the potential of random weight vector with limited length as much smaller repetitive granularity to ﬁll in the whole network, which is more challenging than cross-layer sharing strategy.  3  Figure 2: Illustrations of different strategies in PEMN to represent network structures. Compared with regular fashion where all parameters are randomly initialized, we provide three parameter-efﬁcient strategies, One-layer, Max-layer padding (MP), and Random vector padding (RP), to fully explore the representative capacity of random weights.  3 Parameter-Efﬁcient Masking Networks  3.1  Instinctive Motivation  Overparameterized randomly initialized neural network beneﬁts network optimization to get higher performance. Inevitably, the trained network contains redundant parameters but can be further compressed, which deﬁnes the conventional neural network pruning. On the other side, the network redundancy also ensures a large random network contains a huge number of possible subnetworks, thus, carefully selecting a speciﬁc subnetwork should obtain promising performances. This point of view has been proved by [31, 44]. These works demonstrate the representative potential of certain subset combinations of a given random weights. Following this lane, we naturally ask a question: what is the maximum representative potential of a set of random weights? or in another word: can we use random weights with limited unique values to represent a usable network? We answer this question as positive and introduce our Parameter-Efﬁcient Masking Networks (PEMN). Moreover, leveraging on 1) compared with trained network where the weight values cannot be predicted, we can pre-access the random weights before we select the subnetwork; 2) selected subnetwork can be efﬁciently represented by a bunch of masks, we can extremely reduce the network storage size and establish a new paradigm for network compression.  3.2 Sparse Selection  We follow [31] to conduct the sparse network selection. We start from a randomly initialized neural network consisting of L layers. For each l ∈ {1, 2, ..., L}, it has  Il+1 = σ(F[Il; wl]),  (1)  where Il and Il+1 are the input and output of layer l. σ is the activation. F represents the encoding layer such as convolutional or linear layer with parameter wl = {w1 l }, where dl is the parameter dimension of layer l. To perform the sparse selection, all the weights w = {w1, w2, ..., wL} are ﬁxed and denoted as (cid:101)w. To pick the ﬁxed weights for subnetwork, each weight wj l is assigned a learnable element-wise score sj l to indicate its importance in the network. The Eq. 1 is rewrited as (2)  Il+1 = σ(F[Il; wl (cid:12) h(sl)]),  l , ..., wdl  l , w2  where sl = {s1 l , s2 l } is the score vector and h(·) is the indicator function to create the mask. It outputs 1 when the value of sj l belongs to the top K% highest scores and outputs 0 for others, where K is predeﬁned sparse selection ratio. Through optimizing s with ﬁxed w, a subset of original dense  l , ..., sdl  4  weights is ﬁnally selected. Since h(·) is a non-derivable function, the gradient of each sj l cannot be obtained directly. The straight-through gradient estimator [3] is applied to treat h(·) as identity function during gradient backwards pass. Formally, the gradient of s is approximately computed as  (cid:101)g(sj  l ) =  ∂L ∂ (cid:101)Il+1  ∂ (cid:101)Il+1 ∂sj l  ≈  ∂L ∂Il+1  ∂Il+1 ∂sj l  ,  (3)  where (cid:101)Il+1 = σ(F[Il; wl (cid:12) sl]), which is applied estimation. (cid:101)g(sj l ) is approximately estimated gradient of weight score sj l . In this way, the dense network is randomly initialized but ﬁxed, but one of its subnetwork can be selected using Backpropagation. In our work, we name this optimization process as sparse selection.  3.3 Parameter-Efﬁcient Strategy  Following the logic of parameter-efﬁcient exploration from speciﬁc to general scenario, PEMN utilizes three strategies to construct the whole network based on given random weights: 1) One-layer (Sec. 3.3.1), 2) Max-layer padding (MP) (Sec. 3.4), and 3) Random vector padding (RP) (Sec. 3.4), which are detailedly introduced below.  3.3.1 One-Layer  Sparse selection initializes a dense network as a pool to pick certain weights. It provides a novel di- rection to ﬁnd admirable subnetwork without pre- training and pruning. However, with increasing of network scales, the cost of restoring and transfer- ing a neural network grows rapidly. Noticed that more popular network structures follow a similar design style: proposing a well-designed model- ing block and stacking it several times to boost network capacity, we are inspired to explore the feasibility of ﬁnding subnetworks by iteratively selecting different masks in a series of repetitive modules. Formally, a L-layer randomly initial- ized network NL can be represented as a series of parameters:  NL : w = [w1, w2, ..., wL]; wl ∈ Rl,  (4)  Algorithm 1 One-Layer  1: Input: A random network with L-layer:  w = {w1, w2, ..., wL}  2: Output: A L-layer network ﬁlled by P prototype layers with parameters: w∗ = {wpro1, wpro2 , ..., wproP }  3: Randomly initialize layers from 1 to L 4: Record  parameter  space  dimensions:  {R1, R2, ..., RL}  5: Initialize a prototype layers list: Listpro =  []  if ∀Rprop ∈ Listpro (cid:54)= Rl then Append wl into Listpro  else  6: for l in 1, 2, ..., L do 7: 8: 9: 10: 11: end if 12: 13: end for 14: Return updated w as w∗  Find wprop , where Rprop = Rl Replace wl with wprop  for linear, RN ×H×W  where l ∈ {1, 2, ..., L} and wl is used for various layers. Rl denotes different parameter spaces (e.g., RI×O for CNN layer, where l I/O are input/output dimensions and N /H/W are CNN kernel dimensions). From shallow to deep layer, we ﬁrst sample the prototype layers for the whole network with unique parameter spaces, represented as wpro = [wpro1, wpro2 , ..., wproP ]. For each wprop , we use its parameters to replace its target layers wtarp which share the same parameter space:  l  wtarp  t  ← wprop ; Rtarp  t  = Rprop , t ∈ {1, 2, ..., T p},  (5)  where T p is the number of target layers of prototype p. The whole network ﬁlled by several layers with unique weight size and Eq. 4 can be rewrited as  T 1+T 2+...+T P (cid:122) (cid:125)(cid:124) (cid:123) NL : w∗ = [ wpro1, ..., wpro2, ..., wprop , ..., wproP ];  (cid:88)  T p = L.  (6)  We take a simple 5-layer MLP to clarify this operation: its dimensions are [512, 100, 100, 100, 10] with 4 weight matrices w1 ∈ R512×100, w2 ∈ R100×100, w3 ∈ R100×100, and w4 ∈ R100×10. In this case, w1, w2 and w4 are three prototype layers. w2 has two target layers w3 and itself. w1/w4 only has itself as target layer. The general algorithm is summarized in Alg. 1.  5  In this way, any repetitive modules in a given network structure can be represented by one bunch of random weights. Using the sparse selection strategy, we iteratively pick subnetworks in the same set of random weights to obtain diverse feature mappings. Therefore, the cost to represent the network signiﬁcantly reduces, especially for deep network with many repetitive blocks. In other words, one random layer with different masks can represent the majority of a complete network structure, which is named as one-layer.  3.4 Random Weights Padding  One-layer strategy efﬁciently handles networks with many repetitive modules. The majority of the whole network can be compressed into one random layer with a set of masks. However, in real-world applications, various network architectures may not follow a tidy pattern resulting in different shapes for different layers. Hence, the one-layer is not ﬂexible enough to efﬁciently represent such networks. To handle it, we naturally propose a enhanced strategy, Random Weights Padding. It consists of two versions, Max-layer padding and Random vector padding. We ﬁrst formally rewrite Eq. 4 as  NL : w = [w1, w2, ..., wL]; wl ∈ Rdl, l ∈ {1, 2, ..., L}, (7) where wl is ﬂatten into a vector with dimension dl (e.g., dl = I × O for linear layer and dl = N × H × W for CNN layer).  It chooses the layer wm as the prototype where dm = Max-Layer Padding (MP). max([d1, d2, ..., dL]) is the highest dimension. All other layers in NL have fewer parameters than wm. We keep the prototype as it is and simply pick the ﬁrst dl parameters from wm to replace the parameters in wl, which is described by  wl ← wm[: dl]; l ∈ {1, 2, ..., L}.  (8)  Random Vector Padding (RP). Instead of picking a complete layer as prototype, RP further reduces the granularity of random prototype from a layer to a random weights vector with a relatively short length. We let vpro ∈ Rdv as the random weights vector with length dv. For each layer l, we repeat vpro several times to reach the length of wl. It can be formally described as  (9) After the padding operation, weights in Eq. 7 are reshaped back into the format of Eq. 4 to perform as a network. These two padding strategies are summarized in Alg. 2 and Alg. 3.  wl ← [  dl (cid:122) (cid:125)(cid:124) (cid:123) vpro, ...]; l ∈ {1, 2, ..., L}.  In the series of one-layer, MP, and RP, based on sparse selection, we explore using fewer unique weights to represent the whole network. Leveraging on the property of the ﬁxed weight values, the cost of delegating a network keeps decreasing by using a random vector with a bunch of masks. By this mean, we fully explore the representative capacity of random weights with limited unique values. Furthermore, a novel model compression paradigm can be correspondingly established by restoring a set of random weights with different masks. Our three strategies compared to regular network setting are shown in Fig. 2.  Algorithm 2 Max-Layer Padding  1: Input: A L-layer random network with  parameters: w = {w1, w2, ..., wL}  2: Output:  A L-layer network with MP: w∗ = {wm[: d1], wm[: d2], ..., wm[: dL]}  3: Randomly initialize layers from 1 to L 4: Find the layer wm with the maximum weight dimension dm among all layers wl, l = {1, 2, ..., L}  5: for l in 1, 2, ..., L do 6:  Replace wl with the ﬁrst dl values in wm given by wm[: dl]  7: end for 8: Return updated w as w∗  6  Algorithm 3 Random Vector Padding  1: Input: A L-layer random network with  parameters: w = {w1, w2, ..., wL}  2: Output: A L-layer network with RP: w∗ =  {[  d1 (cid:122) (cid:125)(cid:124) (cid:123) vpro, ...], [  d2 (cid:122) (cid:125)(cid:124) (cid:123) vpro, ...], ..., [ 3: Randomly initialize layers from 1 to L 4: Randomly initialize a weights vector vpro ∈  dL (cid:122) (cid:125)(cid:124) (cid:123) vpro, ...]}  Rdv with dv dimension  Repeat vpro until reaching the length dl Replace wl with the repeated vector vpro  5: for l in 1, 2, ..., L do 6: 7: 8: end for 9: Return updated w as w∗  (a) 6-block ConvMixer with 256/512 dimensions.  (b) 8-block ConvMixer with 256/512 dimensions.  (c) 6-block ViT with 256/512 dimensions.  (d) 8-block ViT with 256/512 dimensions.  Figure 3: Performances of ConvMixer and ViT backbones on CIFAR10 dataset with different model hyperparameters. Y-axis represent the test accuracy and X-axis denotes different network parameter settings. Dense means the model is trained in regular fashion. Mask is the sparse selection strategy. One-layer, MP, and RP are our strategies. The decimal after RP means the number of unique parameters compared with MP. From Mask to RP 1e-5, the unique values of network decrease. Different experimental settings illustrate the representative potential of random weights.  4 Experiments  Our experiments conduct empirically validations on two aspects of our interests. Firstly, we validate how large is the representative potential of random weights with limited unique values to test the effectiveness of our proposed Parameter-Efﬁcient Masking Networks (PEMN). Secondly, leveraging on the pre-accessibility of random weights and lightweight storage cost of binary mask, it is promising to establish a new model compression paradigm.  4.1 Preparation  We comprehensively use several classic or recently popular backbones for image classiﬁcation task to conduct general validations. Backbones include ResNet32, ResNet56 [18], ConvMixer [33], and ViT [9]. We use CIFAR10 and CIFAR100 datasets [21] for our experiments.  4.2 Representative Random Weights in PEMN  We ﬁrst explore the representative potential of random weights in PEMN based on our proposed strategies, One-Layer, Max-Layer Padding (MP), and Random Vector Padding (RP). We use a CNN based architecture Convmixer [33] and a MLP based model ViT [9] to conduct our experiments on CIFAR10 dataset [21].  In Fig. 3, we show 8 pairs of experiments based on 2 backbones (ConvMixer, ViT) using 2 depth numbers (6, 8) and 2 hidden dimensions (256, 512). Each pair includes a dense network performance and a series of results obtained by sparse selection with different random weighting strategies. Speciﬁcally, Mask learns the mask on the randomly initialized network. One-layer, MP, and RP represent our proposed strategies. To simplify the comparison, we use a rate number after RP to show how many unique parameters used in RP compared with MP. From the left Mask to right  7  (a) ResNet32/ResNet56 on CIFAR10 dataset.  (b) Resnet32/ResNet56 on CIFAR100 dataset.  Figure 4: Compression performance validation on CIFAR10/CIFAR100 datasets on ResNet32/ResNet56 backbones. Y-axis denotes the test accuracy. X-axis means the network size compression ratio. Different colors represent different network architectures. The straight lines on the top are performance of dense model with regular training. Lines with different symbol shapes denote different settings. For ResNet, our three points are based on MP, RP 1e-1, and RP 1e-2, respectively. This pair of ﬁgures show that our proposed paradigm achieves admirable compression performance compared with baselines. In very high compression ratios, we can still maintain the test accuracy.  RP 1e-5, the number of unique parameters gradually decreases. Different settings show the similar patterns concluded as: 1) Compared with regular trained dense model, sparse selection approach generally obtains promising results, even if with a performance drop caused by the constraint of ﬁxing all the parameters; 2) From left to right on X-axis, the performance gradually drops. This is caused by the decreasing number of unique values in network, which makes network has less representative capacity; 3) However, performance drop arises when the number of unique parameters is extremely low (e.g., RP 1e-4, RP 1e-5). The results remain stable for the most of random weights strategies; 4) Larger depth and hidden dimension boost the model capacity for different conﬁgurations. The performance drop of random weights strategies from their dense counterpart is also decreased. In addition, ViT shows some unstable ﬂuctuation when fewer unique parameter, compared with ConvMixer with relatively stable patterns. This may caused by the difﬁculty of training MLP based network itself and will not affect our main conclusions.  The performance stability shown above illustrates the network representative capacity can be realized not only by overparameterizing the model, but also carefully picking different combinations of random parameters with limited unique values. In this way, the effectiveness of our PEMN is validated and we can represent a network using a random parameters prototype with different learned masks, instead of typically restoring all the different parameters. This property inspires us to introduce a new model compression paradigm proposed in following section.  4.3 A New Model Compression Paradigm  Practically, our work proposes a new network compression paradigm based on a group of random weights with different masks. We ﬁrst elaborate the network compression and storage processes to clarify our advantages then report the empirical results.  4.3.1 Sparse Network Storage  Previous works aim to remove redundant weights (e.g., unstructured pruning) among different layers. The trivial weights are set to zero based on different criteria. The ratio of zero-weight in the whole network is regarded as sparsity ratio. Different approaches are compared based on their ﬁnal test accuracy with a given sparsity ratio. Different from this conventional fashion which restoring sparse trained weights, we instead use ﬁxed random weights with different masks to represent a network. To compare these two paradigm, we calculate the required storage size as an integrated measurement.  Assuming we have a trained network with p as parameter numbers and r as sparsity ratio. Due to its sparsity, we only need to restore the non-zero weight values accompanied with their position [13] denoted as a binary mask. The storage cost can be separated into two parts, Cw for weight values and  8  Cm for mask given by  C = Cw + Cm.  (10)  For conventional setting, Cw restores the values of kept sparse weights which is p · (1 − r) for the whole network. It needs to be restore in ﬂoat format. Cm restores sparse positions of these weights, which can be restored into compressed sparse column (CSC) or compressed sparse row (CSR) formats with cost around 2p · (1 − r) [13]. In our new paradigm, Cw records the values of the given random weights. For example, the one-layer requires to record all weights of non-repetitive layers and one prototype weights of all repetitive layers. MP requires to keep the values of layer with the largest number of parameters. RP only requires to restore the values of a random vector with given length. Cm is also for the sparse positions to record the selected subnetwork.  4.3.2 Compression Performance Validation  We test the compression performance on CI- FAR10 and CIFAR100 datasets using Con- vMixer with 6/8 depths, ResNet32, and ResNet56 backbones. The compression ratio is based on the storage size as we discussed in- stead of the conventional pruning ratio. Since we propose a new strategy to compress network, we involve two sparse network training base- lines in our experiments. Speciﬁcally, we train a sparse network from scratch by removing ran- dom weight and minimum magnitude weights. For compression ratio, we set four settings for baselines: 90%, 92%, 94%, and 96%. We di- rectly refer to settings, MP, RP from Sec. 4.2 for our paradigm. Their compression ratio is calcu- lated using the same measurement as baselines.  Figure 5: Compression performance validation on CIFAR10 dataset on ConvMixer backbone. Y-axis is the test accuracy. X-axis means compression ratio. Two pairs of comparisons are for different depths shown in different colors. Straight line on the top is the dense model performance. Curves in different symbols represent baselines and our method. For ConvMixer, our three points are based on RP 1e-1, RP 1e-2, and RP 1e-3, respectively.  In Fig. 4, we show 4 pairs of comparison based on 2 backbones (ResNet32/ResNet56) and 2 datasets (CIFAR10/CIFAR100). Each pair in- cludes dense model, 2 baselines with 4 com- pression ratios, and our results in 3 ratios. Two baselines are sparse network training by prun- ing random weights and minimum magnitude weights, respectively. For convenience, we do not follow exactly the same compression ratios as baseline but directly use settings from Sec.4.2. For ResNet, we use MP, RP 1e-1, and RP 1e-2. Their corresponding compression ratios are computed as shown in ﬁgures. Experiments based on different networks and datasets show the similar conclusions summarized as: 1) our method outperforms the baselines by a signiﬁcant margin, with even higher compression ratio; 2); Compared with conventional sparse network training where compressed model performance decreases obviously along with increasing compression ratio, our method is relatively robust to the compressed model size; 3) If we compare cross different models, we ﬁnd compressed small model by our method even performs better than baselines using larger model; 4) Network scale affects the compression performance, compared with ResNet32, ResNet56 basically contains more parameters and performance drop between compressed network with its dense counterpart is relatively small. In Fig. 5, we show compression performance on CIFAR10 dataset using ConvMixer with depth 6 and 8. The settings are basically similar to Fig. 4. We can also draw the similar conclusions: 1) Our method outperforms the baselines on ConvMixer with different depths; 2) Our method compresses network into lower size but maintains higher performance.  As a summary, in Sec. 4, our experiments can be separated into two parts. Firstly, to validate our PEMN, we investigate the representative potential of random weights which are used to ﬁll in the complete network structure using different proposed strategies. Secondly, the promising conclusion (Sec. 4.2) for this investigation naturally leads to the newly proposed network compression paradigm. Different from conventional fashion restoring sparse weights, we instead restore the ﬁxed random weights and different masks. Empirically, we validate the effectiveness of our new paradigm for  9  network compression. Our experiments involve diverse network architectures to demonstrate the proposed paradigm can be generalized into different network designs.  5 Discussion and Conclusion  Discussion We summarize our intuitive logic and potential research direction in the future. Our fundamental insight is motivated by Supermasks [44] and Popup [31] showing random network encodes informative pattern by selecting subnetworks. They inspire us to understand neural network in a decoupled perspective: the informative output is delivered by certain weight-structure combination. Even if weights are ﬁxed, the ﬂexibility of learnable masks still provides promising capacity to represent diverse semantic information. We are the ﬁrst to fully explore the representative potential of random weights, and practically, a new network compression paradigm is naturally established. We further discuss some research directions in the future following this study. Firstly, compared with conventional approaches need to record learned weights, our paradigm records random weights which can be pre-accessed, can this property be used for improve the model security? In addition, leveraging on the property that repetitive random weights existing in networks for our strategies, is it possible to speciﬁcally design hardware deployment conﬁgurations to achieve further compression or acceleration? Moreover, our PEMN is based on random initialized weights but cannot be directly deployed on pretrained models, which are more powerful these days. How to further improve our strategy on large-scale pretrained models is another interesting point to explore. We leave these topics in our future work.  Conclusion We ﬁrst explore the maximum representative potential of a set of ﬁxed random weights, which leverages different learned masks to obtain different feature mappings. Correspondingly, we introduce our proposed Parameter-Efﬁcient Masking Networks (PEMN). Speciﬁcally, we naturally propose three strategies, one-layer, max-layer padding (MP), and random vector padding (RP), to ﬁll in a complete network with given random weights. We ﬁnd that a neural network with even limited unique parameters can achieve promising performance. It shows that parameters with fewer unique values have great representative potential achieved by learning different masks. Therefore, we can represent a complete network by combining a set of random weights with different masks. Inspired by this observation, we propose a novel network compression paradigm. Compared with traditional approaches, our paradigm can restore and transfer a network by only keeping a random vector with masks, instead of recording sparse weights for the whole network. Since the cost of restoring a mask is signiﬁcantly lower than weight, we can achieve admirable compression performance. We conduct comprehensive experiments based on several popular network architectures to explore the random weights potential for PEMN and test the compression performance of our new paradigm. We expect our work can inspire further researches for both exploring network representative potential and network compression.  References  [1] S. Bai, J. Z. Kolter, and V. Koltun. Deep equilibrium models. NeurIPS, 32, 2019.  [2] Y. Bai, H. Wang, Z. Tao, K. Li, and Y. Fu. Dual lottery ticket hypothesis. arXiv preprint  arXiv:2203.04248, 2022.  [3] Y. Bengio, N. Léonard, and A. Courville. Estimating or propagating gradients through stochastic  neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.  [4] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. NeurIPS, 33:1877–1901, 2020.  [5] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Ł. Kaiser. Universal transformers. arXiv  preprint arXiv:1807.03819, 2018.  [6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical  image database. In CVPR, pages 248–255, 2009.  [7] T. Dettmers and L. Zettlemoyer. Sparse networks from scratch: Faster training without losing  performance. arXiv preprint arXiv:1907.04840, 2019.  10  [8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional  transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.  [9] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.  [10] U. Evci, T. Gale, J. Menick, P. S. Castro, and E. Elsen. Rigging the lottery: Making all tickets  winners. In ICML, pages 2943–2952. PMLR, 2020.  [11] J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural  networks. arXiv preprint arXiv:1803.03635, 2018.  [12] A. Gaier and D. Ha. Weight agnostic neural networks. NeurIPS, 32, 2019.  [13] S. Han, H. Mao, and W. J. Dally. Deep compression: Compressing deep neural networks with  pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.  [14] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights and connections for efﬁcient  neural network. NeurIPS, 28, 2015.  [15] B. Hassibi and D. Stork. Second order derivatives for network pruning: Optimal brain surgeon.  NeurIPS, 5, 1992.  [16] B. Hassibi, D. G. Stork, and G. J. Wolff. Optimal brain surgeon and general network pruning.  In IEEE international conference on neural networks, pages 293–299, 1993.  [17] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers: Surpassing human-level  performance on imagenet classiﬁcation. In ICCV, pages 1026–1034, 2015.  [18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR,  pages 770–778, 2016.  [19] S. Jastrz˛ebski, D. Arpit, N. Ballas, V. Verma, T. Che, and Y. Bengio. Residual connections  encourage iterative inference. arXiv preprint arXiv:1710.04773, 2017.  [20] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint  arXiv:1412.6980, 2014.  [21] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.  [22] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.  [23] Y. Le and X. Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.  [24] Y. LeCun, J. Denker, and S. Solla. Optimal brain damage. NeurIPS, 2, 1989.  [25] N. Lee, T. Ajanthan, and P. H. Torr. Snip: Single-shot network pruning based on connection  sensitivity. arXiv preprint arXiv:1810.02340, 2018.  [26] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf. Pruning ﬁlters for efﬁcient convnets.  arXiv preprint arXiv:1608.08710, 2016.  [27] I. Loshchilov and F. Hutter. Fixing weight decay regularization in adam. 2018.  [28] E. Malach, G. Yehudai, S. Shalev-Schwartz, and O. Shamir. Proving the lottery ticket hypothesis:  Pruning is all you need. In ICML, pages 6682–6691. PMLR, 2020.  [29] H. Mostafa and X. Wang. Parameter efﬁcient training of deep convolutional neural networks by  dynamic sparse reparameterization. In ICML, pages 4646–4655. PMLR, 2019.  [30] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748–8763. PMLR, 2021.  11  [31] V. Ramanujan, M. Wortsman, A. Kembhavi, A. Farhadi, and M. Rastegari. What’s hidden in a  randomly weighted neural network? In CVPR, pages 11893–11902, 2020.  [32] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. NeurIPS, 34, 2021.  [33] A. Trockman and J. Z. Kolter. Patches are all you need? arXiv preprint arXiv:2201.09792,  2022.  [34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and  I. Polosukhin. Attention is all you need. NeurIPS, 30, 2017.  [35] C. Wang, G. Zhang, and R. Grosse. Picking winning tickets before training by preserving  gradient ﬂow. arXiv preprint arXiv:2002.07376, 2020.  [36] H. Wang and Y. Fu. Trainability preserving neural structured pruning. arXiv preprint  arXiv:2207.12534, 2022.  [37] H. Wang, C. Qin, Y. Bai, Y. Zhang, and Y. Fu. Recent advances on neural network pruning at  initialization. arXiv e-prints, pages arXiv–2103, 2021.  [38] H. Wang, C. Qin, Y. Zhang, and Y. Fu. Neural pruning via growing regularization. arXiv  preprint arXiv:2012.09243, 2020.  [39] H. You, C. Li, P. Xu, Y. Fu, Y. Wang, X. Chen, R. G. Baraniuk, Z. Wang, and Y. Lin. Draw- ing early-bird tickets: Towards more efﬁcient training of deep networks. arXiv preprint arXiv:1909.11957, 2019.  [40] Y. Zhang, H. Wang, C. Qin, and Y. Fu. Aligned structured sparsity learning for efﬁcient image  super-resolution. NeurIPS, 34:2695–2706, 2021.  [41] Y. Zhang, H. Wang, C. Qin, and Y. Fu. Learning efﬁcient image super-resolution networks via  structure-regularized pruning. In ICLR, 2021.  [42] Y. Zhang and Q. Yang. A survey on multi-task learning. TKDE, 2021.  [43] Z. Zhang and C. Jung. Recurrent convolutions: A model compression point of view. 2018.  [44] H. Zhou, J. Lan, R. Liu, and J. Yosinski. Deconstructing lottery tickets: Zeros, signs, and the  supermask. NeurIPS, 32, 2019.  Checklist  The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or [N/A] . You are strongly encouraged to include a justiﬁcation to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example:  Did you include the license to the code and datasets? [Yes] • Did you include the license to the code and datasets? [No] The code and the data are  proprietary.  Did you include the license to the code and datasets? [N/A]  Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.  1. For all authors...  (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s  contributions and scope? [Yes]  12  (b) Did you describe the limitations of your work? [Yes] See supplementary material. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See  supplementary material.  (d) Have you read the ethics review guidelines and ensured that your paper conforms to  them? [Yes]  2. If you are including theoretical results...  (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A]  3. If you ran experiments...  (a) Did you include the code, data, and instructions needed to reproduce the main experi-  mental results (either in the supplemental material or as a URL)? [Yes]  (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they  were chosen)? [Yes] See supplementary material.  (c) Did you report error bars (e.g., with respect to the random seed after running experi-  ments multiple times)? [Yes] See supplementary material.  (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See supplementary material.  4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Section 4. (b) Did you mention the license of the assets? [Yes] See Section 4. (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you’re  using/curating? [N/A]  (e) Did you discuss whether the data you are using/curating contains personally identiﬁable  information or offensive content? [N/A]  5. If you used crowdsourcing or conducted research with human subjects...  (a) Did you include the full text of instructions given to participants and screenshots, if  applicable? [N/A]  (b) Did you describe any potential participant risks, with links to Institutional Review  Board (IRB) approvals, if applicable? [N/A]  (c) Did you include the estimated hourly wage paid to participants and the total amount  spent on participant compensation? [N/A]  13  Appendix  A.  Implementation Details  For all the backbones used in our experiments, we follow their default training settings. For Con- vMixer [33], we use AdamW [27] optimizer with triangular learning rate scheduler. We set the maximum learning rate as 0.05 with weight decay as 0.005. We set batch size as 512 and the number of total epochs is 100. We use different conﬁgurations for hidden dimension (256/512) and depth (6/8) in our experiments section. For ViT [9], we use Adam [20] optimizer with cosine learning rate scheduler. We set the maximum learning rate as 0.0001. We set batch size as 256 and the number of total epochs as 200. We use different conﬁgurations for hidden dimension (256/512) and depth (6/8) in our experiments section. For ResNet [18], we follow the implementation conﬁgurations in Popup [31]. Speciﬁcally, we use SGD optimizer with maximum learning rate 0.1 and cosine scheduler. The weight decay and momentum are set as 0.0005 and 0.9. We train 100 epochs with 256 batch size. For the sparse selection to pick the subnetworks, we follow the optimal choice in Popup and set it as 0.5 for our experiments. And we use kaiming uniform and kaiming normal [17] to initialize the scores and random weights, respectively.  B. Limitations and Potential Negative Societal Impacts  Firstly, our study focuses on exploring the random weight potential and representing a neural network with small storage cost. However, it cannot further help for accelerating the training and inference. This is a good point for us to further explore. Secondly, our proposed network compression strategy leverages on the representative potential of random weights with different masks, which beneﬁts to reducing storage cost. However, following this strategy, it is hard to take advantages of powerful pretrained model these days. How to tailor our insight to large-scale pretrained model is another good point to explore. To the best of our knowledge, our study has no potential negative societal impacts.  C. More Experimental Evaluations  We further add experiments for CIFAR100 [21] and Tiny-ImageNet [23] datasets using ConvMixer as backbone to test our new network compression paradigm. The results are shown in Fig. 6. We use 8 and 6 as depth number for CIFAR100 and Tiny-ImageNet datasets, respectively and other settings follow the same mentioned above. The results in ﬁgure show that our compression strategy outperforms the baseline methods and validate the effectiveness of our propose network compression paradigm. We also conduct experiments on large-scale ImageNet [6] dataset using ResNet50 [18] as backbone. For simplicity, we customize two ImageNet subset for convenient evaluation. We sample 100/200 classes from original ImageNet to construct our subsets. We compare our compression strategy with magnitude pruning baseline and results are shown in Fig. 7. The results of our method are promising and demonstrate its effectiveness on challenging ImageNet dataset.  D. The Results of Repeated Experiments  We further supplement the repeated experimental results. We supplement the main results from Fig. 3 and Fig. 4. We repeated each experiment three times and report mean and std. In Tab. 1, Tab. 2, Tab. 3, Tab. 4, we provide the repeated results for Fig. 3. According to these results, we make several conclusions: 1) Our experimental performance are generally consistent across different datasets and different settings. The supplemented results follow the accuracy patterns and support the conclusions provided in our draft; 2) ConvMixer is more stable than ViT backbone across different settings; 3) Overall, along with the decreasing number of unique values in the network (from the left column to the right column of tables), the performance variations increase correspondingly. The limited unique weight values decreases the stability of the network. In Tab. 5, Tab. 6, we provide the repeated results for Fig. 4. The ﬁrst four columns show the compression baselines (the ﬁrst / second items represent random and magnitude pruning). Based on the results shown above, we ﬁnd our strategies generally outperform the model compression baselines and these results support our conclusion in the draft.  14  (a) ConvMixer on CIFAR100 dataset.  (b) ConvMixer on Tiny-ImageNet dataset.  Figure 6: Compression performance validation on CIFAR100/Tiny-ImageNet datasets on ConvMixer backbone. Y-axis denotes the test accuracy. X-axis means the network size compression ratio. The straight lines on the top are performance of dense model with regular training. Lines with different symbol shapes denote different settings. Our three points are based on RP 1e-1, RP 1e- 2, and RP 1e-3, respectively. This ﬁgure shows that our proposed paradigm achieves admirable compression performance compared with baselines. We can still maintain the test accuracy in very high compression ratios.  (a) ResNet50 on ImageNet 100 subset.  (b) ResNet50 on ImageNet 200 subset.  Figure 7: Compression performance validation on ImageNet 100/200 subsets on ResNet50 backbone. Y-axis denotes the test accuracy. X-axis means the network size compression ratio. The straight lines on the top are performance of dense model with regular training. Lines with different symbol shapes denote different settings. Our three points are based on RP 1e-1, RP 1e-2, and RP 1e-3, respectively. This ﬁgure shows that our proposed compression strategy achieves promising performance on challenging ImageNet dataset compared with baselines.  15  Table 1: Repeated experimental results for ConvMixer 6-block in subﬁgure (a) of Fig. 3  Dim  256 512  Mask  One-layer  MP  RP_1e-1  RP_1e-2  RP_1e-3  RP_1e-4  RP_1e-5  89.31 (0.16) 91.90 (0.03)  88.80 (0.11) 91.87 (0.14)  88.97 (0.08) 92.02 (0.02)  88.70 (0.21) 92.07 (0.12)  88.89 (0.13) 92.13 (0.16)  88.52 (0.15) 92.05 (0.03)  85.76 (0.34) 90.55 (0.14)  81.01 (0.38) 87.40 (0.20)  Table 2: Repeated experimental results for ConvMixer 8-block in subﬁgure (b) of Fig. 3.  Dim  256 512  Dim  256 512  Dim  256 512  Mask  One-layer  MP  RP_1e-1  RP_1e-2  RP_1e-3  RP_1e-4  RP_1e-5  90.42 (0.09) 92.69 (0.11)  90.47 (0.04) 92.71 (0.06)  90.65 (0.11) 93.21 (0.05)  90.63 (0.14) 92.90 (0.07)  90.59 (0.06) 92.88 (0.15)  90.06 (0.22) 92.90 (0.07)  87.64 (0.19) 91.71 (0.20)  82.34 (0.26) 87.40 (0.22)  Table 3: Repeated experimental results for ViT 6-block in subﬁgure (c) of Fig. 3.  Mask  One-layer  MP  RP_1e-1  RP_1e-2  RP_1e-3  RP_1e-4  RP_1e-5  76.35 (0.15) 80.73 (0.21)  76.21 (0.14) 81.56 (0.25)  76.70 (0.10) 81.50 (0.11)  77.01 (0.17) 81.87 (0.06)  76.76 (0.21) 81.25 (0.13)  76.80 (0.20) 80.98 (0.16)  64.84 (0.21) 79.17 (0.25)  65.76 (0.23) 79.00 (0.14)  Table 4: Repeated experimental results for ViT 8-block in subﬁgure (d) of Fig. 3.  Mask  One-layer  MP  RP_1e-1  RP_1e-2  RP_1e-3  RP_1e-4  RP_1e-5  79.21 (0.09) 83.44 (0.17)  79.54 (0.16) 83.50 (0.26)  79.23 (0.22) 83.66 (0.12)  79.30 (0.08) 83.67 (0.13)  79.62 (0.15) 83.25 (0.20)  79.28 (0.14) 83.34 (0.19)  73.79 (0.26) 76.73 (0.31)  71.71 (0.28) 78.84 (0.29)  Table 5: Repeated experimental results for ResNet56/32 on CIFAR10 in subﬁgure (a) of Fig. 4  Network  pr0.9  pr0.92  pr0.94  pr0.96  Ours-MP  Ours-RP_1e-1 Ours-RP_1e-2  ResNet56 ResNet32  86.35 (0.22) / 86.74 (0.28) 84.21 (0.13) / 84.22 (0.05)  85.55 (0.14) / 86.01 (0.17) 83.29 (0.11) / 83.60 (0.20)  85.01 (0.19) / 84.62 (0.27) 82.08 (0.16) / 81.56 (0.28)  81.93 (0.16) / 82.04 (0.18) 79.36 (0.13) / 79.12 (0.15)  88.13 (0.19) 86.33 (0.14)  88.36 (0.24) 86.58 (0.09)  87.97 (0.21) 86.39 (0.18)  Table 6: Repeated experimental results for ResNet56/32 on CIFAR100 in subﬁgure (b) of Fig. 4.  Network  pr0.9  pr0.92  pr0.94  pr0.96  Ours-MP  Ours-RP_1e-1 Ours-RP_1e-2  ResNet56 ResNet32  55.21 (0.36) / 54.01 (0.38) 49.21 (0.29) / 49.35 (0.14)  51.96 (0.33) / 52.54 (0.16) 47.36 (0.11) / 47.38 (0.07)  49.72 (0.18) / 49.70 (0.25) 43.70 (0.41) / 44.60 (0.29)  44.00 (0.13) / 44.18 (0.26) 39.78 (0.23) / 39.54 (0.37)  56.39 (0.29) 51.76 (0.25)  56.58 (0.21) 50.78 (0.30)  55.84 (0.27) 51.94 (0.19)  16\n",
      "2 2 0 2  n u J  8 2  ]  G L . s c [  1 v 1 9 9 3 1 . 6 0 2 2 : v i X r a  Increasing Conﬁdence in Adversarial Robustness Evaluations  Roland S. Zimmermann∗ University of Tübingen  Wieland Brendel University of Tübingen  Florian Tramèr Google  Nicholas Carlini Google  Abstract  Hundreds of defenses have been proposed to make deep neural networks robust against minimal (adversarial) input perturbations. However, only a handful of these defenses held up their claims because correctly evaluating robustness is extremely challenging: Weak attacks often fail to ﬁnd adversarial examples even if they unknowingly exist, thereby making a vulnerable network look robust. In this paper, we propose a test to identify weak attacks, and thus weak defense evaluations. Our test slightly modiﬁes a neural network to guarantee the existence of an adversarial example for every sample. Consequentially, any correct attack must succeed in breaking this modiﬁed network. For eleven out of thirteen previously-published defenses, the original evaluation of the defense fails our test, while stronger attacks that break these defenses pass it. We hope that attack unit tests — such as ours — will be a major component in future robustness evaluations and increase conﬁdence in an empirical ﬁeld that is currently riddled with skepticism. Online version & code: zimmerrol.github.io/active-tests/  1  Introduction  Suppose that someone presents you with a purported proof that P(cid:54)=NP. The proof is long, complicated, and difﬁcult to follow. How would you go about checking if this proof is correct?  One cumbersome way would be to directly refute the proof’s claim, e.g., to demonstrate that actually P=NP by designing an algorithm that solves 3-SAT in polynomial time. While this would deﬁnitely refute the proof, it is likely orders of magnitude more difﬁcult than simply showing that the proof is incorrect. Accordingly, researchers typically refute incorrect proofs by studying proofs line-by-line, until they identify some major ﬂaw in the reasoning.  Evaluating adversarial defenses is somewhat similar to proving a theorem: It is difﬁcult to estimate the robustness of a defense correctly (just as it is difﬁcult to write down a correct proof), but relatively easy to severely overestimate its robustness (just as it is easy to write down a wrong proof). This contrasts with many other areas of machine learning, where performance evaluations are often trivial — for example, by computing accuracy on some held-out test set. However, evaluating defense robustness necessarily involves reasoning over all possible adversaries and showing none can succeed. That is, a defense evaluation aims to prove that something is impossible. As a result, despite signiﬁcant evaluation effort, most published defenses are later broken by stronger attacks [10, 3, 11, 38, 14].  In this light, we here argue for viewing defense proposals as theorem statements, and the correspond- ing evaluations as proofs. The purpose of a defense evaluation, then, is to provide a convincing and rigorous argument that the defense is correct. Currently, for an adversary to claim to have a “break” of a defense, it is necessary to actually produce the adversarial examples that cause the model to make an error — analogous to refuting a complexity-theoretic impossibility result by producing an  ∗Work done while at Google. Correspondence to: roland.zimmermann@uni-tuebingen.de  Preprint. Under review.  Figure 1: A: Reasons for seemingly high robustness. There are two reasons an attack might not ﬁnd an adversarial example. Either the classiﬁer is robust or the attack is too weak and it could not ﬁnd the existing adversarials. In our proposed Binarization Test we pose a new binary classiﬁcation problem based on the original classiﬁer such that adversarial examples always exist. Thus, if the attack does not ﬁnd an adversarial example it follows that the attack is too weak. B: Setup of the Binarization Test. We construct a binary classiﬁcation problem around a clean example such that there exists a valid “adversarial” example within the feasible set of the attack’s threat model. Based on the original classiﬁer’s features, we create a new binary classiﬁer whose robustness can be evaluated with the same evaluation method as used for the original classiﬁer.  efﬁcient algorithm. We argue that this is not how things should work. A valid refutation of a theorem would be to say “there is a ﬂaw in your proof on line 9”. This is because the null hypothesis for a theorem is that it is false, just as the null hypothesis for a defense should be that it is not robust.  Unfortunately, for defenses against adversarial examples, outside of studying the actual code used to evaluate (i.e., attack) the defense, there are few opportunities to identify ﬂawed evaluations by reading a research paper. Indeed, current best practices for identifying ﬂawed evaluations are limited to looking for artifacts that indicate something has gone (terribly) wrong — for example, that an attack fails even when it is allowed to construct unbounded perturbations [11].  We develop a new active robustness test to complement existing (passive) tests [11, 26]. Our test designs a new task that is solvable by any sufﬁciently strong attack. This test can be seen as a “unit test” that the evaluation methodology is correct. Our test purposefully injects adversarial examples into a defense and then checks if the attack used to evaluate the defense is able to ﬁnd them. If the attack fails this test, we know that the attack is too weak to distinguish between a robust and a non-robust defense, and thus the evaluation should not be trusted.  Returning to our earlier theorem analogy, showing that a defense evaluation fails our test should be viewed similarly as showing a ﬂaw in part of a theorem’s proof. Note that this does not necessarily imply that the theorem is not correct (a.k.a. that the defense is not effective), but it should not give us any conﬁdence that it is.  We show that our test would have potentially identiﬁed eleven out of thirteen weak evaluations found in peer-reviewed papers. We hope that our testing methodology can become a standard component of future defense evaluations. To this end, defenses with exceptionally novel or different techniques, training algorithms, or architectures, may need to develop their own tailored version of our active unit test, in order to provide convincing evidence that the defense evaluation is indeed correct.  2 Background  Adversarial Examples Adversarial examples contain imperceptible perturbations that change the decision of a deep neural network in arbitrary directions [4, 37]. Since they can manipulate the behavior of a model, they are seen as a security concern for machine learning applications. To ﬁnd adversarial examples for a network, one looks for inputs changing the output of the network while being close (under some norm) to the original data sample. There are a number of methods to solve this optimization problem and to attack a network. Adversarial attacks can be divided into white box methods that use gradient information about the model [e.g., 20, 10, 6, 13, 14], and black box methods that only use the output of the network [e.g., 5, 17, 1, 23, 2].  Defenses With an increasing awareness of the risk posed by adversarial examples, a vast number of defenses have been proposed to increase adversarial robustness. For example, some defenses rely on additional input pre-processing steps [e.g., 16], some introduce architectural changes [e.g.,  2  41], and others propose methods for detecting adversarial examples [e.g., 21]. However, most of these defenses eventually turned out to be ineffective against stronger attacks [3, 38]. Until now only adversarial training [20] and its variants [e.g., 27, 28, 15] stood the test of time and could not be circumvented. A different approach to defend classiﬁers against adversarial perturbations are certiﬁed defenses which give a theoretical guarantee of the classiﬁer’s robustness. However, the robustness of these approaches does not yet reach that of adversarial training [40, 12, 19].  Challenges in Evaluating Defenses Properly evaluating the robustness of a model against adver- sarial examples is non-trivial and there are many potential pitfalls [11]. The critical issue is that when a defense is shown to be robust to a speciﬁc attack, this either means that the model is truly robust, or that the attack is suboptimal (see Figure 1A). Possible reasons for an attack to be suboptimal include mechanisms in the model that (often unintentionally) hinder the attack’s optimization process or poor choices of hyperparameters [3]. Examples of the former include defenses built around non-continuous activation functions [e.g., 41] or relying on vanishing gradients [e.g., 36]. To address the latter issue, prior work has developed attacks that alleviate the need to manually tune hyperparameters [14]. But as we will show, these attacks are not guaranteed to work well for any model. While the latter issue can be counteracted by using adaptive attacks [38] that are adjusted to a speciﬁc model’s idiosyncrasies, it remains non-trivial to detect suboptimal attacks in the ﬁrst place. Previous work suggested guidelines for evaluating the adversarial robustness of a model [11] and developed (passive) indicator values hinting at a failed evaluation [26]. Speciﬁcally, Pintor et al. [26] hook various metrics into the execution of gradient-based attacks to check for failure cases such as vanishing gradients. Our work goes beyond these passive failure indicators and argues that researchers should actively demonstrate that their adversarial attack is sufﬁciently strong, and that their empirical ﬁndings can, thus, be trusted. Our proposed approach also has the advantage of being agnostic to the type of attack being used in an evaluation (e.g., gradient-based, decision-based, transfer-based, etc.).  3 Active Attack Evaluation Tests  The evaluation of a defense against adversarial attacks becomes more reliable — and the estimated robustness more correct — if the attack is sufﬁciently strong. The strength of an attack is not an absolute value but depends on the defense it is meant to evaluate, as various defense mechanisms hinder speciﬁc attacks [3]. Thus, for a new defense, one needs to demonstrate that the attack proposed to evaluate it is appropriate. In this section, we propose a test that measures the adequacy of a defense’s evaluation, and is thereby able to warn researchers of potentially unreliable robustness claims.  To empirically demonstrate the robustness of a defense for some clean input xc, one runs an attack and shows that it fails to ﬁnd an adversarial example xadv within distance d(xc, xadv) ≤ (cid:15). But can we really be sure there are no adversarial examples in the (cid:15) ball if the attack fails? Since the attack cannot guarantee this, there might exist stronger attacks that do ﬁnd adversarial examples (see Figure 1A).  We propose a test that enables researchers to check whether an attack is too weak to support their robustness claims. In our test, we build a new defense that is as similar as possible to the original, but where we intentionally inject an adversarial example xadv.2 After building the new (by deﬁnition vulnerable) defense, we measure its robustness by running the original evaluation method, and checking whether an adversarial example is found. If the originally used attack fails to ﬁnd any adversarial example for the modiﬁed defense, we should not expect it to properly estimate the robustness of the original defense either.  3.1 Test for Classiﬁers with Linear Classiﬁcation Readouts  We begin by describing how to construct the modiﬁed vulnerable model for a classiﬁer f that consists of a feature extractor f ∗ followed by a linear classiﬁcation head. Any standard neural network architecture falls into this category: the feature extractor f ∗ comprises every layer except the last, and the linear classiﬁcation head is the ﬁnal logit projection layer. Our evaluation methodology keeps the feature extractor f ∗ unchanged to avoid changing the fundamental behavior of the model, but replaces the classiﬁcation readout with a newly trained module. This module is trained on a new, specially  2Intentionally injecting adversarial examples has been (unsuccessfully) considered before as a “honeypot”  defense [34]. Here, we use this idea in a completely different context, namely to unit-test attacks.  3  constructed dataset which allows us to reliably create a classiﬁer where — by design — there exists at least one adversarial example for each sample. A simpliﬁed pseudocode implementation of our test is shown in Algorithm 1.  Algorithm 1 Binarization Test for Classiﬁers with Linear Classiﬁcation Readouts  input: test samples Xtest, feature extractor f ∗ of original classiﬁer, number of inner/boundary samples Ni and Nb, distance (cid:15), sampling functions for data from the inside/boundary of the (cid:15)-ball.  function BINARIZATIONTEST(f ∗, Xtest, Nb, Ni, (cid:15))  attack_successful = [] random_attack_successful = [] for all xc ∈ Xtest do  h = CreateBinaryClassiﬁer(f ∗, xc, (cid:15)) # evaluate robustness of binary classiﬁer attack_successful.insert (RunAttack(h, xc)) random_attack_successful.insert (RunRandomAttack(h, xc))  ASR = Mean(attack_successful) RASR = Mean(random_attack_successful) return ASR, RASR  end function  function CREATEBINARYCLASSIFIER(f ∗, xc) # draw input samples around clean example Xi = { xc } ∪ { SampleInnerPoint(xc, (cid:15)) }1,...,Ni Xb = { SampleBoundaryPoint(xc, (cid:15)) }1,...,Nb # get features for images Fi = { f ∗(x) | x ∈ Xi } Fb = { f ∗(x) | x ∈ Xb } # deﬁne labels & create labeled dataset D = { (ˆx, 0) | ˆx ∈ Fi } ∪ { (ˆx, 1) | ˆx ∈ Fb } # train linear readout on extracted features g = TrainReadout(D) h = g ◦ f ∗ return binary classiﬁer h based on feature encoder f ∗  end function  In detail, for each test sample xc our test consists of the following steps:  Creating the Dataset versions of xc  Initially, we create two collections of input samples which are perturbed  Xi := { ˆx | d(xc, ˆx) < ξ · (cid:15) ∧ ˆx (cid:54)= xc } ∪ { xc }1,...,Ni and Xb := { ˆx | d(xc, ˆx) = (cid:15) }1,...,Nb ,  (1) which are sets of points (randomly sampled) from the inside and the boundary of the (cid:15)-ball, respec- tively, with size Ni, Nb > 0. Further, ξ ∈ (0, 1) controls the margin between the inner Xi and the boundary set Xb. Decreasing ξ effectively increases the gap between inner and boundary points, thus, making it easier to distinguish between the two sets of samples.  Next, for every sample in each of the two sets, obtain the feature representation of the penultimate layer of f ,  Fi := { f ∗(x) | x ∈ Xi } and Fb := { f ∗(x) | x ∈ Xb }.  (2)  Training the Vulnerable Classiﬁer Now, we train a linear (binary) discriminator g that distin- guishes samples from Fi and Fb, i.e., it distinguishes between mildly perturbed images — in the interior of the (cid:15)-ball — and some more strongly perturbed images — on the boundary of the (cid:15)-ball.3  3In some cases, the range of values of the classiﬁer’s logits has a large inﬂuence on the reliability of an adversarial evaluation (e.g., some weak attacks can fail due to very large or very small logits [10, 3]). To ensure that our modiﬁed classiﬁer matches the original classiﬁer as much as possible, we thus aim to mimic the properties of the original classiﬁer’s logits. To this end, we rescale the weights of the linear discriminator so as to match the value range of the original classiﬁer’s logits.  4  We want to make sure there exists at least one sample within the threat model’s (cid:15)-ball that g classiﬁes differently than the clean original sample. Due to the construction of the dataset, we can guarantee this by ensuring that g achieves a perfect accuracy on these two sets. If this is not possible for an original sample xc, we cannot apply the test and, hence, skip the sample. Combining the original classiﬁer’s feature extractor f ∗ with the binary discriminator g yields a new classiﬁer h = g ◦ f ∗ that maps samples to a binary decision. Most importantly, for this new classiﬁer each boundary sample Xb acts as an (cid:15)-bounded adversarial example xadv for the clean sample xc.  Evaluating the Vulnerable Classiﬁer We evaluate two properties of this new classiﬁer h:  1. The efﬁcacy of the used evaluation method/adversarial attack. For this, one uses the original adversarial attack to attack the modiﬁed model h for the clean sample xc and records whether an adversarial sample x∗ within the allowed (cid:15)-ball is found. When calculated and averaged over multiple samples, we call this value the test score.  2. The difﬁculty of the test. To assess this, we use a model-agnostic attack, namely a purely randomized one. We attack the modiﬁed classiﬁer h by randomly sampling approximately as many additional data points from within the (cid:15)-ball around the clean sample xc as the adversarial attack queries the model, e.g. for an N -step PGD attack [20] use N additional random samples. Finally, one tests whether at least one of them turns out to be an adversarial perturbation for h. Averaging over multiple samples yields the random attack success rate (R-ASR).  Note that if the classiﬁer f does not use a linear classiﬁcation readout, one has to modify the test slightly: Instead of using a linear readout for g one needs to use the same type of mechanism that was used originally. While this modiﬁcation is conceivable for various mechanisms, e.g., k-nearest neighbors classiﬁcation [35], there might be architectures for which this is harder or impossible, e.g., classiﬁcation through likelihood estimations based on generative models [32, 45].  3.2 Tests for Models Leveraging Detectors  Detection defenses use an additional algorithm that detects and rejects adversarial examples [21]. A successful attack, thus, has to fool both the classiﬁer and the detector, and we have to probe both in our active test.  As earlier, we assume that the classiﬁer can be divided into a feature encoder and linear readout. We make no assumptions on the architecture of the detector, as a wide variety of designs have been proposed in the literature. We deﬁne two tests: a regular and an inverted test. Any reliable evaluation method must pass both. A pseudocode deﬁnition of the proposed tests is given as Algorithm 2 in the Appendix.  Regular Test Adversarial examples for a detection defense need to change the classiﬁer’s output while also remaining undetected. Thus, to still guarantee that there exists a valid adversarial example, we need to change the construction of the binary classiﬁer slightly to take into account the detector. Speciﬁcally, we modify the set Xb of boundary points such that none of these samples gets rejected by the detector. In practice, we enforce this with rejection sampling, by redrawing boundary points until we ﬁnd a point that is undetected. Note, that we make no modiﬁcations to the detector, since this might require non-trivial optimization of the detector’s parameters.  Some adversarial attacks for detector defenses (e.g., feature matching [30]) assume access to reference data samples that belong to a different class but are not adversarial and, thus, do not get rejected. In our setting this can be realized by randomly sampling data points outside the (cid:15) ball. Thus, we create a new collection  Xr := { ˆx | d(xc, ˆx) = η(cid:15) }1,...,Nr, for which the binary classiﬁer must predict the same class as for the boundary samples Xb. Here, Nr ≥ 0 and η > 1.0 control the number of samples and how far outside of the (cid:15) ball they are located. Again, as for Xb, we need to ensure that none of these samples get detected. By training the linear readout on Xi, Xb and Xr we guarantee that there exists at least one undetected adversarial sample within the (cid:15)-ball around xc, and at least Nr samples outside the (cid:15)-ball that are also undetected. Inverted Test Since the detector was tuned for a classiﬁcation problem different from the one posed in the regular test above, it might not work well and rarely reject samples. Thus, a potential  5  issue with the regular test is that a weak attack might pass the test even though the attack completely ignores the detector. Indeed, many evaluations of detector defenses consider weak attacks that are oblivious to the presence of the detector [9]. Thus, an attack passing the test may not be sufﬁcient to tell us that the attack is actually strong enough to successfully target the detector.  To this end, we introduce a second inverted test that inverts the attack’s goal: Instead of ﬁnding adversarial samples that do not get detected, the goal is now to ﬁnd an adversarial example that does get detected. Since any detector that claims non-zero robustness must detect some adversarial examples, we use these to construct the set of boundary samples Xb. Finally, we only need to negate the decision of the detection algorithm before proceeding exactly as for the previously described test.  Passing both the regular as well as the inverted test is a necessary condition for an adequate adversarial attack. In fact, this indicates that the attack is not agnostic to the detector but properly takes it into account. In contrast, passing only one of the tests indicates that only the classiﬁer and not the detector is directly targeted.  4 Evaluation  We now show that our binarization test could have revealed insufﬁciently strong evaluations in eleven out of thirteen previously peer-reviewed and published defenses. Of these eleven defenses, nine had already been broken by subsequent attacks — thereby conﬁrming that the original evaluation was indeed too weak. For the other two defenses that fail our test, closer inspection of both defenses’ evaluations and code indeed reveal serious issues, and we show that the defenses are broken by stronger attacks. All of these defenses assume an (cid:96)∞ threat model. The speciﬁc design choices for the tests adapted to each defense can be found in Appendix B.  Defenses without Detectors We analyze eight defenses which use a classiﬁer with a linear classiﬁ- cation readout [8, 41, 43, 22, 25, 33, 31, 44].  We additionally apply our test to two defenses that do not use a simple linear readout to perform classiﬁcation. However, it is straightforward to adapt the binarization test deﬁned in Algorithm 1 for these classiﬁer architectures. The defense by Verma and Swami [39] leverages an ensemble of readouts. For our test, we therefore also train an ensemble of binary readouts. The classiﬁer of Pang et al. [24] learns to map images to pre-deﬁned class-prototype vectors, and then uses nearest neighbour classiﬁcation. We reﬂect this in the test by using two of the class prototypes and associating them with the inner and boundary samples, respectively. Then we re-train a linear layer mapping from features to class prototypes.  In fact, we are the ﬁrst to show that the defense by Sarkar et al. [31] is less robust than originally reported, as suggested by the fact that it fails our test. All of the above defenses except those by Sarkar et al. [31] were known to be ﬂawed and have been circumvented before.  Defenses with Detectors We investigate three published defenses that aim to detect adversarial perturbations. Following previous work [7], we analyze each defense in a setting where it achieves a false positive rate of 5 %. While the detection algorithm proposed by Roth et al. [29] runs statistical tests on the classiﬁer’s conﬁdence, Shan et al. [34] and Yang et al. [42] analyze earlier activations of the classiﬁer. The ﬁrst two defenses have been broken before [38, 7] while the latter had not been independently re-evaluated yet.  4.1  Initial Evaluation of Not-previously-broken Defenses  We begin by investigating the two recent and not yet broken defenses. Here, we are interested in seeing whether the original robustness evaluations pass our binarization test. While a positive result would increase conﬁdence in the defenses’ claims, a negative outcome would cast doubts.  Sarkar et al. [31] The original evaluation of this defense fails our test with a test score of 0.04. That is, the original attack only ﬁnds an adversarial example in the modiﬁed binary classiﬁer 4% of the time — even though at least one adversarial example is guaranteed to exist for every test sample. This is strong evidence that the attack is weak and thus the robustness claim likely overestimated. Upon investigation, we found a ﬂaw in the original evaluation’s code: The statistics of the batch normalization layers are not frozen during evaluation, which changes the behavior of the model during the attack. Properly freezing these layers at inference and increasing the number of PGD steps  6  Figure 2: The binarization test identiﬁes ﬂawed adversarial evaluations. The x-axis shows the score in our proposed binarization test for the original attack (upper) and a subsequent improved attack (lower). We deﬁne a threshold of 0.95 that attacks need to achieve to pass our test. For detector-based defenses, we visualize the minimum of the performance on the regular and inverted tests. Note that for each defense, the subsequent improved attack substantially decreases the defense’s robust accuracy (by at least 12%). Black markers indicate original attacks that fail the test, as well as improved attacks that pass the test (i.e., true positives and true negatives for our test). Red markers indicate suboptimal original evaluations that nevertheless pass our test (false positives). Orange markers indicate re-evaluations that used suboptimal attacks (as shown by our test) that still broke the defense. We discuss these cases in Section 4.2. Checks and crosses in the legend indicate passing/failing tests for the original and the re-evaluation, respectively. See Figure 4 in the Appendix for the robust accuracies.  from 20 to 75 yields a perfect score (1.0) in our binarization test. Moreover, this updated evaluation methodology reduces the robust accuracy to ≤ 1 % down from the originally reported 60.15 % and, thus, effectively breaks the defense.  Yang et al. [42] For this detector-based defense, we ﬁnd that the attack used in the original evaluation is agnostic to the detector and only targets the classiﬁer. Consequentially, this attack fails both the regular and the inverted binarization test with a low score of 0.26 and 0.63, respectively. We thus create a new adversarial attack based on PGD that combines two objectives: (1) fool the classiﬁer by maximizing the adversarial loss and (2) stay undetected by matching the features of a non-adversarial sample as much as possible (a feature matching attack [30, 38]). This adaptive attack achieves a nearly perfect score of 0.99 in the test and reduces the robust accuracy of the defense from the originally reported 99 % down to below 12 %.  4.2  Interpreting Test Results for Weak and Strong Attacks  Since eleven of the considered defenses have already been broken before, and we showed how to break the remaining two, we now have access to both a ﬂawed and a well-working adversarial evaluation method for each defense. This allows us to compare how these attacks perform in terms of both the estimated robust accuracy and the score on the binarization test. We visualize the results in Figure 2. For eleven out of the thirteen considered defenses, our proposed test would have ﬂagged their evaluation as insufﬁcient: the original attacks’ test performance is substantially below a perfect score (i.e., < 0.95). Furthermore, the test scores improve for all defenses when replacing the original evaluation code with an improved attack (the defense of Sen et al. [33] is an exception, as the original evaluation already obtains a perfect test score due to an integration bug, as described below).  Explaining the False Positives Our test incorrectly lets two defense evaluations that had bugs pass (see red markers in Figure 2). When investigating these failure cases in more detail, we ﬁnd that the original attack used by Sen et al. [33] is not ﬂawed or incorrectly implemented per se, but it is not used correctly. Namely, the attack generates adversarial examples with respect to the classiﬁer’s predicted label, instead of the ground-truth label. As a result, for some misclassiﬁed samples the attack actually corrects the classiﬁer’s mistake! By switching to an attack that correctly targets the ground-truth label, we reduce the robust accuracy drastically.  7  Our test is unable to catch such a mistake: By design, our test constructs a binary classiﬁer with 100% accuracy (and thus the classiﬁer’s predicted label is always equal to the ground-truth). If we view our proposed test as a unit test for an attack, then the type of bug in the above evaluation is akin to an integration bug, where the (correct) attack is called with incorrect parameters.  For the defense by Zhang and Wang [43] we notice, a high R-ASR value (> 0.75) that we could not decrease further. Thus, for this defense, our binarized classiﬁer is too easy to attack. We hypothesize that by increasing the number of inner samples Ni substantially, the test might become hard enough to indicate sub-optimal evaluations for this defense.  Explaining the Suboptimal Re-evaluations There are also four defenses for which the improved attacks still fail our test, even though their test performance is better than for the original attacks. The authors of the improved attack for Pang et al. [25] note that while this attack already breaks the defense, one could improve the attack further [38]. For the defenses by Mustafa et al. [22] and Zhang and Xu [44], the improved attacks are not adaptive attacks but part of AutoAttack’s attack collection [14]. Although these attacks were sufﬁcient to drastically reduce the measured robustness of the defenses (see Appendix, Figure 4), they are likely not optimal attacks for these defenses. While the attack [7] used for the re-evaluation breaks the defense by Shan et al. [34], the imperfect test score hints at the possibility of an even more potent and yet-to-be-discovered attack.  Figure 3: Hyperparameters inﬂuence the test’s hardness. For the defense by Mustafa et al. [22], we compare the test performance of three attacks: two sub-optimal attacks, namely the originally used PGD attack (orange) and AutoPGD (red) [14], yielding robust accuracies of 32.32 % and 8.16 % and the stronger FAB attack (blue) [13] yielding 0.71 %. As one indicator of the test’s hardness, we show the ASR of a random attacker (R-ASR, black). The test’s hardness is controlled by the hyperparameter κ which, in feature space, measures the distance between the decision boundary and the boundary sample relative to the distance between boundary and the closest inner sample. Note that the larger κ is, the closer the decision boundary is to the boundary sample.  4.3 Hardness of the Test  To put the performance that an adversarial attack achieves in the binarization test into perspective, we quantify the hardness of the test using the previously introduced random attack success rate (R-ASR). Comparing it to the test result of the attack allows us to deduce how effective the attack is in ﬁnding adversarial examples for the model in question.  There are several parameters and design choices relevant for our test that inﬂuence its hardness. For one, by increasing Ni we train the binary discriminator on a larger number of different non-adversarial points which increases the robustness of the discriminator and, thus, makes the test harder. Conversely,  8  by increasing Nb we plant a larger number of adversarial examples for the discriminator within the (cid:15)-ball, making the test simpler.  Even with a large but ﬁnite number of training samples, there is no unique solution for the binary discriminator but instead a set of valid solutions. While all of these classiﬁers have perfect accuracy on the training set, they differ in how close the decision boundary is placed to the boundary samples. The closer the decision boundary is placed to the boundary samples, the smaller the volume of valid adversarial examples and, thus, the harder the test becomes. The effect of the the decision boundary’s closeness on the test’s hardness is illustrated in Figure 3 for the defense of Mustafa et al. [22]. Here, placing the boundary closer to the boundary samples decreases both the R-ASR as well as the ASR of two sub-optimal attacks while that of a better suited attack stays robustly at 1.0.  On the one hand, a test that is too easy has no predictive power about the attack’s efﬁcacy (since any attack might trivially pass it), while on the other hand, a test that is too challenging might actually underestimate the attack’s true performance (i.e., ﬁnding an adversarial example in the modiﬁed defense might be harder than in the original defense, and thus even a strong attack against the defense might fail the test if it is tuned too aggressively). Therefore, one needs to tune the test’s hardness to a reasonable level.  We recommend the following procedure to adjust the test’s hardness: To ensure we do not overestimate the test performance (since this is the more dangerous direction), we start with a conﬁguration that makes the test as hard as possible. Then, decrease the hardness until the adversarial attack in question reaches an (almost) perfect ASR. Note that if there is no conﬁguration that yields a near-perfect ASR, then the attack did not pass the test and one should be skeptical of the attack’s ability to properly estimate the classiﬁer’s robustness. Finally, compare the ASR with the R-ASR (the success rate of a random attack): If the ASR is not substantially higher — or is even lower — than the R-ASR, this is strong evidence that the attack performs poorly. If instead the gap is large, the attack has passed this necessary test and might be powerful enough to properly estimate the classiﬁer’s robustness.  5 Discussion & Conclusion  This paper made a case for active tests to evaluate adversarial robustness. The goal of an active test is to provide compelling evidence that an attack has sufﬁcient power to evaluate a classiﬁer’s robustness. We presented such a test for defenses using linear classiﬁcation readouts and showed how to adapt this test for different defense mechanisms such as detector-based defenses. The type of test proposed in this work acts as a necessary condition for robustness evaluations, i.e., an attack that fails the test will most likely overestimate the defense’s robustness.  While we have presented a potential test that could help defense authors demonstrate sufﬁcient power of their adversarial evaluation, our tests are not meant to be comprehensive and directly applicable to every possible defense. For example, our tests are primarily designed to work for defenses that use linear classiﬁcation readout layers. If a defense were to have a different classiﬁcation layer instead, such as a k-Nearest Neighbor classiﬁer, then our tests would need to be modiﬁed accordingly. Consequentially, defense authors should aim to develop their own attack unit tests, depending on the particular claims made.  Our test could have revealed weak evaluations in eleven out of thirteen previously peer-reviewed (and subsequently broken) defenses. We are thus optimistic that active tests can improve the reliability of future publications in the ﬁeld of adversarial robustness.  Acknowledgements  We thank Alexey Kurakin for his valuable feedback. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting RSZ. This work was supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039A. WB acknowledges ﬁnancial support via an Emmy Noether Grant funded by the German Research Foundation (DFG) under grant no. BR 6382/1-1. WB is a member of the Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 390727645.  9  References  [1] Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty, Huan Zhang, Cho-Jui Hsieh, and Mani B Srivastava. Genattack: Practical black-box attacks with gradient-free optimization. In Proceedings of the Genetic and Evolutionary Computation Conference, pages 1111–1119, 2019.  [2] Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square at- tack: a query-efﬁcient black-box adversarial attack via random search. In European Conference on Computer Vision, pages 484–501. Springer, 2020.  [3] Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 274–283. PMLR, 2018. URL http://proceedings.mlr. press/v80/athalye18a.html.  [4] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndi´c, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European conference on machine learning and knowledge discovery in databases, pages 387–402. Springer, 2013.  [5] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/ forum?id=SyZI0GWCZ.  [6] Wieland Brendel, Jonas Rauber, Matthias Kümmerer, Ivan Ustyuzhaninov, and Matthias Bethge. Accurate, reliable and fast robustness evaluation. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 12841–12851, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ 885fe656777008c335ac96072a45be15-Abstract.html.  [7] Oliver Bryniarski, Nabeel Hingun, Pedro Pachuca, Vincent Wang, and Nicholas Carlini. Evading adversarial example detection defenses with orthogonal projected gradient descent. arXiv preprint arXiv:2106.15023, 2021.  [8] Jacob Buckman, Aurko Roy, Colin Raffel, and Ian J. Goodfellow. Thermometer encoding: One hot way to resist adversarial examples. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id= S18Su--CW.  [9] Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM workshop on artiﬁcial intelligence and security, pages 3–14, 2017.  [10] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In  2017 ieee symposium on security and privacy (sp), pages 39–57. IEEE, 2017.  [11] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness. ArXiv preprint, abs/1902.06705, 2019. URL https://arxiv.org/abs/1902. 06705.  [12] Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certiﬁed adversarial robustness via randomized smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 1310–1320. PMLR, 2019. URL http://proceedings.mlr.press/v97/cohen19c.html.  10  [13] Francesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast adap- tive boundary attack. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 2196–2205. PMLR, 2020. URL http://proceedings.mlr.press/v119/ croce20a.html.  [14] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 2206–2216. PMLR, 2020. URL http://proceedings. mlr.press/v119/croce20b.html.  [15] Sven Gowal, Sylvestre-Alvise Rebufﬁ, Olivia Wiles, Florian Stimberg, Dan Andrei Calian, and Timothy A Mann. Improving robustness using generated data. Advances in Neural Information Processing Systems, 34, 2021.  [16] Chuan Guo, Mayank Rana, Moustapha Cissé, and Laurens van der Maaten. Countering adversarial images using input transformations. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id= SyJ7ClWCb.  [17] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited queries and information. In Jennifer G. Dy and Andreas Krause, editors, Proceed- ings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 2142–2151. PMLR, 2018. URL http://proceedings.mlr.press/v80/ilyas18a. html.  [18] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.  2009.  [19] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certiﬁed robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security and Privacy (SP), pages 656–672. IEEE, 2019.  [20] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/ forum?id=rJzIBfZAb.  [21] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial perturbations. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=SJzCSf9xg.  [22] Aamir Mustafa, Salman H. Khan, Munawar Hayat, Roland Goecke, Jianbing Shen, and Ling Shao. Adversarial defense by restricting the hidden space of deep neural networks. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 3384–3393. IEEE, 2019. doi: 10.1109/ICCV.2019.00348. URL https://doi.org/10.1109/ICCV.2019.00348.  [23] Nina Narodytska and Shiva Prasad Kasiviswanathan. Simple black-box adversarial attacks on  deep neural networks. In CVPR Workshops, volume 2, 2017.  [24] Tianyu Pang, Kun Xu, Yinpeng Dong, Chao Du, Ning Chen, and Jun Zhu. Rethinking softmax cross-entropy loss for adversarial robustness. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=Byg9A24tvB.  11  [25] Tianyu Pang, Kun Xu, and Jun Zhu. Mixup inference: Better exploiting mixup to defend In 8th International Conference on Learning Representations, ICLR adversarial attacks. 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https:// openreview.net/forum?id=ByxtC2VtPB.  [26] Maura Pintor, Luca Demetrio, Angelo Sotgiu, Giovanni Manca, Ambra Demontis, Nicholas Carlini, Battista Biggio, and Fabio Roli. Indicators of attack failure: Debugging and improving optimization of adversarial examples. ArXiv preprint, abs/2106.09947, 2021. URL https: //arxiv.org/abs/2106.09947.  [27] Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Helper-based adversarial training: Reducing excessive margin to achieve a better accuracy vs. robustness trade-off. In ICML 2021 Workshop on Adversarial Machine Learning, 2021.  [28] Sylvestre-Alvise Rebufﬁ, Sven Gowal, Dan Andrei Calian, Florian Stimberg, Olivia Wiles, and Timothy A Mann. Data augmentation can improve robustness. Advances in Neural Information Processing Systems, 34, 2021.  [29] Kevin Roth, Yannic Kilcher, and Thomas Hofmann. The odds are odd: A statistical test for detecting adversarial examples. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 5498–5507. PMLR, 2019. URL http://proceedings.mlr.press/v97/roth19a. html.  [30] Sara Sabour, Yanshuai Cao, Fartash Faghri, and David J. Fleet. Adversarial manipulation of deep representations. In Yoshua Bengio and Yann LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.05122.  [31] Anindya Sarkar, Anirban Sarkar, Sowrya Gali, and Vineeth N Balasubramanian. Get fooled for the right reason: Improving adversarial robustness through a teacher-guided curriculum learning approach. ArXiv preprint, abs/2111.00295, 2021. URL https://arxiv.org/abs/ 2111.00295.  [32] Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the ﬁrst ad- versarially robust neural network model on mnist. In International Conference on Learning Representations, 2018.  [33] Sanchari Sen, Balaraman Ravindran, and Anand Raghunathan. EMPIR: ensembles of mixed precision deep networks for increased robustness against adversarial attacks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=HJem3yHKwH.  [34] Shawn Shan, Emily Wenger, Bolun Wang, Bo Li, Haitao Zheng, and Ben Y Zhao. Gotta catch’em all: Using honeypots to catch adversarial attacks on neural networks. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pages 67–83, 2020.  [35] Chawin Sitawarin and David Wagner. Defending against adversarial examples with k-nearest  neighbor. arXiv preprint arXiv:1906.09525, 2019.  [36] Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https: //openreview.net/forum?id=rJUYGxbCW.  [37] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In Yoshua Bengio and Yann LeCun, editors, 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/abs/1312.6199.  12  [38] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial example defenses. Advances in Neural Information Processing Systems, 33, 2020.  [39] Gunjan Verma and Ananthram Swami. Error correcting output codes improve probability estimation and adversarial robustness of deep neural networks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 8643–8653, 2019. URL https://proceedings.neurips.cc/paper/2019/ hash/cd61a580392a70389e27b0bc2b439f49-Abstract.html.  [40] Eric Wong and J. Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 5283–5292. PMLR, 2018. URL http://proceedings.mlr.press/v80/wong18a. html.  [41] Chang Xiao, Peilin Zhong, and Changxi Zheng. Enhancing adversarial defense by k-winners- take-all. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview. net/forum?id=Skgvy64tvr.  [42] Puyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling Wang, and Michael Jordan. Ml-loo: Detecting adversarial examples with feature attribution. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pages 6639–6647, 2020.  [43] Haichao Zhang and Jianyu Wang. Defense against adversarial attacks using feature scattering- based adversarial training. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neu- ral Information Processing Systems 32: Annual Conference on Neural Information Pro- cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 1829–1839, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ d8700cbd38cc9f30cecb34f0c195b137-Abstract.html.  [44] Haichao Zhang and Wei Xu. Adversarial interpolation training: A simple approach for improv- ing model robustness, 2020. URL https://openreview.net/forum?id=Syejj0NYvr.  [45] Roland S Zimmermann, Lukas Schott, Yang Song, Benjamin A Dunn, and David A Klindt.  Score-based generative classiﬁers. arXiv preprint arXiv:2110.00473, 2021.  13  A Algorithm of Binarization Test for Detection Defenses  Algorithm 2 Binarization Test for classiﬁers with a linear classiﬁcation readout and a detector  input: test samples Xtest, feature extractor f ∗ of original classiﬁer, adversarial detector d returning 1 for detected samples and 0 otherwise, number of inner/boundary/reference samples Ni/Nb/Nr, distance (cid:15), sampling functions for data from the inside/boundary of the (cid:15)-ball, relative distance (in terms of (cid:15)) of positive and reference samples η > 1.  function BINARIZATIONTEST(f ∗, d, Xtest, Nb, Ni, Nr, (cid:15), η)  attack_successful = [] random_attack_successful = [] for all xc ∈ Xtest do  b, Xr = CreateBinaryClassiﬁer(f ∗, xc, (cid:15)) # evaluate robustness of binary classiﬁer attack_success.insert (RunAttack(b, d, xc, Xr)) random_attack_success.insert (RunRandomAttack(b, d, xc))  ASR = Mean(attack_successful) RASR = Mean(random_attack_successful) return ASR, RASR  end function  function INVERTEDBINARIZATIONTEST(f ∗, d, Xtest, Nb, Ni, Nr, (cid:15), η)  # ¬d denotes the negated/inverted detector return BinarizationTest(f *, ¬d, Xtest, Np, Nn, (cid:15), η  end function  function CREATEBINARYCLASSIFIER(f ∗, xc, d) # draw input samples around clean example Xi = { xc } ∪ { SampleInnerPoint(xc, (cid:15)) }1,...,Ni Xb = { SampleBoundaryPoint(xc, (cid:15)), d(z) = 1 }1,...,Nb # get positive samples outside the (cid:15)-ball, e.g., as a reference for logit matching attacks Xr = { SampleBoundaryPoint(xc, η(cid:15)), d(z) = 1 }1,...,Nr # get features for images Fi = { f ∗(x) | x ∈ Xi } Fb = { f ∗(x) | x ∈ Xb } Fr = { f ∗(x) | x ∈ Xr } # deﬁne labels & create labeled dataset D = { (ˆx, 0) | ˆx ∈ Fi } ∪ { (ˆx, 1) | ˆx ∈ Fb } ∪ { (ˆx, 1) | ˆx ∈ Fr } # train linear readout on extracted features b = TrainReadout(D) return binary classiﬁer b based on feature encoder f ∗ and reference samples Xr  end function  B Experimental Details  For all defenses considered, we use the source code and hyperparameters originally used by the defenses’ authors to evaluate them. For integrating our test in the respective evaluations, we aimed to minimally modify the original code.  The experiments were run on a computer with eight NVIDIA GeForce RTX 2080 Ti GPUs, whereby for each experiment only a single GPU was used. In total, approximately 2500 GPU hours were required for obtaining the results presented in this work.  All investigated defenses consider an (cid:96)∞ threat model. While the defense by Shan et al. [34] focuses on an (cid:15) = 0.01 bound, the rest uses the more common (cid:15) = 8/255 bound.  We evaluate the binarization test for 512 randomly chosen samples from the CIFAR-10 [18] test set.  14  For all attacks we set the gap between the boundary and inner points to η = 0.05, measured relatively to the used (cid:15) value. We evaluated detector-based defenses using Algorithm 2, and use ξ = 1.75, measured in terms of (cid:15).  As outlined above in Section 4.3, we adjust the hardness of the test until the test produces conclusive results, i.e., the random attack success rate (R-ASR) is not too high. This leads to a parameter choice of Ninner = 999 for all defenses but that of Zhang and Wang [43] for which used Ninner = 9999. While we set the number of boundary samples to Nboundary = 10 for Zhang and Xu [44], we set it to 1 for all other defenses. Also, we sample the boundary point(s) from the corners of the (cid:96)∞ (cid:15)-box, since this increases the test’s difﬁculty further.  Further, for adjusting the hardness of the test we adjust the bias of the linear classiﬁer such that the distance between boundary sample and decision boundary measured in terms of the distance between boundary sample and closest inner sample is κ = 0.999 (see Section 4.3).  We sample the inner samples uniformly from the (cid:15) hypercube (i.e., the (cid:96)∞ ball), and the boundary samples from the corners of the cube. We opted for this, since it increases the hardness of the test. Further, for calculating the R-ASR we samples both 200 points from the inner and 200 more from the corners of the space, as this signiﬁcantly increased the R-ASR and, thus, gives a more realistic estimate of the test’s difﬁculty.  C Additional Results  Figure 4: Robust accuracy as a function of the test performance. Thicker markers denote results for the attacks originally used by the defenses’ authors, while smaller ones correspond to that of adaptive attacks that broke the defense. The gray arrows between these points indicate how the scores change by using using a better suited attack. Orange points indicate false negatives/non-conclusive test results. Triangles denote defenses leveraging detection algorithms. For these defenses we visualize the minimum of the performance on the regular and inverted tests.  15\n",
      "2 2 0 2  y a M 5 2  ] L M  . t a t s [  1 v 2 4 6 2 1 . 5 0 2 2 : v i X r a  On the Interpretability of Regularisation for Neural Networks Through Model Gradient Similarity  Vincent Szolnoky Department of Mathematical Sciences Chalmers University of Technology Göteborg, Chalmers Tvärgata 3, 41296, Sweden szolnoky@chalmers.se  Viktor Andersson Department of Electrical Engineering Chalmers University of Technology Göteborg, Chalmersplatsen 4, 41296, Sweden vikta@chalmers.se  Balázs Kulcsár Department of Electrical Engineering Chalmers University of Technology Göteborg, Chalmersplatsen 4, 41296, Sweden kulscar@chalmers.se  Rebecka Jörnsten Department of Mathematical Sciences Chalmers University of Technology Göteborg, Chalmers Tvärgata 3, 41296, Sweden jornsten@chalmers.se  1 Abstract  Most complex machine learning and modelling techniques are prone to over-ﬁtting and may subse- quently generalise poorly to future data. Artiﬁcial neural networks are no different in this regard and, despite having a level of implicit regularisation when trained with gradient descent, often require the aid of explicit regularisers. We introduce a new framework, Model Gradient Similarity (MGS), that (1) serves as a metric of regularisation, which can be used to monitor neural network training, (2) adds insight into how explicit regularisers, while derived from widely different principles, operate via the same mechanism underneath by increasing MGS, and (3) provides the basis for a new regularisation scheme which exhibits excellent performance, especially in challenging settings such as high levels of label noise or limited sample sizes.  2  Introduction  Since the inception of modern neural network architecture and training, many efforts have been made to understand their generalisation properties. When neural networks are trained with Gradient Descent (GD) this induces implicit regularisation in the network, causing it to attain surprising generalisation performance with no extra intervention. Despite this, neural networks will in many instances overﬁt in the presence of noise and have been shown to have the capacity to completely memorise data [Zhang et al., 2016]. Therefore, alongside the desire to understand how neural networks are implicitly regularised, explicit regularisation has been an ongoing pursuit to improve generalisation even further.  A broad range of explicit regularisers now exist that attack the problem from many different angles. Below, we provide a short overview of some of the most commonly used methods. Weight decay [Krogh and Hertz, 1991] places a L2-penalty on the parameters to encourage a minimum-norm solution. Normalisation schemes, that normalise the data as it ﬂows through the network, have  Preprint. Under review.  been shown to act as explicit regularisers [Ioffe and Szegedy, 2015]. Double back-propagation incorporates the gradients with respect to the loss itself as part of the loss, hence the name, as the gradient of the gradient will be evaluated in the complete back-propagation step [Drucker and Le Cun, 1991]. Although these methods were initially used as a part of a energy minimisation principle, they are now believed to help ﬁnding \"ﬂat minima\", which have been shown to produce solutions that generalise better [Zhao et al., 2022]. Dropout turns off connections between neurons during training and thus forces the network not to rely on any one given connection [Srivastava et al., 2014]. Another way to view the learning problem is to ignore the network itself and instead focus on the optimisation scheme. Here, a multitude of different learning rate and optimisers exist that offer explicit regularisation. Some standout examples include Cyclical Learning Rate [Li and Yang, 2020] and the Adam optimisation algorithm [Kingma and Ba, 2015]. More recently, especially in works related to the Neural Tangent Kernel (NTK) [Jacot et al., 2018], focus has been put onto functional regularisation. The core idea is to view the neural network architecture as being an opaque function approximator and instead regularise it as it is viewed from function space rather than the parameter space. This is usually achieved via a form of kernel ridge regression or an approximation thereof [Bietti et al., 2019, Hoffman et al., 2019].  Our contribution. The fact that so many different explicit regularisers exist shows that their precise mechanisms are not well understood. Ultimately, this leads back to the insufﬁcient understanding of what causes neural networks to generalise in the ﬁrst place. In this article, a new framework called Model Gradient Similarity (MGS) is introduced. MGS builds on the idea that regularisation and, to a large extent, generalisation, is tightly connected to how the model evolves during training. We show that MGS can be used to analyse neural network training, monitoring how model gradients evolve together. It is revealed that explicit regularisers, despite their fundamentally different construction, operate via the same underlying mechanisms in terms of increasing model gradient similarity. This insight lays the foundation for a new class of regularisers aimed at directly boosting model gradient similarity. We derive two simple MGS regularisation techniques from ﬁrst principles and compare performance to a wide range of regularisation methods. In a majority of the cases, the MGS regularisation achieves top performance and exhibits robustness qualities. This suggests that the MGS framework opens the door for the further development of both novel regularisers and the improvement of existing ones.  3 Model Gradient Similarity  Let x ∈ X denote an input observation in a data batch and y ∈ Y the target. We will train the model fθ, parameterised by θ, using loss function L via gradient descent (GD). Thus, the standard single-observation batch update rule using GD at time i is θi+1 = θi − η∇θL (f (x), y), where η is the learning rate. When training a neural network, fθ shall represent the raw output from the network before any additional normalisation is performed (e.g. softmax as in the case of classiﬁcation).  The loss gradients with respect to the parameters ∇θL (loss-parameter gradient) are thus the main contributing factor to the update of the model. In the general case for common losses, such as mean-squared error or cross-entropy, we apply the chain rule to the loss-parameter gradient: ∇θL = ∇θfθ · ∇f L. We denote the two components as: the loss gradients with respect to the model output ∇f L (loss-model gradients) and the model gradients with respect to the model parameters ∇θfθ (model-parameter gradients).  The idea of gradient similarity/coherence has recently been suggested as an explanation for a neural network’s ability to generalise [Faghri et al., 2020, Chatterjee and Zielinski, 2022]. It is here, however, mainly considered from the perspective of the loss-parameter gradient ∇θL and not the model- parameter gradients ∇θfθ. Contrary to the loss-parameter gradients, which diminish as any minima is approached, the model-parameter gradients display different behaviour and describe the evolution of the function realised by the model. To help understand why they can be useful in understanding neural network generalisation and regularisation thereof, we will borrow some inspiration about gradient similarity from Charpiat et al. [2019] and use it to deﬁne the core ideas behind MGS.  3.1 Deﬁning MGS and its Relation to Generalisation  When judging the similarity of two points x and x(cid:48) from the model’s point of view, one may be inclined to simply compare the output fθ(x) to the other fθ(x(cid:48)). Unfortunately, due to the non-linear  2  Poor generalisation  Good generalisation  Figure 1: An illustration on the connection between MGS and generalisation. The arrows represent the model gradients, ∇fθ, for each data point. Left: A model that exhibits low similarity between its gradients. The model is free to adapt the gradients for each point and therefore learns them individually. The most likely outcome then is that it will overﬁt the data and generalise poorly. Right: A model that is required to maintain a level of similarity between its gradients. To lower the overall loss, while maintaining model gradient similarity, the model is forced to learn from the data in groups deﬁned by similar gradients. Thus, the model will have to learn separations in the data based on such groupings instead of each point individually.  nature of neural networks, the same output might be produced for a number of different reasons and it is therefore difﬁcult to draw conclusions about similarity directly from the output. An alternative notion of similarity between x and x(cid:48) can be deﬁned by how much changing the model output for x (by training on this sample) changes the output for x(cid:48). If the points are dissimilar from the model’s standpoint, then changing fθ(x) should have little inﬂuence over fθ(x(cid:48)) and vice-versa. That is, after one step using GD for a single input sample x the parameters are changed by:  δθ = −η∇θf ∇f L (f (x), y) .  This induces the following change in the output of the model for that speciﬁc x:  fθ+δθ(x) = fθ(x) + ∇θf (x) · δθ + O(||δθ||2)  ≈ fθ(x) − η (∇θf (x) · ∇θf (x)) ∇f L (f (x), y) .  On the other hand, this update will also yield a change in the model for another x(cid:48) given by:  fθ+δθ(x(cid:48)) = fθ(x(cid:48)) + ∇θf (x(cid:48)) · δθ + O(||δθ||2)  ≈ fθ(x) − η (∇θf (x(cid:48)) · ∇θf (x)) ∇f L (f (x), y) .  (1)  (2)  Therefore, the kernel kθ(x, x(cid:48)) = ∇θf (x) · ∇θf (x(cid:48)) (derived from the model-parameter gradients) is crucial to understanding how the model evolves after one step of GD. It determines how much of the loss-model gradient ∇f L (f (x), y) is used to update the model, up to a scaling by the learning rate. Another way to look at it is that kθ(x, x(cid:48)) describes the inﬂuence learning x has over x(cid:48). If kθ(x, x(cid:48)) is large, which occurs when the model gradients are similar, then an update for f (x) will also move f (x(cid:48)) in the same direction. Likewise, if k(x, x(cid:48)) is small, meaning the model gradients are dissimilar, then the update for f (x) will have little affect on f (x(cid:48)). The cartoon example in ﬁgure 1 illustrates how this property can be useful to understand how a model generalises better if it exhibits higher MGS.  3  3.2 The contribution of MGS to Gradient Descent  If equations 1 and 2 above are combined, the complete update for a single step of gradient descent with training data X, is:  fθ+δθ(X) = fθ(X) + ∇θf (X) · δθ + O(||δθ||2)      ≈ fθ(X) − ηKθ(X) · ∇f L (f (X))  · · · . . . · · ·  k(x1, x1) ... k(xn, x1)  fθ(x1) ... fθ(xn)    − η        =        ·     k(x1, xn) ... k(xn, xn)  ∇f L(f (x1), y1) ... ∇f L(f (xn), yn)      .  Here Kθ(X) = k(X, X) is the kernel matrix for data X. The update for each model output fθ(xk) can thus be seen as a weighted average of the loss-model gradients ∇f L (f (xj), yj): n (cid:88)  fθ+δθ(xk) = fθ(xk) − η  k(xk, xj) · ∇f L(f (xj), yj), xk ∈ X.  (3)  j=1  From equation 3, it follows that the kernel ultimately controls the weighting of the loss-model gradients: the greater the similarity exhibited between the model gradients, the greater the averaging effect will be. This partially explains how GD induces implicit regularisation as, unless Kθ(X) is dominated by its diagonal, a model update for observation xk will also utilise the loss gradients of other observations.  Furthermore, it has been observed from the Neural Tangent Theory perspective that Kθ(X) will often align with the ideal kernel Y Y T in classiﬁcation problems, which perfectly discriminates targets. This phenomenon has already been investigated as one of the reasons behind implicit generalisation for neural networks when trained with SGD [Kopitkov and Indelman, 2020, Baratin et al., 2021]. Here, we also see that the alignment will naturally engage the averaging effect, grouping gradients according to their target classes. It is still unclear what the cause of the alignment is. However, MGS provides an explanation as to why such an alignment is useful. More detail on the connection to Neural Tangent Theory is presented in the supplementary material.  To summarise, apart from capturing to what extent the model gradients are aligned with each other, MGS also determines how loss-model gradients for observations that have high MGS with each other are averaged in the GD step.  3.3 Metrics to Quantify MGS  Since the kernel Kθ(X) captures coordinated learning between similar observations (equation 3), it is desirable to identify metrics which can summarise the overall model gradient similarity in a batch of data. To ﬁnd relevant metrics of Kθ we note that the spectrum of Kθ(X) = (∇θf (X))T ∇θf (X) is the same as that of ˜C = ∇θf (X)(∇θf (X))T ; the non-centered version of the sample covariance matrix C = (∇θf (X) − ∇θf (X))(∇θf (X) − ∇θf (X))T . The spectrum of C and ˜C are interlaced as their corresponding eigenvalues λ1 ≤ ˜λ1 ≤ . . . ≤ λn ≤ ˜λn. We also note that the trace and determinant of C are commonly used measures of the overall variance and covariance for a given sample of data. Here, the model-parameter gradients ∇fθ constitute the data. Moreover, the trace and determinant of ˆC give a rough approximation of the same metrics of C. Proofs of these two properties are given in the supplementary material.  Thus, the summary statistics tr Kθ and det Kθ can be seen as approximations of the model gradient variance and covariance. Smaller values for the trace and determinant of the kernel, Kθ, reﬂect a higher degree of model gradient similarity and thus a larger averaging effect in equation 3.  These metrics also serve as proxies for measuring the magnitude of the diagonal elements and relative magnitude of the diagonal-to-off-diagonal elements of Kθ. From this perspective, smaller values for the trace and determinant of the Kθ reﬂect how well the kernel can be summarised by a low-rank representation. Low-rank Kθ can be viewed as indicative of coordinated learning in that it restricts the number of independent directions in which the functions can evolve in equation 3.  4  From a practical standpoint, the exact values of these metrics are not important. Rather, we wish to monitor how they evolve during training. Utilising the kernel also constitutes a pragmatic solution since calculating C, of size p × p, is near infeasible for larger networks, whereas Kθ is only of size n × n, where p and n are the number of parameters and data batch respectively, and generally n (cid:28) p.  4 Tracking MGS Metrics During Training  Let us now investigate how the metrics tr Kθ and det Kθ evolve during training. Using these metrics, a wide range of regularisers, representing the most commonly used ones, are compared. Additionally, two new regularisation methods based on MGS are plotted alongside as a reference. These are introduced in the next section.  The results are shown in ﬁgures 2 and 3. Figure 2 presents results from a simple FCN architecture trained on a toy problem generated from two concentric cirles perturbed with noise. Figure 3 stem from training a large convolution network (AlexNet) on a corrupted version of MNIST with label noise. In each example, despite being different problems and different architectures, all regularisation methods exhibit the same behaviour: when regularisation is applied the rate of MGS metric growth is decreased, indicating higher model gradient similarity. Furthermore, the test accuracy or generalisation performance (ﬁgure 3) is strongly correlated with the evolution of the MGS metrics. The slower the rate of MGS growth, the better the ﬁnal generalisation, and when the growth plateaus so does the test accuracy.  We list some interesting observations from the MNIST example in ﬁgure 3:  The unregularised network (gray) goes through a rapid boost in accuracy and then quickly overﬁts. When test accuracy peaks, there is a small plateau in the MGS metrics. However, once it starts to decline in accuracy, the MGS metrics again grow, indicating model gradient similarity is decreased.  Weight penalty (blue) is able to regularise the network initially, but also eventually overﬁts. This is reﬂected in the MGS metrics as they plateau when test accuracy peaks and then start to increase at a seemingly proportional rate to the decline in performance.  All methods that achieve high test accuracy control the growth of the MGS metrics. Still, many of them begin to overﬁt towards the end of training which coincides with the MGS metrics also gradually increasing.  Only the MGS penalty (to be introduced in the next section) is able to maintain a stable test  accuracy  Dropout (brown) has gaps in its determinant metric. This means that it is causing Kθ to be  singular which could have ramiﬁcations on the stability of the training.  More extensive test bench experiments are provided in section 6.  5 MGS regularisation  In the previous section, we saw a clear connection between boosting MGS (reﬂected by lower MGS metrics) and the ability of the network to generalise. The effect on MGS by the explicit regularisers was common to all the methods despite their widely differing design. Thus, we may think of each regularisation method as acting as a proxy for enforcing model gradient similarity. This motivates the construction of a new regularisation scheme that directly optimises MGS.  We thus modify the original loss to include a new penalty term g(Kθ(X))) which acts on the MGS kernel:  (cid:98)L(f (X), Y ) = L(f (X), Y ) + g(Kθ(X))  We already have two prime candidates for g, namely the previously investigated MGS metrics:  gtr(Kθ(X)) = α tr Kθ(X) = α  (cid:88)  λi  i  gdet(Kθ(X)) = α det Kθ(X) = α log  (cid:32)  (cid:89)  (cid:33)  λi  ,  i  5  Figure 2: FCN on two-circle classiﬁcation. Top: Decision boundaries are shown to visualise how well the network has generalised for each method. Bottom: The MGS metrics for each regulariser are tracked during the training period. The connection between network generalisation and the MGS metrics is clear. For an unregularised network which overﬁts the data, MGS metrics grow rapidly. Once any explicit regularisation is used, MGS metric growth is slowed. Coupled with this, the decision boundaries approach the true model for methods that constrain the MGS metrics better. (Complete set of decision boundaries are provided in the supplementary material.)  Figure 3: AlexNet on Corrupted MNIST. Left: Test accuracy. Middle and Right: MGS metrics. Test accuracy is clearly coupled to MGS evolution. All methods that yield high ﬁnal test accuracy exhibit small and slow-increasing MGS metrics, indicating a higher degree of model gradient similarity.  6  where α is a penalty factor and λi are the eigenvalues from the spectral decomposition of Kθ. As previously mentioned in section 3.3, smaller values for these penalty metrics reﬂect higher model gradient similarity. Adding an explicit penalty on these metrics thus forces the network to learn data in groups and preventing it from learning points individually (over-ﬁtting or memorization).  The evolution of the two were shown in ﬁgures 2 and 3. The metrics seem to behave similarly. However, gtr(Kθ) is more efﬁcient to compute as it does not require either the complete spectral decomposition nor the full gradient similarity kernel to be computed. However, since both metrics are gradient based, they are, of course, computationally more burdensome than commonly used regularisers such as weight decay. For calculations of the penalties we make use of Neural Tangents [Novak et al., 2019] and Fast Finite Width NTK [Novak et al., 2022] libraries which are built on JAX [Bradbury et al. [2018]].  We note that the penalties are equivalent to penalising the arithmetic and geometric mean of the eigen- values of the kernel, respectively. The former will mainly be affected by large leading eigenvalues. The latter essentially penalises the eigenvalues of the kernel on a log-scale and may thus take more of the overall structure of the kernel into account. Supplemental ﬁgure A.1 depicts results on the toy two-circle data from using both penalties. The differences are subtle. For pragmatic considerations, on real data we therefore utilise only the trace penalty for explicit regularisation, but both metrics can be used for monitoring neural network training.  Details on how Kθ is calculated with regards to mini-batches, vector-outputs and large data sets are provided in the supplementary material.  6 Experiments  We compare the performance of optimising MGS directly versus the most common, explicit regu- larisers. Although there exists a plethora of explicit regularisers and modiﬁcations thereof, the ones chosen are a representative of the main types of regularisers with the most widespread use in practice. As the aim is to compare performance between regularisers, a systematic investigation is done where the regularisers are rigorously tested against common overﬁtting scenarios caused by target noise and training size. Two main tests are done, one on a classiﬁcation problem and the other on a regression problem. For each test two different types of architectures are chosen. This is to test how universal each method is in handling vastly different problems and networks. Finally, a test of robustness to variability found in common training setups is performed. Complete details on the experiments, complete results and code to run them can be found in the supplementary material.  6.1 Classiﬁcation: corrupted MNIST  We generate a corrupted version of the popular MNIST dataset by applying a motion blur to each image. A testbench of challenging scenarios are created by varying the amount of label noise and training size. Each regularisation method is tuned in an identical setting and then retains the chosen parameters for the entire testbench. This ensures an even playing ﬁeld and that each method is given the same starting point. For each testing scenario, multiple runs are performed using a new network initialisation and resampling of the training data.  Table 1 shows the results for 4 scenarios from the 143 scenario large testbench (see supplemental). In this scenario 3000 samples are used for training at 4 noise levels. It’s immediately clear that MGS achieves the best performance overall in many aspects. Not only does it reach higher accuracy levels than the other methods, it’s also robust in its performance. This can be seen both by the standard deviation of its ﬁnal test accuracy, but also when comparing the max test accuracy to its ﬁnal one. For MGS max and ﬁnal test accuracy essentially coincide, indicating that MGS regularised networks do not overﬁt. The same conclusions hold true for a fully connected network (see supplemental) where performance is overall worse for all methods but MGS outperforms the others.  6.2 Regression: Facial Keypoints  A similarly corrupted version of the Facial Keypoints data set was used as a regression benchmark. Based on images of human faces, the problem is to predict the coordinates of 15 key facial features. Each image is ﬁrst corrupted using a motion blur. Noise is then introduced by adding normally  7  Table 1: Test accuracy for corrupted MNIST dataset using a LeNet-5 architecture. Final test accuracy and one standard deviation is shown with the maximum test accuracy in parenthesis underneath. Noise column represents percentage of training labels that have been randomly ﬂipped. Note the ability of MGS to handle large amounts of noise. Comparing max accuracy (in parenthesis) during training to ﬁnal accuracy, MGS is also the most consistent and is not susceptible to overﬁtting.  Noise Unregularised  Dropout  Weight  Loss grad.  MGS  0%  30%  60%  80%  89.6 ±1.3 (90.2) 62.0 ±2.5 (85.2) 37.3 ±2.9 (73.7) 23.9 ±2.4 (62.4)  95.8 ±0.5 (95.9) 80.9 ±2.9 (92.0) 59.2 ±4.1 (83.7) 39.5 ±4.7 (56.6)  83.1 ±9.6 (87.1) 72.2 ±4.2 (79.2) 10.0 ±0.4 (62.3) 10.1 ±0.5 (23.5)  95.1 ±0.5 (95.1) 88.7 ±2.0 (92.0) 80.7 ±5.9 (81.4) 15.1 ±5.4 (17.2)  95.2 ±0.3 (95.2) 93.1 ±0.7 (93.4) 88.4 ±1.4 (89.1) 74.5 ±4.0 (75.1)  Table 2: Test loss for the corrupted Facial Keypoints dataset using a LeNet-5 architecture. Final test loss and one standard deviation is shown with the minimum test loss in parenthesis underneath. Only Dropout and MGS achieve an acceptable level of ﬁnal test loss. The other methods also provide a marginal increase in performance compared to the unregularised network. In the supplemental we compare results on a FCN architecture for which only MGS is able to attain good performance.  Noise Unregularised  Dropout  Weight  Loss grad.  MGS  0  10  20  30  68.2 ±36.7 (54.2)  54.8 ±24.0 (45.2)  102.0 ±116.2 (48.2) 94.8 ±131.5 (39.0)  14.9 ±2.2 (13.9)  15.4 ±2.2 (14.4)  16.6 ±2.2 (15.3) 19.7 ±2.8 (17.7)  113.2 ±53.2 (90.3)  92.6 ±46.5 (69.0)  83.8 ±52.7 (49.3) 70.6 ±58.0 (41.9)  212.4 ±174.1 (104.0) 235.2 ±188.6 (140.5) 266.1 ±233.1 (115.9) 229.4 ±225.7 (151.5)  14.1 ±1.0 (13.8)  14.3 ±1.1 (13.6)  15.3 ±1.3 (14.7) 17.8 ±2.0 (16.9)  distributed values, using different scale parameters, to the target variables. Here, we ﬁxed the number of training samples while the amount of target noise was varied. Each method is tuned using the same scenario ﬁrst. Similar conclusions can be drawn as from the classiﬁcation results. The results are visible in table 2 where the noise column corresponds to the scale parameter of the normally distributed noise. Interestingly, only Dropout and MGS were actually able to converge to a satisfactory performance level. However, when results are compared for a FCN architecture (table A.2, supplemental), Dropout falls into the same category as the other methods while MGS performance remains high. This shows a weakness in a method such as Dropout: that it is inevitably architecture dependent.  6.3 Training parameter robustness  Finally, we test the robustness of each regularisation method by changing: amount of training size, label noise, batch size, learning rate, and epochs. A middle-ground scenario was chosen from this MNIST testbench as a starting point to tune each method. Then, the ﬁve training variables were changed individually to both higher and lower values, tracking the performance of each method after training. Consistent with the current ﬁndings, MGS outperforms each method substantially. In ﬁgure 4 we use radar charts to summarise the test bench results.  We note that MGS is the least affected by a change in all but one of the test parameters or conditions, and exhibits superior performance compared to the other regularisers. Performance is only affected by learning rate to some extent. This is not unexpected as the same can be seen for the other methods  8  Figure 4: Test accuracy quantiles after varying different parameters controlling training. MGS is plotted in red against the other methods. MGS outperforms the other regularisers, and is also the most robust with regards to changes in the training setup, in all but one case. The only area in which it shows a degradation in performance is when the learning rate is changed. However this seems to affect all methods, but weight penalty to a lesser degree.  apart from weight penalty. Also, as is evident from the GD update step (equation 1), the one directly contributing training parameter in MGS is the learning rate.  7 Conclusion  We introduced the concept of Model Gradient Similarity (MGS) and discussed its connections to regularisation for models trained with gradient descent and the generalisation properties of neural networks. We proposed metrics that can be used to summarise MGS for a model and track the training of different neural network architectures for various learning problems.  It was shown that a wide range of explicit regularisers all appeared to attempt to enforce higher model gradient similarity, i.e. lower MGS metrics. Moreover, higher test accuracy performance was shown to be reﬂected in lower MGS metric values.  Based on these ﬁndings, a new type of regulariser, geared toward direct control of MGS, was designed and found to achieve top performance in several rigorous test bench experiments. Its overall robustness to label noise and training parameter settings was also an indication that directly optimising MGS comes closer to a more holistic approach to regularisation.  Taken together, these results provided insight into the underlying mechanisms of neural network regularisation. Due to the higher computational costs for gradient based regularisers, such as the MGS metric penalities introduced here or loss-gradient penalties, their use for direct optimisation is not always efﬁcient. To scale for use in larger networks and in more complex settings, additional work is needed to obtain more efﬁcient ways to compute or approximate the MGS kernel or metrics thereof. Future work could thus focus both on how MGS can be used to design new regularisers as well as to improve upon existing ones. The MGS metrics can be useful as KPI’s for measuring a network’s current capacity for under/over-ﬁtting. Finally, the grouping effect of regularised neural network training, where model gradient similarity encourages coordinated learning across observations, suggests that MGS regularisation can be explored for joint prediction modeling and clustering.  Acknowledgments and Disclosure of Funding  This research was supported by funding from Centiro Solutions, the Swedish Research Council (VR), the Swedish Foundation for Strategic Research, and the Wallenberg AI, Autonomous Systems and Software Program (WASP).  9  References  Aristide Baratin, Thomas George, Csar Laurent, R Devon Hjelm, Guillaume Lajoie, Pascal Vincent,  and Simon Lacoste-Julien. Implicit regularization via neural feature alignment, 2021.  David Barrett and Benoit Dherin. Implicit gradient regularization. In International Conference on  Learning Representations, 2021.  Alberto Bietti, Grégoire Mialon, Dexiong Chen, and Julien Mairal. A kernel perspective for regulariz- ing deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 664–674. PMLR, 09–15 Jun 2019.  James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and JAX: composable transformations of Python+NumPy programs, 2018. URL Qiao Zhang. http://github.com/google/jax.  Guillaume Charpiat, Nicolas Girard, Loris Felardos, and Yuliya Tarabalka. Input similarity from the neural network perspective. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.  Satrajit Chatterjee and Piotr Zielinski. On the generalization mystery in deep learning, 2022. URL  https://arxiv.org/abs/2203.10036.  H. Drucker and Y. Le Cun. Double backpropagation increasing generalization performance. In IJCNN-91-Seattle International Joint Conference on Neural Networks, volume ii, pages 145–150 vol.2, 1991. doi: 10.1109/IJCNN.1991.155328.  Fartash Faghri, David Duvenaud, David J. Fleet, and Jimmy Ba. A study of gradient variance in deep  learning, 2020. URL https://arxiv.org/abs/2007.04532.  Judy Hoffman, Daniel A. Roberts, and Sho Yaida. Robust learning with jacobian regularization,  2019.  Paul Honeine. An eigenanalysis of data centering in machine learning, 2014. URL https://arxiv.  org/abs/1407.2904.  Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ICML’15, page 448–456. JMLR.org, 2015.  Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa- Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.  Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),  2015.  Dmitry Kopitkov and Vadim Indelman. Neural spectrum alignment: Empirical study. In Igor Farkaš, Paolo Masulli, and Stefan Wermter, editors, Artiﬁcial Neural Networks and Machine Learning – ICANN 2020, pages 168–179, Cham, 2020. Springer International Publishing.  Anders Krogh and John A. Hertz. A simple weight decay can improve generalization. In Proceedings of the 4th International Conference on Neural Information Processing Systems, NIPS’91, page 950–957, San Francisco, CA, USA, 1991. Morgan Kaufmann Publishers Inc.  Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl- Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32, pages 8572–8583. Curran Associates, Inc., 2019.  10  Jiaqi Li and Xiaodong Yang. A cyclical learning rate method in deep learning training. In 2020 International Conference on Computer, Information and Telecommunication Systems (CITS), pages 1–5, 2020. doi: 10.1109/CITS49457.2020.9232482.  Jörg Martin and Clemens Elster.  Inspecting adversarial examples using the ﬁsher information.  Neurocomputing, 382:80–86, 2020. doi: https://doi.org/10.1016/j.neucom.2019.11.052.  Norman Mu and Justin Gilmer. Mnist-c: A robustness benchmark for computer vision, 2019. URL  https://arxiv.org/abs/1906.02337.  Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. Neural tangents: Fast and easy inﬁnite neural networks in python, 2019. URL https://arxiv.org/abs/1912.02803.  Roman Novak, Jascha Sohl-Dickstein, and Samuel Stern Schoenholz. Fast ﬁnite width neural tangent  kernel. In Fourth Symposium on Advances in Approximate Bayesian Inference, 2022.  Basri Ronen, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural networks for learned functions of different frequencies. In H. Wallach, H. Larochelle, A. Beygelz- imer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.  Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overﬁtting. J. Mach. Learn. Res., 15(1): 1929–1958, jan 2014.  Yoshio Takane and Tadashi Shibayama. Principal component analysis with external information on both subjects and variables. Psychometrika, 56(1):97–120, Mar 1991. doi: 10.1007/BF02294589.  Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization, 2016. URL https://arxiv.org/abs/1611.03530.  Yang Zhao, Hao Zhang, and Xiuyuan Hu. Penalizing gradient norm for efﬁciently improving  generalization in deep learning, 2022. URL https://arxiv.org/abs/2202.03599.  11  Supplementary Material  A Overview of Popular Regularisation Methods  First we give a brief overview of the regularisation methods tested. The main goal here is to compare a wide variety of different schemes to see if they have any common behaviour.  A.1 Weight decay  One of the most used methods for regularisation is placing a (cid:96)2-penalty on the parameters of the network. Colloquially this is known as Weight Decay [Krogh and Hertz, 1991]. It is a well understood method from classical regression models, where estimation variance is reduced by regularising large weights or regression coefﬁcients. Despite its easy interpretation for these classical models, its application to neural networks is not completely understood.  A.2 Dropout  Dropout [Srivastava et al., 2014] is another commonly used regularisation method that works by overlaying noise during the learning procedure. The output from each neuron in the network is discarded at random during training according to a set distribution. Intuitively, by adding this noise, the network is forced to not rely on any given path through the network which could be used to memorise data.  A.3 Learning rate algorithms and optimisers  There exist many optimisers other than standard SGD that also offer regularisation beneﬁts. Optimis- ers such as Adam [Kingma and Ba, 2015] set an adaptive learning rate by using estimations of ﬁrst and second order terms. On the other simple but effective learning rate algorithms such as Cyclical Learning Rate [Li and Yang, 2020] simply adjust the learning rate in a more nuanced way that leads to a better learning procedure without using any past training information.  A.4 Loss gradient norm penalties  Loss gradient norm penalties add an additional penalty term to the loss: the norm of the gradient of the loss itself. This will lead the gradients of the gradients being calculated in the ﬁnal back-propagation step, hence why it is commonly referred to as double back-propagation. First, a back-propagation pass must be done to obtain the gradients, then a second pass to evaluate the weight updates with the gradient norm included. Two versions exist: one which calculates the gradient of the loss with respect to the input data [Drucker and Le Cun, 1991, Hoffman et al., 2019] and another that instead does so with respect to the parameters [Barrett and Dherin, 2021, Zhao et al., 2022]:  ˜L = L + ||∇xL|| ˜L = L + ||∇θL||.  We note one important detail in the relation of the latter, the loss-parameter gradient norm, to the MGS trace penalty:  ||∇θL|| = ||∇θf · ∇f L|| ≤ ||∇θf || ||∇f L||.  Therefore, ||∇θL|| is proportional to the MGS trace penalty as ||∇θf ||2 tr Kθ. However depending on how the second term ||∇f L|| behaves the two might not be coupled at all. This might explain why in some instances in the experiments, the loss-parameter gradient penalty performed signiﬁcantly worse. However, it is still the closest to directly optimising MGS and when the loss- model gradient is well behaved, can perform just as well as MGS. Still, the issue arises as to how this penalty performs when any minima is approached, as the second term will diminish, causing the whole penalty to lose inﬂuence over training. This might then lead to the model starting to overﬁt after landing in a minima, which we have observed in our experiments.  2 =  √  12  A.5 Functional regularisation  Recently it has also been shown that Kernel Ridge Regression can be applied to neural networks. This places a penalty on function complexity by trying to minimise the norm of the function given by its Reproducing Kernel Hilbert Space (RKHS) (e.g. [Bietti et al., 2019]). For neural networks, the NTK can be used to deﬁne such a RKHS, however the actual function realised by the network is not necessarily close to its RKHS counterpart. Additionally, in our experience, these penalties are the most costly to calculate and were not efﬁcient enough even to run on small networks.  B Connection to Neural Tangent Theory  It should most certainly be noted that the MGS kernel is indeed the same as the empirical Neural Tangent Kernel ˆΘ which is derived from the Neural Tangent Kernel (NTK) itself. From the NTK literature, the NTK describes the evolution of the network function fθ in function space in a kernel gradient descent setting. Therefore we can also make use of ﬁndings that have been made from the NTK perspective.  A observation made by about the empirical NTK is the notion of \"feature learning speed\" [Jacot et al., 2018, Baratin et al., 2021, Ronen et al., 2019]. Neural networks seem to learn functions/features of increasing complexity starting with low frequency content ﬁrst and then sequentially higher frequency information as training progresses. The spectral decomposition of the empirical NTK describes the primary directions of learning with large eigenvalues corresponding to learning directions with low complexity which is also where convergence is the fastest. While most of the ideas from the NTK side are formulated by viewing the network as learning in function space, it can be useful to see what practical implications this has on the model gradients and therefore MGS.  Due to the NTK growing in popularity there exist a number of performant libraries for calculating the empirical NTK. We make use of Neural Tangents [Novak et al., 2019] and Fast Finite Width NTK [Novak et al., 2022] libraries which are built on JAX [Bradbury et al. [2018]].  C Using MGS metrics for adversarial sample detection  The authors Martin and Elster [2020] used tr Kθ for detection of adversarial samples when running a neural network. They do this by calculating the trace of what they call the approximate Fisher Information Matrix (FIM) which is the actually same as Kθ in our case. They observed that if adversarial samples were present in the input, then the trace of the approximate FIM would be large. Their reasoning behind this behaviour, however, was deduced from the properties of the complete FIM. From the gradient similarity perspective it also makes sense. Adversarial samples are intended to look very similar to trained samples, but corrupt the model by causing it to produce different outputs. Therefore, if for adversarial data the trace is large, then the network considers that data to be very different from existing data. This also makes sense as that is one of the objectives of adversarial attacks in the ﬁrst place.  D Experiments  All experiments using a FCN were run locally on a laptop with a Nvidia Quadro T2000 graphics card. For convolutional networks, the laptop graphics card was not sufﬁcient so instead a single Nvidia K80 was used on another machine.  D.1 Experiment details  Classiﬁcation: MNIST A corrupted version (motion blur) of MNIST was used from the MNIST-C dataset [Mu and Gilmer, 2019]. Each method was tuned using a simple grid search over a set of parameters. The scenario in which the tuning took place was using 6000 randomly stratiﬁed sampled training points and 50% of the target labels were randomly ﬂipped. Each method was trained for 100 epochs and also run 5 times over, with differently randomly sample data each time. The ﬁnal test loss was used to determine the optimal parameter. The architectures tested included a standard fully connected architecture with 6 hidden layers, 300 neurons wide, and ReLU activations as well as a  13  LeNet-5 style convolutional network. Cross-entropy with softmax was used as the loss function. In the case of Dropout, a Dropout layer was added between all fully connected layers in both architectures.  After tuning, each method retained the same parameters and was then run over a large testbench, for 400 epochs for both networks, where the training size and label noise were varied between 250 and 9000 training samples and 0% to 100% respectively. The test accuracy was calculated on the pre-determined test sample found in MNIST and not the data that was left over from the training sample. Each scenario was run 10 times, again with new network initialisations and sampled training data. A slow exponential decay was used for the learning rate starting at 0.1 and a batch size of 32 was used.  Each of the metrics, including test accuracy, were sampled at 100 evenly spaced points during training, so as to not run too slowly. The ﬁnal test accuracy was calculated as an average of the previous 5 metric samples and then averaged between the training runs. The max value was calculated as the max of the averaged runs.  Regression: Facial Keypoints A similarly corrupted version (motion blur) of the Facial Keypoints dataset 1 was used. The same architecture types were used as in the MNIST experiment. Standard MSE loss is used. Unlike the MNIST experiment, a grid of scenarios was not run for this problem. Instead the training size was held ﬁxed at 30% of the total data, while the amount of noise was changed. Gaussian noise was added to the target coordinates with a scale indicated by the “noise” column in the tables. Each method was tuned with a noise scale of 15 for 50 epochs.  Each scenario was then run for 400 epochs in the case of the FCN architecture and 50 epochs for the LeNet-5 architecture. This was due to the larger size of the input and output, causing the convolutional network to run much slower. A batch size of 128 was used for both architectures in an attempt to lessen the training wall-clock time. This could explain why MGS in some cases achieved slightly better performance for the FCN architecture over the LeNet-5 architecture as it was allowed to train for longer.  The metrics were tracked in the same way as the MNIST experiment, however test MSE was used instead of test accuracy.  Training parameter robustness To test robustness with regards to different training parameters, a benchmark was run where each method is ﬁrst tuned using the same scenario. Then, the tuned scenario is used as a starting point from which each of the 5 training parameters were changed independently. Both larger and smaller parameter values were selected to test as broad of a spectrum as possible. For each parameter changed, multiple runs were performed, again by resampling the data and using a different initialisation. The ﬁnal performance results were aggregated, with the quantiles being drawn, based on all the results. The test was run using the same classiﬁcation problem based on the corrupted MNIST dataset described previously. Speciﬁcally, each method was tuned using 3000 training samples, 50% label noise, and for 100 epochs with all the other settings such as learning rate the same as per section D.1.  D.2 Results for MNIST and Facial Keypoints for a FCN network  1https://www.kaggle.com/competitions/facial-keypoints-detection/  14  Table A.1: Test accuracy for corrupted MNIST dataset using a FCN architecture. Noise column represents percentage of training labels that have been randomly ﬂipped. MGS is able to handle large amounts of noise. If the max accuracy is compared to the ﬁnal accuracy, MGS is also the most consistent and is not susceptible to overﬁtting. The FCC results are worse than those of LeNet-5.  Noise Unregularised  Dropout  0%  30%  60%  80%  82.1 ±1.9 (82.3) 59.5 ±3.7 (75.3) 36.9 ±2.8 (65.6) 24.8 ±2.7 (53.0)  79.5 ±1.2 (80.0) 73.7 ±2.5 (75.5) 55.6 ±4.8 (57.1) 26.5 ±3.7 (29.6)  Weight  72.8 ±9.7 (79.6) 58.7 ±4.0 (74.6) 10.0 ±0.5 (63.1) 10.1 ±0.6 (41.3)  Loss grad.  74.9 ±22.0 (79.8) 70.3 ±4.3 (77.4) 41.2 ±6.4 (43.4) 12.1 ±2.5 (14.8)  MGS  84.6 ±1.2 (84.8) 77.7 ±1.8 (79.4) 67.8 ±3.4 (70.0) 51.4 ±3.8 (55.0)  Table A.2: Test loss for the corrupted Facial Keypoints dataset using a FCN architecture. MGS is the only regulariser that is able to attain convergence with good performance. Dropout  Noise Unregularised  Loss grad.  Weight  MGS  0  10  20  30  281.1 ±264.6 (181.5)  208.2 ±220.9 (74.3)  305.7 ±344.7 (176.8)  240.2 ±291.1 (191.7)  325.2 ±371.7 (158.6)  390.8 ±453.2 (162.6)  287.1 ±321.9 (88.0) 210.4 ±270.9 (91.7)  276.4 ±262.3 (180.2) 305.2 ±342.2 (175.0) 328.5 ±379.4 (157.7) 393.0 ±454.2 (162.2)  337.9 ±303.8 (184.7)  10.6 ±0.3 (10.5)  335.3 ±332.1 (208.2)  11.0 ±0.5 (10.7)  353.1 ±382.7 (174.2)  13.6 ±1.3 (12.8)  411.8 ±442.8 (175.6)  22.4 ±2.7 (20.9)  15  E Two circles problem decision boundaries  16  Figure A.1: Decision boundaries for all regularisation methods.  17  F MNIST testbench results  18  Figure A.2: Test accuracy during training using a FCN architecture. Test accuracy plotted during training for two training sizes and different levels of label noise.  Figure A.3: Test accuracy during training using a LeNet-5 architecture. Test accuracy plotted during training for two training sizes and different levels of label noise.  19  Figure A.4: Unregularised LeNet-5. Average ﬁnal test accuracy for MNIST. Label noise on the x-axis and training size on the y-axis.  20  Figure A.5: LeNet-5 regularised with weight penalty. Average ﬁnal test accuracy with label noise on the x-axis and training size on the y-axis.  21  Figure A.6: LeNet-5 regularised with dropout. Average ﬁnal test accuracy with label noise on the x-axis and training size on the y-axis.  22  Figure A.7: LeNet-5 regularised with loss-gradient penalty (parameter). Average ﬁnal test accuracy with label noise on the x-axis and training size on the y-axis.  23  Figure A.8: LeNet-5 regularised with MGS penalty (trace). Average ﬁnal test accuracy with label noise on the x-axis and training size on the y-axis.  24  Figure A.9: Unregularised FCN. Average ﬁnal test accuracy for MNIST. Label noise on the x-axis and training size on the y-axis.  25  Figure A.10: FCN regularised with weight penalty. Average ﬁnal test accuracy with label noise on the x-axis and training size on the y-axis.  26  Figure A.11: FCN regularised with dropout. Average ﬁnal test accuracy with label noise on the x-axis and training size on the y-axis.  27  Figure A.12: FCN regularised with loss-gradient penalty (parameter). Average ﬁnal test accuracy with label noise on the x-axis and training size on the y-axis.  28  Figure A.13: FCN regularised with MGS penalty (trace). Average ﬁnal test accuracy with label noise on the x-axis and training size on the y-axis.  29  G Calculation of Kθ  G.1 Vector output models  In section 3 Kθ is deﬁned for a scalar output model and therefore is a matrix of size n × n. In the case of a vector output model Kθ becomes a tensor of size n × n × q × q where q is the number of outputs. The reasoning about MGS still holds. However, the calculations become more involved as one not only has to consider the model update for a single output (given by the diagonal matrices K i,j θ , i = j) but also how an update for one output affects other outputs (given by the off diagonal matrices K i,j θ , i (cid:54)= j). Therefore, it is easier to instead combine all the outputs by concatenating their model gradients and then calculate a single kernel Kθ of size np × np, as is common in the NTK literature [Lee et al., 2019]:  ∇θf = [∇θf1; · · · ; ∇θfq].  G.2 Handling of mini-batches and large data sets  When using mini-batches, Kθ is no longer square and instead will be of size n × m, where m is the number of samples in the current batch.  Both this matrix and the n × n matrix from standard gradient descent can be infeasible to calculate for large data sets. Instead, an approximation is calculated using only the data from the current batch which will be of size m × m. Throughout the entire article, Kθ is always calculated for the current batch and not the full dataset.  H Relation between spectrum of MGS kernel and gradient sample  covariance matrix  Proposition 1 The matrices X T X (Gram matrix) and XX T (scatter matrix) share the same non- zero eigenvalues. Proof: Suppose that λ is a non-zero eigenvalue of X T X with the associated eigenvector u.  Then:  X T Xu = λu XX T Xu = Xλu XX T (Xu) = λ(Xu) XX T ˜u = λ˜u.  Thus, λ is an eigenvalue of XX T , with the associated eigenvector ˜u = XU.(cid:4)  Theorem 1 Separation theorem [Takane and Shibayama, 1991].  Let M be a d-by-n matrix. Let two orthogonal projection matrices be Pleft and Pright of size d-by-d and n-by-n respectively.  Then:  σj+t(M ) ≤ σj(PleftM Pright) ≤ σj(M ), where σj(·) denotes the j-th largest singular value of the matrix, while t = d − r(Pleft) + n − r(Pright) and r(·) is the rank of the matrix.  Theorem 2 Let C and ¯C be the scatter matrix and its centred counterpart, then their eigenvalues are interlaced, such that [Honeine, 2014]:  λj+1 ≤ ¯λj ≤ λj.  Proof: Let M = X, Pleft being the d-by-d identity matrix and Pright = (I − 1 n-by-n centering matrix.  n 11T ) which is the  30  With r(Pleft) = d and r(Pright) = n − 1, it follows from the Separation theorem 1 that:  σj+1(X) ≤ σj(X − (I −  1 n  11T )) ≤ σj(X)  Furthermore it is well known that the eigenvalues of C are equal to the square roots of the singular values of X.(cid:4)  By using proposition 1 and theorem 2 we see that the eigenvalues of the MGS kernel, which is the Gram matrix for the model-gradients, and the centred scatter matrix (sample-covariance matrix) will be interlaced.  31\n",
      "2 2 0 2  l u J  0 2  ]  V C . s c [  1 v 5 3 0 0 1 . 7 0 2 2 : v i X r a  Fully Sparse 3D Object Detection  Lue Fan CASIA  Feng Wang TuSimple  Naiyan Wang TuSimple  Zhaoxiang Zhang CASIA  {fanlue2019, zhaoxiang.zhang}@ia.ac.cn  {feng.wff, winsty}@gmail.com  Abstract  As the perception range of LiDAR increases, LiDAR-based 3D object detection becomes a dominant task in the long-range perception task of autonomous driv- ing. The mainstream 3D object detectors usually build dense feature maps in the network backbone and prediction head. However, the computational and spatial costs on the dense feature map are quadratic to the perception range, which makes them hardly scale up to the long-range setting. To enable efﬁcient long-range LiDAR-based object detection, we build a fully sparse 3D object detector (FSD). The computational and spatial cost of FSD is roughly linear to the number of points and independent of the perception range. FSD is built upon the general sparse voxel encoder and a novel sparse instance recognition (SIR) module. SIR ﬁrst groups the points into instances and then applies instance-wise feature extraction and predic- tion. In this way, SIR resolves the issue of center feature missing, which hinders the design of the fully sparse architecture for all center-based or anchor-based detectors. Moreover, SIR avoids the time-consuming neighbor queries in previous point-based methods by grouping points into instances. We conduct extensive experiments on the large-scale Waymo Open Dataset to reveal the working mechanism of FSD, and state-of-the-art performance is reported. To demonstrate the superiority of FSD in long-range detection, we also conduct experiments on Argoverse 2 Dataset, which has a much larger perception range (200m) than Waymo Open Dataset (75m). On such a large perception range, FSD achieves state-of-the-art perfor- mance and is 2.4× faster than the dense counterpart. Codes will be released at https://github.com/TuSimple/SST.  1  Introduction  Autonomous driving systems are eager for efﬁcient long-range perception for downstream tasks, espe- cially in high-speed scenarios. Current 3D LiDAR- based object detectors usually convert sparse features into dense feature maps for further feature extrac- tion and prediction. For simplicity, we name the detectors utilizing dense feature maps as dense de- tectors. Dense detectors perform well on current popular benchmarks [30, 7, 2], where the percep- tion range is relatively short (less than 75 meters). However, it is impractical to scale the dense detec- tors to the long-range setting (more than 200 meters, Fig. 2). In such settings, the computational and spa- tial complexity on dense feature maps is quadratic to the perception range. Fortunately, the sparsity of LiDAR point clouds also increases as the perception range extends (see Fig. 2), and the calculation on the  Preprint. Under review.  Figure 1: Illustration of center feature miss- ing and feature diffusion on dense feature maps from Bird’s Eye View. The empty in- stance center (red dot) is ﬁlled by the features diffused from occupied voxels (with LiDAR points), after several convolutions.  unoccupied area is essentially unnecessary. Given the inherent sparsity, an essential solution for efﬁcient long-range detection is to remove the dense feature maps and make the network architectures fully sparse.  However, removing the dense feature map is non-trivial since it plays a critical role in current designs. Commonly adopted sparse voxel encoders [37, 5, 28] only extract the features on the non-empty voxels for efﬁciency. So without dense feature maps, the object centers are usually empty, especially for large objects. We name this issue as “Center Feature Missing (CFM)” (Fig. 1). Almost all popular voxel or pillar based detectors [27, 5, 41, 29, 37] in this ﬁeld adopt center-based or anchor- based assignment since the center feature is the best representation of the whole object. However, CFM signiﬁcantly weakens the representation power of the center voxels, even makes the center feature empty in some extreme cases like super large vehicles. Given this difﬁculty, many previous detectors [37, 41, 27, 5] have to convert sparse voxels to dense feature maps in Bird’s Eye View after the sparse voxel encoder. Then they resolve the CFM issue by applying convolutions on the dense feature maps to diffuse features to instance centers, which we name as feature diffusion (Fig. 1).  To properly remove the dense feature map, we then investigate the purely point-based detectors because they are naturally fully sparse. However, two draw- backs limit the usage of point-based methods in the autonomous driving scenario. (1) Inefﬁciency: The time-consuming neighborhood query [23] is the long- standing difﬁculty to apply it to large-scale point cloud (more than 100K points). (2) Coarse represen- tation: To reduce the computational overhead, point- based methods aggressively downsample the whole scene to a ﬁxed number of points. The aggressive downsampling leads to inevitable information loss and insufﬁcient recall of foreground objects [39, 42]. As a result, very few purely point-based detectors have reached state-of-the-art performance in the re- cent benchmarks with large-scale point clouds.  Figure 2: Short-range point clouds (red, from KITTI [7]) v.s. long-range point clouds (blue, from Argoverse 2 [36]). The radius of the red circle is 75 meters. The sparsity quickly increases as the range extends.  In this paper, we propose Fully Sparse Detector (FSD), which takes the advantages of both sparse voxel encoder and point-based instance predictor. Since the central region might be empty, the detector has to predict boxes from other non-empty parts of instances. However, predicting the whole box from individual parts causes a large variance on the regression targets, making the results noisy and inconsistent. This motivates us to ﬁrst group the points into an instance, then we further extract the instance-level feature and predict a single bounding box from the instance feature. To implement this principle, FSD ﬁrst utilizes the sparse voxel encoder [37, 28, 5] to extract voxel features, then votes object centers based on these features as in VoteNet [22]. Then the Instance Point Grouping (IPG) module groups the voted centers into instances via Connected Components Labeling. After grouping, a point-based Sparse Instance Recognition (SIR) module extracts instance features and predicts the whole bounding boxes. As a point-based module, SIR has several desired properties. (1) Unlike previous point-based modules, SIR treats instances as groups, and does not apply the time-consuming neighborhood query for further grouping. (2) Similar to dynamic voxelization [45], SIR leverages dynamic broadcast/pooling for tensor manipulation to avoid point sampling or padding. (3) Since SIR covers the whole instance, it builds a sufﬁcient receptive ﬁeld regardless of the physical size of the instance. We list our contributions as follows.  We propose the concept of Fully Sparse Detector (FSD), which is the essential solution for efﬁcient long-range LiDAR detection. We further propose Sparse Instance Recognition (SIR) to overcome the issue of Center Feature Missing in sparse feature maps. Combining SIR with general sparse voxel encoders, we build an efﬁcient and effective FSD implementation.  FSD achieves state-of-the-art performance on the commonly used Waymo Open Dataset. Besides, we further apply our method to the recently released Argoverse 2 dataset to demonstrate the superiority of FSD in long-range detection. Given its challenging 200 meters perception range, FSD showcases state-of-the-art performance while being 2.4× faster than state-of-the-art dense detectors.  2  Figure 3: Overall architecture of FSD. For simplicity, we only use two instances to illustrate the pipeline. Red dots are the voted centers from each LiDAR point (blue dots). The SIR module and the SIR2 module all contain 3 SIR layers.  2 Related Work  Voxel-based dense detectors Pioneering work VoxelNet [44] uses dense convolution for voxel feature extraction. Although it achieves competitive performance, it is inefﬁcient to apply dense convolution to 3D voxel representation. PIXOR [38] and PointPillars [12] adopt 2D dense convolution in Bird’s Eye View (BEV) feature map achieving signiﬁcant efﬁciency improvement. We name such detectors as dense detectors since they convert the sparse point cloud into dense feature maps.  Voxel-based semi-dense detectors Different from the dense detectors, semi-dense detectors in- corporate both sparse features and dense features. SECOND [37] adopts sparse convolution to extract the sparse voxel features in 3D space, which then are converted to dense feature maps in BEV to enlarge the receptive ﬁeld and integrate with 2D detection head [18, 24, 43]. Based on SECOND-style semi-dense detectors, many methods attach a second stage for ﬁne-grained feature extraction and proposal reﬁnement [28, 27, 29, 3]. It is noteworthy that the semi-dense detector is hard to be trivially lifted to the fully sparse detector since it will face the issue of Center Feature Missing, as we discussed in Sec. 1.  Point-based sparse detectors The purely point-based detectors are born to be fully sparse. PointR- CNN [26] is the pioneering work to build the purely point-based detector. 3DSSD [39] accelerates the point-based method by removing the feature propagation layer and reﬁnement module. VoteNet [22] ﬁrst makes a center voting and then generates proposals from the voted center achieving better precision. Albeit many methods have tried to accelerate the point-based method, the time-consuming neighborhood query is still unaffordable in large-scale point clouds (more than 100k points per scene). So current benchmarks [30, 2] with large-scale point clouds are dominated by voxel-based dense/semi-dense detectors [10, 29, 14].  3 Methodology  3.1 Overall Architecture  Following the motivation of instances as groups, we have four steps to build the fully sparse detector (FSD): 1) We ﬁrst utilize a sparse voxel encoder [5, 28, 37] to extract voxel features and vote object centers(Sec. 3.2). 2) Instance Point Grouping groups foreground points into instances based on the voting results (Sec. 3.2). 3) Given the grouping results, Sparse Instance Recognition (SIR) module extracts instance/point features and generates proposals (Sec. 3.3). 4) The proposals are utilized to correct the point grouping and reﬁne the proposals via another SIR module (Sec. 3.4).  3.2  Instance Point Grouping  Classiﬁcation and Voting We ﬁrst extract voxel features from the point cloud with a sparse voxel encoder. Although FSD is not restricted to a certain sparse voxel encoder, we utilize sparse attention block in SST [5] due to its demonstrated effectiveness. Then we build point features by concatenating voxel features and the offsets from points to their corresponding voxel centers. These point features are passed into two heads for foreground classiﬁcation and center voting. The voting is similar to VoteNet [22], where the model predicts the offsets from foreground points to corresponding object  3  Figure 4: Illustration of building instance-level point operators with dynamic broadcast/pooling. Best viewed in color. Left: calculating center-to-neighbor offsets given raw point clouds. Right: updating point features. Note that the operation is parallel among all instances.  centers. L1 loss [24] and Focal Loss [17] are adopted as voting loss Lvote and semantic classiﬁcation loss Lsem.  Connected Components Labeling (CCL) To group points into instances, we regard all the pre- dicted centers (red dots in Fig. 3) as vertices in a graph. Two vertices are connected if their distance is smaller than a certain threshold. Then a connected component in this graph can be viewed as an instance, and all points voted to this connected component share a group ID. Unlike the ball query in VoteNet, our CCL-based grouping greatly avoids fragmented instances. Although there are many elaborately designed instance grouping methods [11, 33, 9], we opt for the simple CCL because it is adequate in our design and can be implemented by the efﬁcient depth-ﬁrst search.  3.3 Sparse Instance Recognition  3.3.1 Preliminaries: Dynamic Broadcast/Pooling  Given N points belong to M groups, we deﬁne their corresponding group ID array as I in shape of [N, ] and their feature array as F in shape of [N,C], where C is the feature dimensions. F (i) is the feature array of points belonging to the i-th group. Dynamic pooling aggregates each F (i) into one group feature gi of shape [C, ]. Thus we have gi = p(F (i)), where p is a symmetrical pooling function. The dynamic pooling on all group features G of shape [M,C] is formulated as G = p(F, I). The dynamic broadcast can be viewed as the inverse operation to dynamic pooling, which broadcasts gi to all the points in the i-th group. Since the broadcasting is essentially an indexing operation, we use the indexing notation [ ] to denote it as G[I], which is in shape of [N,C]. Dynamic broadcast/pooling is very efﬁcient because it can be implemented with high parallelism on modern devices and well ﬁts the sparse data with dynamic size.  The prerequisite of dynamic broadcast/pooling is that each point uniquely belongs to a group. In other words, groups should not overlap with each other. Thanks to the motivation of instances as groups, the groups in 3D space do not overlap with each other naturally.  3.3.2 Formulation of Sparse Instance Recognition  After grouping points into instances in Sec. 3.2, we can directly extract instance features by some basic point-based networks like PointNet, DGCNN, etc. There are three elements to deﬁne a basic point-based module: group center, pair-wise feature and group feature aggregation.  Group center The group center is the representative point of a group. For example, in the ball query, it is the local origin of the sphere. In SIR, the group center is deﬁned as the centroid of all voted centers in a group.  Pair-wise feature deﬁnes the input for per point feature extraction. SIR adopts two kinds of features: 1) relative coordinate between group center and each point, 2) feature concatenation between group and each neighbor point. Taking feature concatenation as example and using the notations in 3.3.1, the pair-wise feature can be denoted as CAT(F, G[I]), where CAT is channel concatenation.  Group feature aggregation In a group, a pooling function is used to aggregate neighbor features. SIR applies dynamic pooling to aggregate feature array F . Following the notations in 3.3.1, we have G = p(F, I), where G is the aggregated group features.  4  Integration Combining the three basic elements, we could build many variants of point-based operators, such as PointNet [21], DGCNN [34], Meta-Kernel [4], etc. Fig. 4 illustrates the basic idea of how to build an instance-level point operator with dynamic broadcast/pooling. In our design, we adopt the formulation of VFE [44] as the basic structure of SIR layers, which is basically a two-layer PointNet. In the l-th layer of SIR module, given the input point-wise feature array Fl, point coordinates array X, the voted center X (cid:48) and group ID array I, the output of l-th layer can be formulated as:  l = LinNormAct (CAT (Fl, X − pavg(X (cid:48), I)[I])) , F (cid:48)  Fl+1 = LinNormAct (CAT (F (cid:48)  l , pmax(F (cid:48)  l , I)[I])) ,  (1)  (2)  where LinNormAct is a fully-connected layer followed by a normalization layer [32] and an activation function [8]. The pavg and the pmax are average-pooling and max-pooling function, respectively. The output Fl+1 can be further used as the input of the next SIR layer, so our SIR module is a stack of a couple of basic SIR layers.  3.3.3 Sparse Prediction  With the formulation in Eqn. 1 and Eqn. 2, SIR extracts features of all instances dynamically in parallel. And then SIR makes sparse prediction for all groups. In contrast to two-stage sparse prediction, our proposals (i.e., groups) do not overlap with each other. Unlike one-stage dense prediction, we only generate a single prediction for a group. Sparse prediction avoids the difﬁculty of label assignment in dense prediction when the center feature is missing, because there is no need to attach anchors or anchor points to non-empty voxels. It is noteworthy that the fully sparse architecture may face a severe imbalance problem: short-range objects contain much more points than long-range objects. Some methods [1, 4] use hand-crafted normalization factors to mitigate the imbalance. Instead, SIR avoids the imbalance because it only generates a single prediction for a group regardless of the number of points in the group. Speciﬁcally, for each SIR layer, there is a Gl = pmax(F (cid:48) l , I) in Eqn. 2, which can be viewed as the group features. We concatenate all Gl from each SIR layer in channel dimension and use the concatenated group features to predict bounding boxes and class labels via MLPs. All the groups whose centers fall into ground-truth boxes are positive samples. For positive samples, the regression branch predicts the offsets from group centers to ground-truth centers and object sizes and orientations. L1 loss [24] and Focal Loss [17] are adopted as regression loss Lreg and classiﬁcation loss Lcls, respectively.  3.4 Group Correction  There is inevitable incorrect grouping in the Instance Point Grouping module. For example, some foreground points may be missed, or some groups may be contaminated by background clutter. So we leverage the bounding box proposals from SIR to correct the grouping. The points inside a proposal belong to a corrected group regardless of their previous group IDs. After correction, we apply an additional SIR to these new groups. To distinguish it from the ﬁrst SIR module, we denote the additional SIR module as SIR2.  SIR2 predicts box residual from the proposal to its corresponding ground-truth box, following many two-stage detectors. To make SIR2 aware of the size and location of a proposal, we adopt the offsets from inside points to proposal boundaries as extra point features following [15]. The regression loss is denoted as Lres = L1(∆res, (cid:91)∆res), where ∆res is the ground-truth residual and (cid:91)∆res is the predicted residual. Following previous methods [28, 27], the 3D Intersection over Union (IoU) between the proposal and ground-truth serves as the soft classiﬁcation label in SIR2. Speciﬁcally, the soft label q is deﬁned as q = min(1, max(0, 2IoU − 0.5)), where IoU is the IoU between proposals and corresponding ground-truth. Then cross entropy loss is adopted to train the classiﬁcation branch, denoted as Liou. Taking all the loss functions in grouping (Sec. 3.2) and sparse prediction into account, we have  Ltotal = Lsem + Lvote + Lreg + Lcls + Lres + Liou,  (3)  where we omit the normalization factors for simplicity.  5  3.5 Discussion  The center voting in FSD is inspired by VoteNet [22], while FSD has two essential differences from VoteNet.  After voting, VoteNet simply aggregates features around the voted centers without further feature extraction. Instead, FSD builds a highly efﬁcient SIR module taking advantage of dynamic broadcast/pooling for further instance-level feature extraction. Thus, FSD extracts more powerful instance features, which is experimentally demonstrated in Sec. 4.6.  VoteNet is a typical point-based method. As we discussed in Sec. 1, it aggressively down- samples the whole scene to a ﬁxed number of points for efﬁciency, causing inevitable information loss. Instead, the dynamic characteristic and efﬁciency of SIR enable ﬁne- grained point feature extraction from any number of input points without any downsampling. In Sec. 4.6, we showcase the efﬁciency of our design in processing large-scale point clouds and the beneﬁts from ﬁne-grained point representation.  4 Experiments  4.1 Setup  Dataset: Waymo Open Dataset (WOD) We conduct our main experiments on WOD [30]. WOD is currently the largest and most trustworthy benchmark for LiDAR-based 3D object detection. WOD contains 1150 sequences (more than 200K frames), 798 for training, 202 for validation, and 150 for test. The detection range in WOD is 75 meters (cover area of 150m × 150m). Dataset: Argoverse 2 (AV2) We further conduct long-range experiments on the recently released Argoverse 2 dataset [36] to demonstrate the superiority of FSD in long-range detection. AV2 has a similar scale to WOD, and it contains 1000 sequences in total, 700 for training, 150 for validation, and 150 for test. In addition to average precision (AP), AV2 adopts a composite score as evaluation metric, which takes both AP and localization errors into account. The perception range in AV2 is 200 meters (cover area of 400m × 400m), which is much larger than WOD. Such a large perception range leads to a huge memory footprint for dense detectors. Model Variants To demonstrate the generality of SIR, we build two FSD variants. FSDsst adopts the emerging single stride sparse transformer [5] as sparse voxel feature extractor. FSDspconv is built upon sparse convolution based U-Net in PartA2 [28]. Unless otherwise speciﬁed, we use FSDsst in the experiments. Implementation Details We use 4 sparse regional attention blocks [5] in SST as our voxel feature extractor. Both the SIR module and SIR2 module consist of 3 SIR layers. A SIR layer is deﬁned by Eqn. 1 and Eqn. 2. Our SST-based model converges much faster than SST, so we train our models for 6 epochs instead of the 2× schedule (24 epochs) in SST. All other hyper-parameters and training schemes are the same with SST. For FSDspconv, in addition to the 6-epoch schedule, we adopt a longer schedule (12 epochs) for better performance. In the longer schedule, we decrease the number of pasted instances in the CopyPaste augmentation, to prevent FSDspconv from overﬁtting.  4.2 Comparison to State-of-the-art Methods  We ﬁrst compare FSD with state-of-the-art detectors and our baseline in Table 1. FSD achieves the state-of-the-art performance among all the mainstream detectors. Thanks to the ﬁne-grained feature extraction in SIR, FSD also obtains exciting performance on Pedestrian class and Cyclist class with single-frame point clouds.  4.3 Study of Treatments to Center Feature Missing  In what follows, we conduct experiments on WOD to elaborate the issue of Center Feature Missing (CFM). We ﬁrst build several models with different characteristics. Note that all the following models adopt the same voxelization resolution, so they face the same degree of CFM at the beginning.  FSDplain: After the sparse voxel encoder, FSDplain directly predicts the box from each voxel. The voxels inside ground-truth boxes are assigned positive. Although FSDplain uses the most  6  Table 1: Performances on the Waymo Open Dataset validation split. All models only take single- frome point cloud as input without any test-time augmentations or model ensemble. All classes are trained in a single model in FSD. †: Adopting longer schedule (12 epochs) and decreasing the number of pasted instances in the data augmentation to prevent the model from overﬁtting Cyclist class.  Methods  SECOND [37] MVF [45] AFDet [6] Pillar-OD [35] RangeDet [4] PointPillars [12] Voxel RCNN [3] RCD [1] VoTr-TSD [20] LiDAR-RCNN [15] Pyramid RCNN [19] Voxel-to-Point [13] 3D-MAN [40] Part-A2-Net [28] CenterPoint-Pillar [41] CenterPoint-Voxel [41] IA-SSD [42] PV-RCNN [27] RSN [31] SST_TS [5] SST [5] AFDetV2 [10] PillarNet-34 [25] PV-RCNN++ [29] PV-RCNN++(center) [29]  FSDspconv (ours) FSDsst (ours) FSDspconv (ours) †  mAP/mAPH L2  Vehicle 3D AP/APH  Pedestrian 3D AP/APH  Cyclist 3D AP/APH  L1  L2  L1  L2  L1  L2  61.0/57.2 -/- -/- -/- 65.0/63.2 62.8/57.8 -/- -/- -/- 65.8/61.3 -/- -/- -/- 66.9/63.8 -/- 69.8/67.6 62.3/58.1 66.8/63.3 -/- -/- 67.8/64.6 71.0/68.8 71.0/68.5 68.4/64.9 71.7/69.5  70.7/68.4 71.7/69.4 72.7/70.5  72.3/71.7 62.9/- 63.7/- 69.8/- 72.9/72.3 72.1/71.5 75.6/- 69.0/68.5 74.9/74.3 76.0/75.5 76.3/75.7 77.2/- 74.5/74.0 77.1/76.5 76.1/75.5 76.6/76.0 70.5/69.7 77.5/76.9 75.1/74.6 76.2/75.8 74.2/73.8 77.6/77.1 79.1/78.6 78.8/78.2 79.3/78.8  76.7/76.3 77.3/76.9 79.5/79.0  63.9/63.3 -/- -/- -/- 64.0/63.6 63.6/63.1 66.6/- -/- 65.9/65.3 68.3/67.9 67.2/66.7 69.8/- 67.6/67.1 68.5/68.0 68.0/67.5 68.9/68.4 61.6/61.0 69.0/68.4 66.0/65.5 68.0/67.6 65.5/65.1 69.7/69.2 70.9/70.5 70.3/69.7 70.6/70.2  67.5/67.1 69.8/69.3 70.3/69.9  68.7/58.2 65.3/- -/- 72.5/- 75.9/71.9 70.6/56.7 -/- -/- -/- 71.2/58.7 -/- -/- 71.7/67.7 75.2/66.9 76.1/65.1 79.0/73.4 69.4/58.5 75.0/65.6 77.8/72.7 81.4/74.0 78.7/69.6 80.2/74.6 80.6/74.0 76.7/67.2 81.3/76.3  82.8/77.2 83.3/77.7 83.6/78.2  60.7/51.3 -/- -/- -/- 67.6/63.9 62.8/50.3 -/- -/- -/- 63.1/51.7 -/- -/- 62.6/59.0 66.2/58.6 68.1/57.9 71.0/65.8 60.3/50.7 66.0/57.6 68.3/63.7 72.8/65.9 70.0/61.7 72.2/67.0 72.3/66.2 68.5/59.7 73.2/68.0  73.7/68.6 74.4/69.3 74.4/69.4  60.6/59.3 -/- -/- -/- 65.7/64.4 64.4/62.3 -/- -/- -/- 68.6/66.9 -/- -/- -/- 68.6/67.4 -/- 72.1/71.0 67.7/65.3 67.8/66.4 -/- -/- 70.7/69.6 73.7/72.7 72.3/71.2 69.0/67.6 73.7/72.7  73.1/71.9 73.2/71.9 75.3/74.1  58.3/57.0 -/- -/- -/- 63.3/62.1 61.9/59.9 -/- -/- -/- 66.1/64.4 -/- -/- -/- 66.1/64.9 -/- 69.5/68.5 65.0/62.7 65.4/64.0 -/- -/- 68.0/66.9 71.0/70.1 69.7/68.7 66.5/65.2 71.2/70.2  70.8/69.5 70.8/69.6 73.3/72.1  straightforward solution for CFM, it suffers from the large variance of regression targets and low-quality predictions from hard voxels.  SSTcenter: It replaces the anchor-based head in SST with CenterHead [43, 41]. Based on sparse voxel encoder, SSTcenter converts sparse voxels into dense feature maps and applies several convolutions to diffuse features to the empty object centers as in Fig. 1. Then it makes predictions from the diffused center feature.  FSDnogc: It removes the group correction and SIR2 module in FSD. • CenterPoint-PP: It does not resort to any sparse voxel encoders. Instead, it applies multiple dense convolutions soon after voxelization for feature diffusion, greatly eliminating CFM. It also is equipped with CenterHead avoiding large variance of regression targets.  and  Table 2: Vehicle detection with vehicle length breakdown. †: re-implemented ourselves. ∗: ofﬁcial Waymo L2 overall metric. Arrows indicate the performance changes from SSTcenter.  Experiments analyses There is usually a quite large unoccupied area around the cen- ters of large vehicles. Thus the performance of large vehicles is an appropriate indicator that reveals the effect of CFM. So we build a customized evaluation tool1, which breaks down the object length following the COCO evaluation [16]. Then we use it to evaluate the performance of vehicles with different lengths. Table 2 shows the results, and we list our ﬁndings as follows.  CenterPoint-PP† FSDplain SSTcenter [5]  Vehicle length (m) [8, 12)  47.9 ↑ 17.4 53.7 ↑ 23.2  47.7 ↑ 14.0 51.3 ↑ 17.6  33.5 ↓ 2.5 36.7 ↑ 0.7  68.2 ↓ 1.2 71.0 ↑ 1.6  65.2 ↓ 1.1 69.3 ↑ 3.0  FSDnogc FSD  66.2 62.3 66.3  34.3 32.2 36.0  69.3 64.6 69.4  43.6 42.2 30.5  42.0 41.3 33.7  [12, +∞)  Ofﬁcial∗  Methods  [4, 8)  [0, 4)  1https://github.com/Abyssaledge/Extensible-Object-Detection-Evaluator  7  Figure 5: Memory footprints and inference latency in different perception ranges. We use FSDsst (Sec. 4.1) here. Statistics are obtained on a single 3090 GPU with batch size 1. Inference latency is evaluated by the standard benchmark script in MMDetection3D without any test-time optimization. CenterPoint-PP and SSTcenter are deﬁned in Sec. 4.3. Best viewed in color.  Comparing FSDplain with SSTcenter, they share the same attention-based sparse voxel encoder. However, the trend is totally opposite w.r.t vehicle size. With feature diffusion, SSTcenter attains much worse performance than FSDplain on large vehicles. It suggests feature diffusion is a sub- optimal solution for CFM in the case of large objects. For those large objects, the features may not be diffused to the centers or the diffused features are too weak to make accurate predictions.  However, FSDplain obtains the worst performance among all detectors on vehicles with normal sizes. Note that the CFM issue is minor for the normal size vehicles. So, in this case, the center- based assignment in SSTcenter shows its superiority to the assignment in FSDplain. It suggests the solution for CFM in FSDplain is also sub-optimal, even if it achieves better performance in large objects.  Comparing FSDnogc with SSTcenter, they share the same sparse voxel encoder while FSDnogc replaces the dense part in SSTcenter with SIR. The huge improvements of FSDnogc on large vehicles fairly reveal that SIR effectively resolves CFM and is better than feature diffusion.  CenterPoint-PP suffers much less from CFM because it leverages dense feature maps from very beginning of the network. It is also equipped with the advanced center-based assignment. Even so, FSDnogc and FSD still outperform CenterPoint-PP, especially on large vehicles.  Table 3: Performance in Argoverse 2 validation split. †: provided by authors of AV2 dataset. ∗: re-implemented by ourselves. C-Barrel: construction barrel. MPC-Sign: mobile pedestrian crossing sign. A-Bus: articulated bus. C-Cone: construction cone. V-Trailer: vehicular trailer. We omit the results of dog, wheelchair and message board trailer because these categories contain very few instances. The average results take all categories into account, including the omitted categories. We mark the categories attaining notable improvements in bold.  e g a r e v A  e l c i h e V  s u B  n a i r t s e d e P  n g i S p o t S  k c u r T x o B  d r a l l o B  l e r r a B C t s i l c y c r o t o M  n g i S - C P M  e l c y c r o t o M  e l c y c i B  s u B A s u B  l o o h c S  b a C k c u r T  e n o C C r e l i a r T - V  Methods  Precision  CenterPoint† [41] CenterPoint∗ FSD (Ours)  13.5 22.0 24.0  61.0 67.6 67.1  36.0 38.9 39.8  33.0 46.5 57.4  28.0 16.9 21.3  26.0 37.4 38.3  25.0 40.1 38.3  22.5 32.2 38.1  16.0 28.6 30.0  16.0 27.4 23.6  12.5 33.4 38.1  9.5 24.5 25.5  8.5 8.7 15.6  7.5 25.8 30.0  8.0 22.6 20.1  8.0 29.5 38.9  7.0 22.4 23.9  e l c i h e V e g r a L  3.0 3.9 5.1  n g i S  6.5 6.3 7.9  r e l l o r t S  2.0 0.5 5.7  t s i l c y c i B  14 20.1 27.0  Composite Score CenterPoint∗ FSD (Ours)  17.6 19.1  57.2 56.0  32.0 33.0  35.7 45.7  13.2 16.7  31.0 31.6  28.9 27.7  25.6 30.4  22.2 23.8  19.1 16.4  28.2 31.9  19.6 20.5  6.8 12.0  22.5 25.6  17.4 15.9  22.4 29.2  17.2 18.1  4.8 6.4  3.0 3.8  0.4 4.5  16.7 22.1  4.4 Long-range Detection  Several widely adopted 3D detection benchmarks [30, 7, 2] have relatively short perception range. To unleash the potential of FSD, we conduct long-range detection experiments on the recently released Argoverse 2 dataset (AV2), with a perception range of 200 meters. In addition, AV2 contains objects in 30 classes, facing the challenging long-tail issue.  8  Main results We ﬁrst list the main results of FSD on AV2 in Table 3. The authors of AV2 provide a baseline CenterPoint model, but the results are mediocre. To make a fair comparison, we re-implement a stronger CenterPoint model on the AV2 dataset. The re-implemented CenterPoint adopts the same training scheme with FSD, including ground-truth sampling to alleviate the long-tail issue. FSD outperforms CenterPoint in the average metric. It is noteworthy that FSD signiﬁcantly outperforms CenterPoint in some tiny objects (e.g., Pedestrian, Construction Cone) as well as some objects with extremely large sizes (e.g., Articulated Bus, School Bus). We owe this to the virtue of instance-level ﬁne-grained feature extraction in SIR.  Range Scaling To demonstrate the efﬁciency of FSD in long-range detection, we depict the trend of training memory and inference latency of three detectors when the perception range increases in Fig. 5. Fig. 5 shows dramatic latency/memory increase when applying dense detectors to larger perception ranges. Designed to be fully sparse, the resource needed for FSD is roughly linear to the number of input points, so its memory and latency only slightly increase as the perception range extends.  4.5 More Sparse Scenes  Table 4: Performance with different detection ar- eas. †: Region of Interest is deﬁned by the HD map in AV2 dataset.  Argoverse 2 dataset provides a highly reliable HD map, which could be utilized as a prior to remove uninterested regions making the scene more sparse. Thus we proceed with experiments removing some uninterested regions to show the advantages of FSD in more sparse scenarios. The results are summarized in Table 4. FSD has a signiﬁcantly lower memory footprint and latency with an acceptable precision loss after removing the uninterested regions. On the contrary, the efﬁciency improvement of CenterPoint is minor. It reveals that FSD beneﬁts more from the increase of data sparsity, which is another advantage of the fully sparse architecture.  3.2 ↓ 45.8% 81↓ 16.5% 2.3 ↓ 61.0% 74↓ 25.8%  9.9↓ 4.8% 227↓ 2.2% 9.7↓ 6.7% 217↓ 6.4%  FSD Latency(ms) mAP  all only RoI† w/o ground  Latency(ms) mAP  22.0 21.5 19.8  24.0 23.2 21.0  CenterPoint  Mem.  Mem.  10.4  232  5.9  97  4.6 Ablation Study  Table 5: Ablation of design factors in SIR. Perfor- mances are evaluated on Waymo validation split.  Effectiveness of Components In addition to FSDplain and FSDnogc (Sec. 4.3), we also de- grade FSD to FSDagg to understand the mecha- nism of FSD. In FSDagg, we aggregate grouped point features by dynamic pooling and then di- rectly make predictions from the pooled features, after Instance Point Grouping. FSDagg is simi- lar to the way in VoteNet [22] as we discussed in Sec. 3.5. Thus, FSDagg can explicitly leverage instance-level features other than the point-level features in FSDplain. However, FSDagg can not take advantage of further point feature extraction in SIR. As can be seen in Table 5, the improvement is limited if we only apply grouping without SIR. The combination of grouping and SIR attain notable improvements.  FSDplain FSDagg FSDnogc FSD  L2 3D APH Pedestrian  64.49 64.52 67.78 69.60  62.29 63.13 65.20 69.30  64.31 65.13 67.39 69.30  Group Correction  Grouping  Vehicle  Cyclist  (cid:88) (cid:88) (cid:88)  (cid:88) (cid:88)  SIR  (cid:88)  Table 6: Performances with different representa- tion granularity. †: Latency of SIR module.  Downsampling in SIR The efﬁciency of SIR makes it feasible to extract ﬁne-grained point features without any point downsampling. This is another notable difference between FSD and VoteNet. To demonstrate the superiority, we apply voxelization on the raw points before SIR module and treat the centroids of voxels as downsampled points. We conduct experiments on AV2 dataset because it contains a couple of categories in a tiny size, which may be sensitive to downsampling. As expected, small objects have notable performance loss when adopting downsampling, and we list some of them in Table 6. We also evaluate the inference latency of the SIR module on 3090Ti GPU. As can be seen, compared with the overall latency (97ms, Fig. 5), the SIR module is highly efﬁcient.  30cm 20cm 10cm Point  18.3 20.0 21.3 21.5  36.5 37.3 38.3 38.6  35.4 37.3 38.9 39.3  24.6 26.4 27.0 27.1  AP Bicyclist  Latency (ms)†  3.5 4.1 4.5 6.3  Voxel size  Stop Sign  Bollard  CC  9  5 Conclusion  This paper proposes FSD, a fully sparse 3D object detector, aiming for efﬁcient long-range object detection. FSD utilizes a highly efﬁcient point-based Sparse Instance Recognition module to solve the center feature missing in fully sparse architecture. FSD achieves not only competitive performance on the widely-used Waymo Open Dataset, but also state-of-the-art performance in the long-range Argoverse 2 dataset with a much faster inference speed than previous detectors.  Limitation A more elaborately designed grouping strategy may help with performance improve- ments. However, it is beyond our design goal in this paper, and we will pursue it in future work.  References  [1] Alex Bewley, Pei Sun, Thomas Mensink, Dragomir Anguelov, and Cristian Sminchisescu. Range Condi- tioned Dilated Convolutions for Scale Invariant 3D Object Detection. arXiv preprint arXiv:2005.09927, 2020.  [2] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In CVPR, 2020.  [3] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and Houqiang Li. Voxel R-CNN:  Towards High Performance Voxel-based 3D Object Detection. In AAAI, 2021.  [4] Lue Fan, Xuan Xiong, Feng Wang, Naiyan Wang, and ZhaoXiang Zhang. RangeDet: In Defense of Range  View for LiDAR-Based 3D Object Detection. In ICCV, 2021.  [5] Lue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyan Wang, and Zhaoxiang Zhang. Embracing Single Stride 3D Object Detector with Sparse Transformer. In CVPR, 2022.  [6] Runzhou Ge, Zhuangzhuang Ding, Yihan Hu, Yu Wang, Sijia Chen, Li Huang, and Yuan Li. AFDet:  Anchor Free One Stage 3D Object Detection. arXiv preprint arXiv:2006.12671, 2020.  [7] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition. IEEE, 2012.  [8] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs).  arXiv preprint  arXiv:1606.08415, 2016.  [9] Fangzhou Hong, Hui Zhou, Xinge Zhu, Hongsheng Li, and Ziwei Liu. Lidar-based panoptic segmentation  via dynamic shifting network. In CVPR, 2021.  [10] Yihan Hu, Zhuangzhuang Ding, Runzhou Ge, Wenxin Shao, Li Huang, Kun Li, and Qiang Liu. AFDetV2: Rethinking the Necessity of the Second Stage for Object Detection from Point Clouds. arXiv preprint arXiv:2112.09205, 2021.  [11] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set  point grouping for 3d instance segmentation. In CVPR, 2020.  [12] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. PointPillars:  Fast Encoders for Object Detection from Point Clouds. In CVPR, 2019.  [13] Jiale Li, Hang Dai, Ling Shao, and Yong Ding. From Voxel to Point: IoU-guided 3D Object Detection for  Point Cloud with Voxel-to-Point Decoder. In ACM-MM, 2021.  [14] Yingwei Li, Adams Wei Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng, Junyang Shen, Bo Wu, Yifeng Lu, Denny Zhou, et al. DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection. In CVPR, 2022.  [15] Zhichao Li, Feng Wang, and Naiyan Wang. LiDAR R-CNN: An Efﬁcient and Universal 3D Object  Detector. In CVPR, 2021.  [16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In European conference on computer vision. Springer, 2014.  [17] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal Loss for Dense Object  Detection. In ICCV, 2017.  10  [18] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and  Alexander C Berg. SSD: Single Shot Multibox Detector. In ECCV, 2016.  [19] Jiageng Mao, Minzhe Niu, Haoyue Bai, Xiaodan Liang, Hang Xu, and Chunjing Xu. Pyramid R-CNN:  Towards Better Performance and Adaptability for 3D Object Detection. In ICCV, 2021.  [20] Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang, Hang Xu, and Chunjing  Xu. Voxel Transformer for 3D Object Detection. In ICCV, 2021.  [21] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. PointNet: Deep Learning on Point Sets for  3D Classiﬁcation and Segmentation. In CVPR, 2017.  [22] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep Hough Voting for 3D Object Detection  in Point Clouds. In ICCV, 2019.  [23] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. PointNet++: Deep Hierarchical Feature  Learning on Point Sets in a Metric Space. In NeurIPS, 2017.  [24] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards Real-time Object  Detection with Region Proposal Networks. NeurIPS, 28, 2015.  [25] Guangsheng Shi, Ruifeng Li, and Chao Ma. PillarNet: High-Performance Pillar-based 3D Object Detection.  arXiv preprint arXiv:2205.07403, 2022.  [26] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. PointRCNN: 3D Object Proposal Generation and  Detection from Point Cloud. In CVPR, 2019.  [27] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li.  PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection. In CVPR, 2020.  [28] Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. From Points to Parts: 3D Object Detection from Point Cloud with Part-aware and Part-aggregation Network. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.  [29] Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi, Xiaogang Wang, and Hongsheng Li. PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection. arXiv preprint arXiv:2102.00463, 2021.  [30] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in Perception for Autonomous Driving: Waymo Open Dataset. In CVPR, 2020.  [31] Pei Sun, Weiyue Wang, Yuning Chai, Gamaleldin Elsayed, Alex Bewley, Xiao Zhang, Cristian Smin- chisescu, and Dragomir Anguelov. RSN: Range Sparse Net for Efﬁcient, Accurate LiDAR 3D Object Detection. In CVPR, 2021.  [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz  Kaiser, and Illia Polosukhin. Attention Is All You Need. In NeurIPS, 2017.  [33] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. SGPN: Similarity Group Proposal  Network for 3d Point Cloud Instance Segmentation. In CVPR, 2018.  [34] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. Acm Transactions On Graphics (tog), 38(5):1–12, 2019.  [35] Yue Wang, Alireza Fathi, Abhijit Kundu, David Ross, Caroline Pantofaru, Tom Funkhouser, and Justin  Solomon. Pillar-based Object Detection for Autonomous Driving. In ECCV, 2020.  [36] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes, Deva Ramanan, Peter Carr, and James Hays. Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting. In NeurIPS Datasets and Benchmarks 2021, 2021.  [37] Yan Yan, Yuxing Mao, and Bo Li. SECOND: Sparsely Embedded Convolutional Detection. Sensors, 18  (10), 2018.  [38] Bin Yang, Wenjie Luo, and Raquel Urtasun. PIXOR: Real-time 3D Object Detection from Point Clouds.  In CVPR, 2018.  11  [39] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3DSSD: Point-based 3D Single Stage Object Detector.  In CVPR, 2020.  [40] Zetong Yang, Yin Zhou, Zhifeng Chen, and Jiquan Ngiam. 3D-MAN: 3D Multi-Frame Attention Network  for Object Detection. In CVPR, 2021.  [41] Tianwei Yin, Xingyi Zhou, and Philipp Krähenbühl. Center-based 3D Object Detection and Tracking.  arXiv preprint arXiv:2006.11275, 2020.  [42] Yifan Zhang, Qingyong Hu, Guoquan Xu, Yanxin Ma, Jianwei Wan, and Yulan Guo. Not All Points Are  Equal: Learning Highly Efﬁcient Point-based Detectors for 3D LiDAR Point Clouds. In CVPR, 2022.  [43] Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. Objects as Points. arXiv preprint arXiv:1904.07850,  2019.  [44] Yin Zhou and Oncel Tuzel. VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection.  In CVPR, 2018.  [45] Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, Jiyang Gao, Tom Ouyang, James Guo, Jiquan Ngiam, and Vijay Vasudevan. End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds. In CoRL, 2020.  12\n",
      "2 2 0 2  n u J  0 1  ]  G L . s c [  1 v 6 6 2 5 0 . 6 0 2 2 : v i X r a  Does Self-supervised Learning Really Improve Reinforcement Learning from Pixels?  Xiang Li  Jinghuan Shang  Srijan Das Michael S. Ryoo  Department of Computer Science Stony Brook University Stony Brook, NY 11790 {xiangli8, jishang, srijan.das, mryoo}@cs.stonybrook.edu  Abstract  We investigate whether self-supervised learning (SSL) can improve online rein- forcement learning (RL) from pixels. We extend the contrastive reinforcement learning framework (e.g., CURL) that jointly optimizes SSL and RL losses and con- duct an extensive amount of experiments with various self-supervised losses. Our observations suggest that the existing SSL framework for RL fails to bring meaning- ful improvement over the baselines only taking advantage of image augmentation when the same amount of data and augmentation is used. We further perform an evolutionary search to ﬁnd the optimal combination of multiple self-supervised losses for RL, but ﬁnd that even such a loss combination fails to meaningfully outperform the methods that only utilize carefully designed image augmentations. Often, the use of self-supervised losses under the existing framework lowered RL performances. We evaluate the approach in multiple different environments in- cluding a real-world robot environment, and conﬁrm that no single self-supervised loss or image augmentation method can dominate all environments and that the current framework for joint optimization of SSL and RL is limited. Finally, we empirically investigate the pretraining framework for SSL + RL and the properties of representations learned with different approaches.  1  Introduction  Learning to act from image observations is crucial in many real-world applications. One popular approach is online reinforcement learning (RL), which requires no human demonstration or expert trajectories. Since all training samples are collected by the agent during policy learning in online RL, the collected data often has strong correlations and high variance, challenging the policy learning. Meanwhile, the cost of interacting with environments requires the RL algorithms to have higher sample efﬁciency. Compared to RL using state-based features, pixel-based RL continuously takes images as inputs, which usually come with a much higher dimensionality than numerical states. Such properties pose serious challenges to image representation learning in RL.  Several recent work studied such challenges from various directions, including: (1) Inspired by the great success of self-supervised learning (SSL) with images and videos (e.g., [5, 6, 8, 10, 14, 15, 17, 21, 31, 32, 37, 40, 52, 54, 55, 61, 71]), some RL methods [1, 42, 46, 59, 63, 69, 81, 88] take advantage of self-supervised learning. This is typically done by applying both self-supervised loss and reinforcement learning loss in one batch. In this paper, we dub such joint optimization of the self-supervised loss and the RL loss as the joint learning framework. (2) On the other hand, many papers [29, 43, 48, 58, 60, 76, 82, 83] investigate how online RL can take advantage of image augmentations. Among them, RAD [43] and DrQ [82, 83] show signiﬁcant improvements by applying relatively simple image augmentations to observations of RL agents.  Preprint. Under review.  Our objective is to study how well a single or combination of self-supervised losses and augmentations work under the current joint learning framework, and to empirically identify their impact in RL systems. In this paper, we extend such joint (SSL + RL) learning framework, conduct experiments comparing multiple self-supervised losses with augmentations, and empirically test them in many environments from different benchmarks. We conﬁrm that a single self-supervised loss under such joint learning framework typically fails to bring meaningful improvements to existing image augmentation-only methods. We also computationally search for a better combination of losses and image augmentations for RL with the joint learning framework. The experiments in different environments and tasks show inconsistency in self-supervised learning’s capability to improve reinforcement learning. Given a sufﬁcient amount of image augmentations, under the current framework, self-supervision failed to show beneﬁts over augmentation-only methods regardless how many self-supervised losses are used.  With all our ﬁndings, we present this work as a thorough reference for investigating better frameworks and losses for SSL + RL and inspiring future research. Our contributions can be summarized as follows:  1. We conduct an extensive comparison of various self-supervised losses under the existing joint learning framework for pixel-based reinforcement learning in many environments from different benchmarks, including one real-world environment.  2. We perform evolutionary search for the optimal combination of multiple self-supervised  losses and the magnitudes of image augmentation, and conﬁrm its limitations.  3. We empirically study a pretraining framework for combining SSL with RL as an alternative  of the joint learning framework.  2 Preliminaries  2.1 Reinforcement Learning  Assume an RL agent acts with a Markov Decision Process (MDP) that is deﬁned by state space S, action space A and state transition function P . For any time step t, the agent takes the current state st and outputs an action at. The environment updates to state s(cid:48) := st+1 and emits a reward rt+1 based on the state transition function P (st+1, rt+1|st, at). The goal of the agent is to maximize the expected reward R = (cid:80)∞ In pixel-based RL, the agent receives images as inputs. Compared to numerical states, images usually come with a much higher dimensionality. Sometimes, images can only partially describe the intrinsic system states and suffer from many typical vision challenges in real-world applications like occlusion, noise, shadow and reﬂection. Therefore, it is critical to perform effective representation learning on images for policy learning.  t=0 γtrt under a discount factor γ ∈ [0, 1), without any prior of P .  In this paper, we extend the conﬁgurations of previous work [42, 81] and exploit SAC (Soft Actor Critic) [25, 26] and Rainbow DQN [33] for the environments with continuous action space and discrete action space respectively.  Soft Actor Critic [25, 26] is an off-policy actor-critic algorithm that takes advantage of the maxi- mum entropy to encourage the agent to explore more states during the training. It maintains a policy network πψ and two critic networks Qφ1 and Qφ2. The goal of πψ is to maximize the expected sum of rewards and a γ-discounted entropy simultaneously, where the entropy encourages the agent to explore during learning.  Rainbow DQN [33] is a variant of DQN [51] with a bag of improvements such as double Q- learning [30, 72], prioritized sampling [62], noisy net [19], distributional RL [4], dueling net- works [78] and multi step reward.  2.2 Pairwise Learning  We coin the term “pairwise” learning for the frameworks that learn visual representations based on semantic invariance between dual-stream encoder representations. A general pairwise learning method  2  ﬁrst generates multiple augmented views by applying a series of random image augmentations to the input sample, then clusters views with the same semantics in the representation space. Optionally in such frameworks, methods using contrastive losses repel samples with different semantics. In this paper, we focus on four representative pairwise learning methods, MoCo [11, 12, 31], BYOL [22], SimSiam [10] and DINO [6]. We have a detailed explanation and comparison on these methods at Appendix A.1.  2.3 Representation Learning for Pixel-based RL  Previous works explore the possibility of learning better visual representation which may ﬁnally beneﬁt the policy learning. One direction is using image augmentation for policy learning [29, 43, 48, 58, 60, 76, 82, 83], where RAD [43] and DrQ [82, 83] achieve signiﬁcant performance using simple image augmentation. Another direction is to combine SSL with RL [1, 42, 46, 59, 63, 69, 81, 88], in which there are two representative methods, SAC+AE [81] and CURL [42].  RAD (Reinforcement Learning with Augmented Data) [43] investigates the impact of different types of image augmentations for both image and state inputs. By applying random translation or random crop to the input image, RAD signiﬁcantly improves data-efﬁciency solely through image augmentation without any auxiliary losses.  DrQ (Data-regularized Q) [82] further investigates the possibilities of utilizing image augmentation. DrQ applies image augmentation twice on the input images, and averages the Q value over two augmented images which is assigned as the Q value of the input images. DrQ v2 [83], which is the successor of DrQ, switches to DDPG (Deep Deterministic Policy Gradient) [47] as the backbone, brings scheduled exploration noise to control the levels of exploration at different learning stages, and introduces faster implementations of the image augmentation and the replay buffer. We only benchmark DrQ in this paper to ensure all the methods are using the same backbone for a fair comparison.  SAC+AE [81] takes advantage of a RAE (deterministic Regularized AutoEncoder) [20], in re- placement of β−VAE Higgins et al. [34] in SAC backbone to improve learning stability. The RAE is jointly trained with SAC by performing both SAC update and RAE update alternatingly in one batch.  CURL (Contrastive Unsupervised Representations for Reinforcement Learning) [42] combines contrastive learning with an online RL algorithm by introducing an additional contrastive learning head at the end of the image encoder. Similar to the aforementioned SAC+AE, here the contrastive loss and reinforcement learning loss are applied alternatively at training.  3 Self-supervision for Reinforcement Learning  To effectively evaluate different self-supervised losses, we extend the well-known joint learning framework widely used in previous papers [1, 42, 46, 63, 81] by adding a general self-supervised learning head to RL backbone. We keep the same backbone in CURL [42] using SAC [26] in tasks with continuous action space and using Rainbow DQN [33] in tasks with discrete action space.  3.1 General Joint Learning Framework  With SAC Fig. 1a shows a general joint learning framework with SAC backbone. The unmodiﬁed SAC backbone contains two image encoders, online encoder fq, target (or momentum) encoder fk and an actor head Ap. Each encoder is followed by two critic heads. Besides that, we attach an additional self-supervised head gq after the online encoder. For pairwise learning losses, we concatenate a momentum SSL head gk after the target encoder when needed.  For every sampled batch of transitions, we ﬁrst apply image augmentation to both the current state s and the next state s(cid:48), and update the SAC model (fq, Qi=1,2 , Ap) using the augmented images. Note that for stability concerns, we do not update the parameters of the image encoder when updating the actor head Ap. Then, the target networks are updated by Exponential Moving Average (EMA). This is followed by also performing an EMA update of the SSL head if required. And ﬁnally the online  q  3  (a) SAC backbone  (b) Rainbow backbone  Figure 1: General joint learning framework for SSL + RL. The red solid arrow represents the RL ﬂow; the blue one represents the SSL ﬂow and the black one means a shared ﬂow for both; sg stands for stop gradients.  encoder fq and the self-supervised head gq are updated by the self-supervised loss. By alternatingly performing RL and SSL in every batch, we jointly train all the components in the framework. The pseudo code of SAC update alternating RL and SSL is provided in Algorithm 1.  Algorithm 1 Update SAC with Self-supervised Losses Green: additional operations for SSL; Orange: only for BYOL and DINO.  procedure UPDATESACWITHSSL(s: current state, s(cid:48): next state, a: action, r: reward, d: done signal, step: model update step counter, fq: online encoder, fk: target/momentum encoder, Ap: actor head, Qi q: online critic head, Q(cid:48)i: target critic head, τ : target/momentum network update rate, gq: online SSL head, gk: momentum SSL head)  sa, s(cid:48) a ← IMAGEAUGMENTATION(s), IMAGEAUGMENTATION(s(cid:48)) fq, Qi=1,2 q fk, Q(cid:48)i=1,2 ← τ (fq, Qi=1,2 q gk ← τ gq + (1 − τ )gk fq, gq ← UPDATESSL(sa, s(cid:48)  , Ap ← UPDATESOFTACTORCRITIC(sa, s(cid:48) ) + (1 − τ )(fk, Q(cid:48)i=1,2)  a, a, r, d)  a, a, r)  (cid:46) EMA update of SAC (cid:46) EMA update of the momentum SSL head  end procedure  With Rainbow DQN Fig. 1b demonstrates how to jointly apply SSL to Rainbow DQN backbone. The unmodiﬁed Rainbow DQN backbone maintains an online encoder fq and a target encoder f (cid:48), followed by two state value heads Qq and Q(cid:48). We introduce an additional momentum encoder fk and self-supervised heads gq and gk as suggested in CURL [42]. For each batch, the self-supervised losses are computed using augmented images, while the RL loss is computed using the original data. Finally, the online encoder fq and the self-supervised head gq are updated by the self-supervised loss. The pseudo code of Rainbow DQN update can be found at Algorithm 2.  For both backbones, when evaluating, all the self-supervised heads are discarded and the agent takes actions based on the original images without any augmentation.  3.2 Losses for Self-supervised Learning  The self-supervised losses we investigated can be categorized into four classes: pairwise learning, transformation awareness, reconstruction, and reinforcement learning context prediction.  3.2.1 Pairwise Learning  We investigate three representative pairwise learning methods: BYOL [22], DINO [6] and Sim- Siam [10], along with existing CURL whose framework is similar to MoCo [31]. BYOL, DINO and  4  Algorithm 2 Update Rainbow with Self-supervised Losses Green: additional operations for SSL; Orange: only for BYOL and DINO.  procedure UPDATERAINBOWDQNWITHSSL(s: current state, s(cid:48): next state, a: action, r: reward, d: done, step: model update step counter, fq: online encoder, f (cid:48): target encoder, Qq: online value head, Q(cid:48): target value head, fk: momentum networks, τ : momentum network update rate, gq: online SSL head, gk: momentum SSL head, wSSL: weights of self-supervised losses)  a ← IMAGEAUGMENTATION(s), IMAGEAUGMENTATION(s(cid:48))  sa, s(cid:48) LSSL ← CALCULATESSLOSS(sa, s(cid:48) LRainbow ← CALCULATERAINBOWLOSS(s, s(cid:48), a, r, d) L ← LRainbow + wSSLLSSL fq, Qq, gq ← ONLINENETWORKSUPDATE(L) f (cid:48), Q(cid:48) ← fq, Qq fk, gk ← τ (fq, gq) + (1 − τ )(fk, gk)  a, a, r)  (cid:46) Copy parameters from online networks to target networks (cid:46) EMA update of momentum networks and SSL head  end procedure  (a) BYOL  (b) SimSiam  (c) DINO  Figure 2: Conceptual comparison of three pairwise learning frameworks  SimSiam only explicitly pull positive samples closer without the need of large number of negative samples. CURL uses a contrastive loss taking both positive and negative samples into consideration.  Given the general joint learning framework described in Sec. 3.1, by substituting the self-supervised head and loss, we can easily formulate different agents w.r.t. self-supervised losses. For BYOL, as shown in Fig. 2a, a projector and a predictor are appended to the online encoder sequentially. And an momentum projector is attached to the end of the target/momentum encoder. DINO (Fig. 2c) maintains only projector in both online and target branches. The momentum projector is also updated by EMA. The two encoders in BYOL and DINO operates on two augmented views of the data respectively whereas SimSiam (see Fig. 2b), uses only the online network and a projector for processing both the augmented views.  3.2.2 Transformation Awareness  Recent works (e.g., [13, 21, 36, 39, 45, 52]) have shown that the awareness of transformations (like rotation, Jigsaw puzzle and temporal ordering) improves many downstream tasks in computer vision like image classiﬁcation and action recognition. Typically such awareness can be acquired by explicitly asking a classiﬁer to identify the applied transformation from the pixel representation. Therefore, we investigate two simple classiﬁcation losses, rotation classiﬁcation (RotationCLS) and shufﬂe classiﬁcation (ShufﬂeCLS), and set a two-layer MLP classiﬁer as the self-supervised head in the joint learning framework.  RotationCLS represents the methods that encourage spatial transformation awareness. Inspired by RotNet [21] and E-SSL [13], we rotate the input image after augmentation by 0°, 90°, 180°and 270°. The classiﬁer predicts the rotation angle from the visual representation and it is trained by cross-entropy loss.  5  Figure 3: ShufﬂeCLS  Figure 4: General RL context prediction  Table 1: I/O of RL context prediction losses  Extract-A Extract-R Guess-A Guess-F  Predict-F  Predict-R Extract-AR Guess-AF  Predict-FR  Rep. of s(cid:48) Action a Reward r  Input Output -  Input - Output  Output Input  Output - Input  Output Input -  Input Output  Input Output Output  Output Output Input  Output Input Output  Shufﬂe Tuple [50] encourages the encoder to develop awareness of action causality by predicting if two frames appear in order. We adapt Shufﬂe Tuple by randomly shufﬂing the current state image and next state image in a state transition tuple and predicting whether it is shufﬂed or not. The classiﬁer also takes action into consideration because some of the transitions are reversible. The overall architecture of ShufﬂeCLS is shown as Fig. 3.  3.2.3 Reconstruction  Reconstructing the input image with an hourglass architecture has been shown to be an effective way to learn image representation [20, 34, 40]. We simply extend SAC+AE by changing the input and reconstruction target to be augmented images. The reconstruction loss and regularization from RAE [20] are left untouched.  Recent study on Masked AutoEncoder [32] (MAE) adapts the reconstruction task for patch-based Vision Transformers [18]. The objective in MAE includes reconstructing the entire image from input masked image patches. Inspired from this, we adapt SAC+AE into SAC+MAE by replacing the augmented input image with its masked version, and only penalizing the reconstruction error for the masked patches.  3.2.4 RL Context Prediction  Besides the self-supervised learning methods that are speciﬁcally designed for pixels, we investigate the losses using attributes naturally collected during RL process. For any state transition that is not the end of a trajectory, it contains four components: current state s, next state s(cid:48), action a and reward r, with trajectory termination signal omitted. Inspired by Shelhamer et al. [67], we concatenate the visual representation of the current state s and another representation h as the input. Without loss of generality, the second input representation h can be any of these three representations of s(cid:48), a and r. Then, we predict the remaining components using a two-layer MLP. For continuous outputs, mean-squared error (MSE) loss is applied, while for the discrete target (e.g., action in discrete action space), we use cross-entropy loss. The architecture of this group of self-supervised losses is shown in Fig. 4. From the combination of inputs and outputs, we deﬁne nine losses whose I/O speciﬁcations are provided in Table 1. For those losses whose outputs include two components, two target prediction networks share the same SSL head except the last task-speciﬁc layer.  3.3 Evolving Multiple Self-supervised Losses  Besides a single self-supervised loss or handcrafted combination of two losses, we further investi- gate how multiple self-supervised losses affect the policy learning together with the joint learning framework. In such a conﬁguration, the agent maintains multiple SSL heads at the same time and  6  we apply losses to their corresponding head individually. We formulate the combination of multiple losses as a weighted sum LCombo = (cid:80)Nl i=1 wi · Li where wi is the weight of a speciﬁc loss Li and Nl is the total number of losses in the search space. In the joint learning framework, we apply both self-supervised LCombo and RL losses jointly to the networks for every mini-batch. Considering that the policy learning is quite sensitive to hyper-parameters (see Appendix A.7), it is non-trivial to ﬁnd each weight for every SSL loss.  ELo (Evolving Losses) [57] shows promising results in unsupervised video representation learn- ing [55, 71], by using evolutionary search to automatically ﬁnd optimal combination of many self-supervised losses. In the spirit of ELo, we turn to evolutionary search for automatically ﬁnding the optimal solution. Assume an unknown objective function whose inputs are weights of multiple losses wi and the magnitudes of image augmentation mj=1,2 for the online encoder and momentum encoder. The function output is the score achieved by the trained agent in its environment with a cer- tain random seed: Rseed env (mj=1,2, wi=1,2,...Nl ). The optimization algorithm approaches the maximum value of the objective function by continuously testing the value of the objective function, which in our case is the training and evaluation of an agent with the given parameters. We choose an off-the-shelf optimization algorithm PSO (Particle Swarm Optimization) [38] for its simplicity, and perform the evolutionary search on one environment from DMControl [70] or Atari game benchmark [3]. For each set of inputs, we ﬁnd it critical to run with multiple random seeds and report IQM (interquartile mean)1 for a stable and robust search. The optimization process is presented as:  argmax mj=1,2,wi=1,...,Nl  IQM(Rseed=1,. . . ,5 env  (mj=1,2, wi=1,...,Nl ))  (1)  Note that we are also implicitly searching for the balance between the self-supervised loss and the RL loss by performing this search, as it has the capability to adjust absolute weights of the self-supervised losses overall. Each wi has a range of [0, 10] and each mj varies from [85, 116].  Searching in DMControl We maintain a population of 50 for DMControl and each particle evolves 15 generations in “cheetah, run”. Before the search, the ﬁrst ith particles are initialized with mj=1,2 = 88, and each particle only has one weight set to 1 and other weights set to 0. In another word, these ﬁrst ith particles start with the existing single self-supervised loss method in the search space. Other particles are randomly initialized. Table 2 shows the combination Elo-SAC found in cheetah run. The columns in Table 2 show the search space. The ﬁrst six columns denote the optimal weight wi of its corresponding loss obtained with the evolutionary search, while the last two columns denote the original image size before random crop (image augmentation magnitude mj=1,2).  Table 2: Parameters found in cheetah run for DMControl  Agent  Searched Env.  CURL w1  BYOL w2  Predict FR w3  Extract AR w4  AutoEncoder w5  RotationCLS w6  Online Aug. m1  Target Aug. m2  ELo-SAC Cheetah, fun  0  0.288  0.628  0  0  0.009  87  86  Searching in Atari For Atari, the population is 30 and we use a similar initialization like DM- Control. The search is performed on Forstbite only for 10 generations and the found combination is shown in Table 3.  Table 3: Parameters found in Frostbite for Atari  Agent  Searched Env.  BYOL w1  Predict Future w2  Extract Reward w3  AutoEncoder w4  Rotation CLS w5  ELo-Rainbow  Frostbite  0.250  1.054  2.280  0.953  0.591  Interestingly, we ﬁnd that the optimal combination on DMControl is relatively sparse, where BYOL and Predict FR are the only two major losses. However, on Atari, the magnitudes of all the weights are relatively similar. The difference between the found results reﬂects the different properties of  1Mean using only the data between the ﬁrst and third quartiles [79]  7  different environments. Our further experiments in DMControl conﬁrm the generalization ability of evolving losses; i.e., the obtained solution of weights in one environment achieves relatively good performance on other environments in the same benchmark. However, results on Atari are much inconsistent with DMControl. We cover detailed observations and discussions in Sec. 4.1.  4 Experiments  We conduct experiments in three directions, in order to better understand how we should integrate SSL with RL. First, we demonstrate how different self-supervised losses affect the RL process, by trying them on multiple challenging tasks. Then, we perform further empirical analysis on the visual representations learned with the joint learning framework. Finally, we benchmark a pretraining framework as an alternative of the joint learning framework.  Note our evaluation setting differs a bit from that in CURL [42] or SAC+AE [81]. In CURL [42], image augmentation was not used for the baseline while the CURL method beneﬁted from the augmentation. In SAC+AE [81], no augmentation was used either for the baselines or the approach. Unlike these prior work, in our experiments, we make all approaches fully beneﬁt from image augmentations unless otherwise speciﬁed. The idea is to compare different self-supervised losses with an assumption that a sufﬁcient amount of augmentation is provided.  4.1  Investigating the Joint Learning Framework  Evaluation Scheme Thorough evaluation of reinforcement learning algorithms is challenging due to the high variances between each run and the extensive requirement of computation. Consequently, we run all experiments with multiple different random seeds and report the interquartile mean and the standard deviation of the scores as suggested by Agarwal et al. [2]. For a quantitative comparison of the different methods mentioned in Section 3.2, in addition to the absolute scores, we assign a Relative Score to each method. We denote the interquartile mean of scores achieved by agent A in environment e ∈ E as IQMA,e and denote the collection of all interquartile mean scores achieved in environment e by different agents as IQMe. The Relative Score of agent A is computed as  SA  Relative =  IQMA,e − mean(IQMe) std(IQMe)  (cid:88)  e∈E  (2)  where mean(·) and std(·) computes the mean and the standard deviation.  DMControl Experiments DMControl (DeepMind Control suite) contains many challenging visual continuous control tasks, which are widely utilized by recent papers. We evaluate all the methods introduced in Sec. 3, along with two important baselines, SAC-NoAug and SAC-Aug(100), in six environments of DMControl that are commonly used in previous papers [42, 43, 81, 82]. Other methods that only take advantage of image augmentation, like RAD [43] and DrQ [82] are also benchmarked for comparison.  The difference between these methods, other than their self-supervised losses speciﬁed, can be summarized as follows: SAC-NoAug is the original pixel-based SAC [25, 26]. SAC-Aug(88) and SAC-Aug(100) use random crop as the only image augmentation, where (88) means the original image has a size of 88 × 88 before randomly cropping to 84 × 84 and (100) means the original image has a size of 100 × 100. These two methods should be regarded as variants of RAD with different augmentation choices. The Random crop from 100×100 to 84×84 is the default image augmentation method for all the methods introduced in Sec. 3.2. Essentially, if we remove their self-supervised loss, they will fall back to SAC-Aug(100). Similarly, we test DrQ variants by replacing its default random shift augmentation with random crop, reported as DrQ(88) and DrQ(100). Meanwhile, RAD uses random translate by default except on walker walk; ELo-SAC renders images at 100 × 100 and ﬁrst perform center crop with sizes of 87 × 87 and 86 × 86. Then two central patches are randomly cropped to 84 × 84 as the inputs for the online networks and the target networks respectively. For the policy learning part, all the methods share the same model. However, DrQ, DrQ(88) and DrQ(100) apply an additional tanh activation after the convolutional layers in the encoder. Therefore, we also report DrQ-w/o-Tanh removing tanh activation from DrQ for reference.  Notice that all tested methods except SAC-NoAug fully take advantage of image augmentation. That is, all the methods with self-supervised losses beneﬁt from the augmentation.  8  Figure 5: Relative Scores on six DMControl tasks, environment step=100k, batch size=512, Number of seeds=10. SAC-NoAug uses no image augmentation, while all the other methods beneﬁt from image augmentation; The methods in blue (like DrQ) only take advantage of image augmentation without any SSL; the methods in black (like CURL) apply one self-supervised loss; the methods in orange (like Extract-AR) manually combines two self-supervised losses; ELo-SAC combines multiple self-supervised losses with speciﬁc weights from an evolutionary search. From this ﬁgure, No existing SSL-based method with the joint learning framework achieves better performance than DrQ and RAD which only use well-designed image augmentation. ELo-SAC achieves higher Relative Score than all the self-supervised methods, but it still performs worse than DrQ and RAD.  We mainly follow the hyper-parameters reported in CURL, except that we use the same learning rate 10−3 in all environments for simplicity. All the methods are benchmarked at 100k environment steps, with training batch size 512 under 10 random seeds. The relative score of each tested algorithm on DMControl is reported as Fig. 5. We also strongly encourage readers to check full results at Table 4 for a full picture.  Table 4: Interquartile mean and standard diviation on Six DMControl Tasks. The last column is colored based on the relative performance w.r.t. SAC-Aug(100) ﬁnger,spin  reacher,easy walker,walk  ball_in_cup,catch  cartpole,swingup  RelativeScore  cheetah,run  Agent  L S S o N  d e s i v r e p u s - f l e S  SAC-NoAug SAC-Aug(88) SAC-Aug(100) RAD  DrQ DrQ-w/o-Tanh DrQ(88) DrQ(100)  CURL BYOL DINO SimSiam  ShufﬂeCLS RotationCLS  SAC+AE MAE  Extract-Action Extract-Reward Guess-Action Guess-Future Predict-Future Predict-Reward  Extract-AR Guess-AF Predict-FR  71.4±139.9 510.8±187.4 541.4±306.2 879.9±82.0  914.9±21.2 869.8±168.6 762.5±139.4 907.6±102.9  730.0±179.4 667.7±281.2 916.9±65.7 82.6±86.7  112.2±101.9 157.9±212.1  616.1±169.9 251.1±231.1  871.0±298.6 598.2±306.2 724.6±265.3 82.4±87.1 121.5±186.9 672.8±260.3  822.2±240.5 329.8±298.4 750.3±256.0  224.8±28.6 714.2±113.9 563.4±235.0 786.4±95.1  663.6±217.5 826.4±44.9 508.2±161.2 675.5±131.1  471.5±89.9 507.2±221.7 686.0±152.2 67.4±68.6  28.8±28.4 336.4±220.1  388.8±130.1 372.8±76.1  493.9±162.7 469.8±218.7 495.7±121.4 146.6±178.0 252.7±219.9 517.8±215.6  592.9±124.7 140.7±144.0 723.2±167.5  120.9±25.7 354.5±68.7 172.1±64.0 387.9±81.3  345.9±70.0 393.7±74.0 331.7±80.5 318.8±54.2  215.1±57.3 70.7±44.3 198.3±79.3 0.7±0.3  238.9±172.6 771.2±175.0 724.6±154.9 920.0±103.6  935.6±201.3 849.6±140.9 877.6±93.2 940.0±127.2  717.8±136.5 547.3±185.6 923.1±124.4 7.6±179.4  204.8±131.8 347.9±148.5 654.4±222.1 508.8±111.5  692.7±163.1 635.0±155.0 395.5±161.0 627.0±233.0  569.8±179.4 403.7±183.7 686.2±198.2 72.3±71.1  99.6±38.7 192.2±165.0 422.1±250.8 522.1±95.5  523.9±182.2 525.0±163.7 119.2±160.5 302.9±295.8  442.6±87.1 449.0±153.5 414.6±162.4 34.1±24.0  0.9±0.4 209.7±44.7  53.0±162.8 801.9±139.7  108.3±55.4 540.3±163.7  127.3±98.9 537.0±170.3  291.8±59.8 282.0±62.3  799.0±138.9 669.5±112.8  481.3±130.4 336.9±170.1  402.6±161.5 489.7±49.4  172.3±65.5 302.1±89.9 204.6±26.2 0.7±0.4 0.7±0.3 279.1±71.9  225.8±60.7 0.9±22.8 12.4±35.7  870.4±108.1 828.7±115.3 669.9±116.8 786.5±117.8 796.7±166.7 837.6±264.6  783.0±112.0 880.0±59.5 861.5±49.2  578.3±144.4 753.2±155.5 578.8±161.1 323.4±229.2 365.3±235.2 796.2±143.5  645.4±207.0 382.9±265.0 636.1±201.4  484.8±70.5 522.2±130.5 410.6±91.1 74.1±73.6 112.7±137.4 520.1±218.1  464.2±141.3 494.7±112.7 270.0±154.9  ELo-SAC  888.3±90.6  772.8±167.3  359.7±69.7  789.3±198.2  478.3±159.9  537.5±164.5  8.313 0.373 1.236 5.471  5.784 6.058 0.372 4.009  1.401 -1.224 4.092 -11.848  11.010 -0.349  0.861 -1.236  2.539 3.477 1.084 -6.831 -5.756 4.010  3.092 -2.942 1.010  4.639  From the ﬁrst glace at Fig. 5, no tested SSL-based method under the joint learning framework achieves better performance than DrQ and RAD which are carefully designed to take the best advantage of speciﬁc image augmentations. We also ﬁnd that compared to the baseline SAC-Aug(100), approaches with a single self-supervised loss frequently (11 out of 18) fail to improve reinforcement learning.  9  Some SSL methods (like SimSiam, ShufﬂeCLS) ruin the policy learning resulting in performance even worse than SAC-NoAug, which suggests that an improper use of self-supervised loss can damage the beneﬁts brought by image augmentation. Note that there are several self-supervised losses that outperform CURL in its joint learning framework, which shows some potential on studying novel self-supervised losses for RL. However, none of them outperformed DrQ or RAD only using image augmentations, as mentioned above.  Table 4 further shows that except in rare cases which the performance is constantly lower than others like SimSiam and ShufﬂeCLS, there is no strong evidence showing that any self-supervised loss is consistently better than others across all the environments. How different tasks “respond” to a certain self-supervised loss can be regarded as a property, which can help us understand the similarities of different tasks and beneﬁt the loss section.  Then, regarding combining losses, Guess-AF and Predict-FR, which are manually designed to combine two individual losses, are not better than the single self-supervised loss in their combinations (see Guess-Action and Predict-Reward). Considering that Extract-AR, Guess-AF and Predict-FR concatenate both the outputs and apply supervision by averaging loss per element of the output, the target with a higher dimension will naturally get more penalty due to the larger number of elements in the output. We further test ‘-Balanced’ conﬁguration, where we only modify how the supervision is applied, and this trick bring overall improvements (See Appendix A.6).  ELo-SAC searches for a combination of multiple self-supervised losses. This search is done in a single task/environment, with the assumption that it will also generalize to other tasks and environments in DMControl and obtain better overall performances. In the ‘cheetah run’ where the search was performed, it obtained the best result among the approaches using SSL. Further, ELo-SAC achieves the best score in ‘walker, walk’ than any other methods although only searched in ‘cheetah run’. This demonstrates the feasibility of ELo-SAC and implies that the obtained combination through evolutionary search has a potential to generalize to other environments in DMControl. However, weaker performance in ‘ﬁnger, spin’ and ‘reacher, easy’ made ELo-SAC relatively worse than DrQ (which does not use any self-supervision) on average.  Since DrQ outperforms the other methods without SSL, we further investigate the reason behind this by comparing variants of DrQ with different augmentation designs. We conﬁrm that the success of DrQ comes not only from the averaging mechanism taking advantage of image augmentation, but also from the speciﬁc well-designed image augmentation method. Another example illustrating the important of augmentation details is the different performance of RAD and SAC-Aug(100), given the only difference between these two methods is the augmentation. We also conﬁrm the importance of image augmentation magnitude from the comparison of DrQ(88) and DrQ(100), as well as from the comparison of SAC-Aug(88) and SAC-Aug(100). Further ablations can be found at Sec. A.7. Overall, our observation suggests that it is critical to engineer image augmentation carefully when designing a RL system with or without SSL. Meanwhile, the better image augmentation for SSL + RL may need further investigations.  Atari Game Experiments Atari 2600 Games are also challenging benchmarks but with discrete action space. We choose seven games in this benchmark for selected methods. All the methods use Efﬁcient Rainbow [73] as the backbone, which is a Rainbow [33] variant with modiﬁcations for better data-efﬁciency. Note that Efﬁcient Rainbow, as a baseline, does not take advantage of image augmentation. Therefore, we also benchmark Rainbow-Aug which is essentially Efﬁcient Rainbow taking the augmented images for policy learning instead. We use the same image augmentation and hyper-parameters reported by CURL for all applicable methods. For a fair comparison, the augmentation for DrQ* is also adopted from CURL, which is different from what the original DrQ paper suggested. We denote our setting as DrQ* to distinguish it from the original DrQ. For each game, we run 20 random seeds and benchmark the agent at 400K environment steps (100K model steps with frame skip of 4). We report interquartile mean, standard deviation and relative score same as DMControl (See Table 5).  Figure 6 shows a summary among the seven different tasks in Relative Score. Firstly, compared to vanilla baseline Efﬁcient Rainbow which does not have any image augmentation or self-supervised learning, Rainbow-Aug performs worse overall with additional image augmentation for RL. This suggests that the image augmentation used for self-supervised learning in CURL does not easily transfer. Similarly, DrQ* achieves compromised performance than Efﬁcient Rainbow, showing  10  Figure 6: Relative Scores on seven Atari games, environment step=400k, batch size=32, Number of seeds=20. The color of a method reﬂects its category same as Fig. 5. The overall results show that image augmentation for RL does not beneﬁt policy learning on Atari which is quite different from DMControl. Most of the self-supervised losses fail to bring improvements even given more computation and extra model capacity from the SSL head. Only Rainbow+AE signiﬁcantly outper- forms Efﬁcient Rainbow, which is inconsistent with SAC+AE in DMC. ELo-Rainbow achieves worse results even than some of the SSL-based methods in the search space like BYOL and Rainbow+AE. The high variance and the image domain gap between different games makes it extremely challenging for ELo-Rainbow to ﬁnd the combined loss that generalizes to all environments.  Table 5: Scores on Atari, the last column is colored based on the relative performance w.r.t. Efﬁcient Rainbow, “*” means using a different image augmentation method from the original paper demon_attack  RelativeScore  battle_zone  jamesbond  kangaroo  frostbite  assault  Agent  pong  L S S o N  d e s i v r e p u s - f l e S  Eff.-Rainbow Rainbow-Aug  506.8±59.3 459.7±79.6  14840.0±6681.7 4770.0±4379.0  519.3±193.1 870.3±345.9  873.1±834.8 1469.7±962.2  318.5±92.7 317.0±110.5  853.0±1304.8 619.0±298.0  19.0±2.4 -20.3±0.5  DrQ*  503.7±89.0  7600.0±6839.0  891.2±322.3  943.7±913.2  321.0±91.6  605.0±462.0  19.9±0.8  CURL 511.6±107.3 514.6±93.4 BYOL  5100.0±5530.2 9470.0±4879.6  615.3±240.4 418.4±246.5  928.3±1018.5 2111.5±982.6  307.0±219.8 291.5±90.9  620.0±300.8 740.0±1573.6  18.1±2.3 -18.5±2.9  RotationCLS  427.1±62.2  12950.0±5742.7  401.0±159.0  1591.9±949.5  285.5±70.2  892.0±1674.2  19.3±1.3  Rainbow+AE  485.2±74.7  14290.0±5927.7  528.8±158.6  1272.5±964.3  320.5±68.8  1155.0±1392.5  18.8±2.3  Extract-Action Extract-Reward Predict-Future Predict-Reward Predict-FR Predict-FR-Balanced  443.6±72.8 494.8±63.7 509.5±67.7 485.6±100.8 476.3±86.4 485.7±82.2  7370.0±3797.5 14420.0±4901.0 10420.0±5252.4 11870.0±4197.2 14060.0±5515.9 14270.0±4421.5  521.0±126.6 533.4±224.1 452.1±145.6 547.9±291.6 475.0±167.5 495.8±209.7  1627.4±874.5 1286.6±1109.0 1144.5±988.7 1155.9±946.9 931.3±837.5 1359.1±1029.2  282.0±56.1 294.5±83.7 295.0±70.7 304.0±92.7 329.5±91.6 293.5±146.8  855.0±612.3 804.0±1001.0 733.0±966.0 908.0±1718.9 986.0±1874.9 664.0±1239.6  18.7±2.5 -18.4±2.1 -19.4±2.1 -19.4±1.7 -18.9±1.7 -18.9±1.3  ELo-Rainbow  493.1±67.4  11750.0±4727.9  623.4±249.9  1027.6±863.8  297.5±66.4  795.0±593.3  19.2±2.3  1.806 -2.539  0.540  0.605 1.832  2.827  4.486  2.322 1.625 -1.914 -0.083 2.171 -0.596  0.493  that using image augmentation for Rainbow on Atari does not beneﬁt policy learning unlike SAC on DMControl. Based on the inconsistent impacts of image augmentation, further investigation is required when applying image augmentation to RL on Atari.  As for the self-supervised losses, BYOL, Rainbow+AE, Extract-Reward, Predict-Reward, Predict-FR gain better performance than CURL. However, only Rainbow+AE shows signiﬁcant improvement on Efﬁcient Rainbow and outperforms all the other tested methods, which interestingly is inconsistent with SAC+AE in DMControl. Predict-FR-Balanced, which shows considerable improvements on DMControl by manually adjusting weights of two self-supervised losses, fails to surpass Predict-FR on Atari. ELo-Rainbow, which searched in Frostbite, improves the baseline only in demon_attack and frostbite. The high visual/action variance in this benchmark made the evolutionary search extremely difﬁcult. Further, there is huge image domain gaps between games, which makes it even harder for ELo-Rainbow to work across multiple games on Atari.  Real Robot Experiments To further evaluate methods in the real world applications, we set up a continuous robot arm control environment, uArm reacher. With the help of some simple techniques in computer vision and robotics, our environment can autonomously randomly reset and keep the agent training without any human input.  The environment requires a robotic arm with a suction cup actuator, two ﬁxed RGB cameras, and a cube which can be picked up by the suction cup as the target, as shown in Fig. 7. The goal is to  11  Figure 7: Real robot environment setup  Figure 8: Scores on real robot uArm, reacher, envi- ronment step=200k, batch size=512. The agent fails to learn effective policy without image augmentation. SAC-Aug(100), using the augmented images for pol- icy learning without any SSL, outperforms the other methods.  move the actuator close to the target as fast as possible. The observation comes from two cameras with a native resolution of 640 × 480. The images are then resized to 100 × 100, stacked along channel axis, and ﬁnally randomly cropped into 84 × 84, resulting in an 84 × 84 × (3 + 3) image observation before fed to the network. The action space is a 3D vector ranging from -1 to 1, and it will be mapped to the actuator position movement in a 3D robot Cartesian coordinates whose original point is the center of the robot base. The robot motion range is manually limited for safety concerns while avoiding the actuator moving the target in one episode. Following reacher in DMControl, we deﬁne a very simple reward function. The reward function returns 1 when the 3D Euclidean distance between the actuator and the target is lower than a threshold, otherwise, it returns −1e − 3. The length of each episode is set to 200 steps, which limits the range of the episode accumulated reward to [−0.2, 200]. Please check Appendix A.4 for how we make this learning environment autonomous.  We benchmark SAC-Aug(100), DrQ(100) and CURL with ﬁve different random seeds and run SAC-NoAug for two random seeds. The ﬁrst three methods share the same image augmentation method, a random crop from 100 × 100 to 84 × 84 and results are shown as Fig. 8.  Surprisingly, in this real world environment, the agent fails to learn an effective policy without any image augmentation, given limited interaction steps. Quite different from our previous observations on DMControl, SAC-Aug signiﬁcantly outperform other methods. Also, opposite to DMControl, CURL shows a slight advantage over DrQ(100). What is consistent with the DMControl experiments is that the image augmentation alone (i.e., SAC-Aug) was sufﬁcient to outperform CURL using self-supervision. We ﬁnd that there is no golden self-supervised loss or image augmentation that can dominate every environment.  Summary From DMControl and the real-world experiments, we empirically show that compared to the image augmentation, the role of existing self-supervised losses with the joint learning framework is usually limited. While results on Atari show a different trend from DMControl, once again we conﬁrm that there is no golden self-supervised loss or image augmentation that can dominate every environment with the current joint learning framework. At the same time, it is usually challenging to conclude a consistent trend that one method is meaningfully better than others across multiple tasks. One should cautiously decide the design choice of image augmentation or self-supervised loss for a speciﬁc RL task.  4.2 Empirical Analysis on the Learned Representations  To further understand the role of self-supervised loss and image augmentation in an online rein- forcement learning system with the joint learning framework, we empirically show the properties of representations learned by different losses.  We ﬁrst follow Wang et al. [75] and measure the three metrics Dynamic Awareness, Diversity and Orthogonality, extending them from discrete action space to continuous action space.  Dynamics Awareness means two states that are adjacent in time should have similar representations, and states further apart should have a low similarity.  Diversity measures a ratio between state and state-value differences. High diversity means two states have two different representations to be distinguished even when they have similar state-values.  12  Orthogonality reﬂects the linearly independence of the representation, in another word, higher the orthogonality, lower the redundancy in the representations.  Assume an image observation xi is taken when the intrinsic system state is si. Denoting the visual representation of xi generated by the encoder from the critic networks as φi, and Critic(φi, ·) is the learned critic network output. Eq. 3 shows how to compute the three representation metrics.  Dynamic Awareness =  (cid:80)N i  (cid:13) (cid:13)φi − φj∼U (1,N )  (cid:13) (cid:13)2 − (cid:80)N i (cid:107)φi − φ(cid:48) (cid:13) (cid:13) (cid:13)φi − φj∼U (1,N ) (cid:13)2  i(cid:107)2  (cid:80)N i  Diversity = 1 −  1 N 2  N (cid:88)  i,j  min  (cid:18)  (cid:19) dv,i,j/ maxi,j dv,i,j ds,i,j/ maxi,j ds,i,j + 10−2 , 1  (3)  Orthogonality = 1 −  2 N (N − 1)  N (cid:88)  i,j,i<j  |(cid:104)φi, φj(cid:105)| (cid:107)φi(cid:107)2 (cid:107)φj(cid:107)2  where N is the total number of samples, U (1, N ) means uniformly sample from [1, N ], ds,i,j = (cid:107)φi − φj(cid:107)2 and dv,i,j = |maxa Critic(φi, a) − maxa Critic(φj, a)|. Predict State from Visual Representation Besides the three metrics on visual representations and state-values, we further measure the quality of visual representation φi by predicting the system state si only using φi. The intuition is that a better visual representation should be able to capture the intrinsic system state more precisely. We utilize a two-layer MLP to regress the system state si on it corresponding visual representation φi. Mean squared error is applied to supervise the network as well as to evaluate the network on the test set.  To properly measure all these metrics, We ﬁrst collect a dataset in cartpole swingup from DMControl using state-based SAC, which is different from any methods we’ll benchmark to avoid bias. We run state-based SAC with ﬁve random seeds, and take the replay buffer of each run to form a dataset. The whole dataset has 12500 × 5 = 62500 state transitions. We measure Dynamics Awareness and Orthogonality on the full dataset, while Diversity is calculated for one run due to computational cost. For state prediction, we use the ﬁrst four runs as the training set and the last run is held for the evaluation.  Finally we benchmark selected methods with ﬁve different random seeds on cartpole swingup, and reports the metrics above every 100 model update steps. We demonstrate how the four metrics correlate to the environment step and agent performance as Fig. 9 and Fig. 10.  Fig. 9 shows how metrics change as training. Most of the methods converge to a similar Orthogonality, Dynamic Awareness and Diversity value. SAC-NoAug has a low Dynamic Awareness measure which could be used to explain its low performance. While a higher Dynamic Awareness measures does not bring extra scores for BYOL and SAC+AE. Similarly, lower Diversity value of DrQ and ELo-SAC do not hurt their performance either. Meanwhile, most of the metrics become relative stable after the ﬁrst 4000 steps. Therefore, we conﬁrm that the shallower layers of the neural networks in visual reinforcement learning converge faster as observed by Chen et al. [7].  Fig. 10 shows the correlation between metrics and the agent performance. We report the Pearson correlation coefﬁcient as Table 6. As Wang et al. [75] suggested, these metrics only measures certain properties of the visual representation, and they do not suggest that a property is necessary for better policy learning. However, we ﬁnd that the state prediction error is correlated to the agent performance to some extent, which may be valuable in some cases.  Table 6: Pearson correlation coefﬁcient between scores and representation metrics  Dynamic Awareness Orthogonality Diversity  Prediction MSE  0.284  0.435  0.111  0.625  13  Figure 9: Scores versus representation metric values  Figure 10: Scores versus representation metric values  4.3 Observation on Pretraining Framework  Besides the joint learning framework used in CURL and SAC+AE, Shelhamer et al. [67] investigate a pretraining framework to combine SSL with RL, and use self-supervised loss as an intrinsic reward to further boost performance during online learning. Recent works on policy learning (e.g., [56, 65, 74, 80, 86]) also take advantage of the self-supervised learning in a multi-step framework and show its great potential in solving challenging visual-based problems.  This pretraining framework is similar to how self-supervision has been beneﬁting supervised Com- puter Vision tasks ([6, 8, 10, 15, 31, 37, 57]): pretrain with self-supervised losses, and then ﬁnetune with the downstream task loss. Motivated by them, in this section, we design and benchmark the two-stage pretraining framework, replacing the joint learning framework used in CURL and SAC+AE.  14  In the ﬁrst stage, we use data collected by training a SAC-Aug(100) agent on the same task, and update the visual encoder only using self-supervised loss. We name this stage pretraining which means to use self-supervised losses to update the model and to be downstream task agnostic. Then in the second stage i.e., the online training stage, we only keep the trained encoder from the ﬁrst stage, and train an agent using SAC-Aug(100). The only difference between this stage and training an agent from scratch is that here the visual encoder has been “initialized” with the pre-trained weights while it is randomly initialized in SAC-Aug(100). This also means that the image encoder can be tuned by RL loss in the online training stage to match the online sample distribution. Fig. 11 compares two training frameworks, in which the rounded rectangle means to update the model with the labeled loss for one step.  Figure 11: Two learning frameworks for SSL + RL, the rounded rectangle means to update the model with the labeled loss for one step.  The methods using the pretraining framework have a preﬁx ‘Pretrain’. ‘Pretrain-Random’ means the data used for pretraining is collected by a random policy. In both cases, the pretraining framework has the same model update steps as the joint learning framework baseline. But note that the pretraining model has access to extra data collected by other policy, which makes it an unfair comparison. To this end, we test another joint learning conﬁguration named with a preﬁx ‘Longer’. Here we match the total number of environment steps (or collected data) to its pretraining variants. We compare two frameworks in six DMControl environments, Relative Scores are reported as Fig. 12 and the full results are shown as Table 7.  Figure 12: Relative Score of two learning frameworks for combining SSL to RL  In general, given the same amount of model update, the pretraining framework performs better than the joint learning framework except ELo-SAC (we believe this is because ELo-SAC search was done only under the joint learning framework). But such advantage of the pretraining framework may come from the extra data used in the pretraining stage. When the same amount of data is given, the longer joint-learning conﬁguration usually performs better than the pretraining methods except when AutoEncoder is the self-supervised loss. Such observations imply that the learning framework has different impacts on policy learning even if the same self-supervised loss is applied. It might not be the best practice to directly use the existing self-supervised losses designed for joint learning framework with the pretraining framework. On the other hand, we argue that on DMControl, the advantages of the pretraining framework come from the access to extra data instead of the framework itself. When the total environment step is limited and no previous data has been collected, the joint learning framework can better solve DMControl problems.  15  Table 7: Comparison of the two frameworks. Methods in gray are without self-supervised loss for reference. The total amount of data/environment step at each stage is listed as the second and the third column.  Agent  Pretraining env.step  Online env.step  cartpole,swingup  reacher,easy  cheetah,run  SAC-Aug(100) RAD DrQ  SAC+AE Longer-AE Pretrain-AE Pretrain-Random-AE  CURL Longer-CURL Pretrain-CURL Pretrain-Random-CURL  DINO Longer-DINO Pretrain-DINO Pretrain-Random-DINO  ELo-SAC Longer-ELo-SAC Pretrain-ELo-SAC Pretrain-Random-ELo-SAC  0 0 0  0 0 100k 100k  0 0 100k 100k  0 0 100k 100k  0 0 100k 100k  100k 100k 100k  100k 200k 100k 100k  100k 200k 100k 100k  100k 200k 100k 100k  100k 200k 100k 100k  563.4±235.0 786.4±95.1 663.6±217.5  654.4±222.1 508.8±111.5 692.7±163.1  172.1±64.0 387.9±81.3 345.9±70.0  388.8±130.1 467.1±196.9(↑78.3) 759.1±99.3(↑370.3) 736.0±97.4(↑347.2)  471.5±89.9 776.2±82.2(↑304.7) 705.4±138.3(↑233.9) 745.3±124.5(↑273.8)  610.7±188.8 858.1±21.4(↑247.4) 759.3±110.8(↑148.6) 758.6±86.1(↑147.9)  481.3±130.4 578.6±160.7(↑97.3) 757.8±174.0(↑276.5) 858.0±155.0(↑376.7)  291.8±59.8 359.0±57.6(↑67.2) 419.8±41.1(↑128.0) 405.3±55.5(↑113.5)  569.8±179.4 688.8±229.8(↑119.0) 754.5±106.2(↑184.7) 804.8±205.8(↑235.0)  215.1±57.3 307.6±57.3(↑92.5) 213.0±56.7(↓-2.1) 224.3±60.6(↑9.2)  635.9±229.6 861.4±131.8(↑225.5) 635.0±172.0(↓-0.9) 712.6±126.6(↑76.7)  176.7±64.1 248.6±49.3(↑71.9) 344.7±56.5(↑168.0) 355.6±77.5(↑178.9)  772.8±167.3 866.6±30.0(↑93.8) 617.9±147.1(↓-154.9) 519.8±175.4(↓-253.0)  478.3±159.9 753.8±159.9(↑275.5) 503.6±220.1(↑25.3) 548.0±136.4(↑69.7)  359.7±69.7 489.6±149.7(↑129.9) 400.2±63.6(↑40.5) 302.9±126.4(↓-56.8)  Agent  Pretraining env.step  Online env.step  ball_in_cup,catch  walker,walk  ﬁnger,spin  SAC-Aug(100) RAD DrQ  SAC+AE Longer-AE Pretrain-AE Pretrain-Random-AE  CURL Longer-CURL Pretrain-CURL Pretrain-Random-CURL  DINO Longer-DINO Pretrain-DINO Pretrain-Random-DINO  ELo-SAC Longer-ELo-SAC Pretrain-ELo-SAC Pretrain-Random-ELo-SAC  0 0 0  0 0 100k 100k  0 0 100k 100k  0 0 100k 100k  0 0 100k 100k  100k 100k 100k  100k 200k 100k 100k  100k 200k 100k 100k  100k 200k 100k 100k  100k 200k 100k 100k  541.4±306.2 879.9±82.0 914.9±21.2  422.1±250.8 522.1±95.5 523.9±182.2  724.6±154.9 920.0±103.6 935.6±201.3  616.1±169.9 579.4±274.0(↓-36.7) 914.7±129.0(↑298.6) 903.1±219.9(↑287.0)  730.0±179.4 930.2±205.4(↑200.2) 921.0±25.5(↑191.0) 874.5±298.3(↑144.5)  466.7±394.3 952.6±48.9(↑485.9) 748.1±164.7(↑281.4) 904.6±266.6(↑437.9)  402.6±161.5 700.3±232.7(↑297.7) 308.0±243.1(↓-94.6) 107.8±216.1(↓-294.8)  799.0±138.9 887.8±127.4(↑88.8) 869.8±150.6(↑70.8) 793.6±175.9(↓-5.4)  442.6±87.1 701.5±148.0(↑258.9) 277.7±152.0(↓-164.9) 356.2±131.8(↓-86.4)  717.8±136.5 732.1±146.2(↑14.3) 785.8±134.1(↑68.0) 693.8±178.2(↓-24.0)  457.0±213.4 722.6±251.4(↑265.6) 260.1±145.8(↓-196.9) 197.5±147.2(↓-259.5)  863.1±187.2 926.0±128.3(↑62.9) 877.5±123.8(↑14.4) 823.4±75.4(↓-39.7)  888.3±90.6 946.3±74.5(↑58.0) 505.8±301.3(↓-382.5) 466.2±200.3(↓-422.1)  537.5±164.5 750.7±321.7(↑213.2) 115.5±152.4(↓-422.0) 179.7±189.5(↓-357.8)  789.3±198.2 919.2±154.1(↑129.9) 711.5±161.7(↓-77.8) 763.6±140.7(↓-25.7)  5 Dicussion  In this paper, we fail to ﬁnd a single self-supervised loss or a combination of multiple SSL methods that consistently improve RL under the existing joint learning framework with image augmentation. Although many previous papers have shown the great potential combining SSL with RL while using the original image, these conclusions do not directly apply to the conﬁgurations with sufﬁcient image augmentations. Our observations from extensive experiments also suggest that the impacts of the existing self-supervised losses with the joint learning framework for RL is limited, while a carefully designed image augmentation method plays a more important role.  We also conﬁrm that there is no golden rule of applying image augmentation method or self-supervised loss that can dominate all tasks or environments in online pixel-based reinforcement learning. The pretraining framework can take advantage of extra data collected by other agents, however, it is also limited.  We are excited to see future works that introduce more self-supervised losses designed speciﬁcally for RL, as well as novel training framework that can beneﬁt policy learning. Further investigation on  16  the properties of different tasks and environments would also guide us to easily pick up an effective self-supervised loss or image augmentation method in a certain scenario. Finally, reliable metrics of representation that can reﬂect policy learning performance should bring the research on sample efﬁciency of existing RL methods to the next level.  6 Related Work  Self-supervised learning can ﬁt in policy learning in multiple fashions and at different stages. Some works [24, 64–66, 69, 74, 80, 86] use SSL for representation learning in a pre-training stage before policy learning. Speciﬁcally, World model [24] uses a VAE to model the visual representation before policy learning. TCN [64], ATC [69] and FERM [86] all take advantage of contrastive learning. TCN and ATC acquire samples for contrastive learning at different timestamps of a trajectory, while FERM compares augmented views like MoCo [31]. MVP [80] follows MAE [32] to train a visual encoder and even outperforms supervised pre-training. RRL [65] and VRL3 [74] also beneﬁt from pre-training a deeper visual encoder on large datasets like ImageNet [16].  Others [1, 23, 27, 35, 42, 44, 46, 49, 54, 63, 63, 81, 84, 85, 87, 88] jointly optimize the self- supervised loss with RL. CPC [54, 68] combines InfoNCE as an auxiliary loss with A2C. DVRL [35], PlaNet [27], and SLAC [44] predict future observations and rewards given current observation and action. Transporter [41] and VAI [77] train an unsupervised keypoint detector to discovery critical objects in image for control. After the agent is deployed, SSL can be used to continuously improve the policy [28].  Shelhamer et al. [67] provided an early study on a limited number of self-supervised losses within both pretraining framework and the joint learning framework for RL. This was done without any image augmentations, different from this paper. Chen et al. [9] focus on imitation learning and test multiple SSL objectives for representation learning on various environments. They conﬁrmed the critical role of image augmentation in imitation learning and showed inconsistencies of performance across environments. Our investigation supports some of their observations, beyond that, our evolving loss, real robot environment and representation analysis provide unique perspectives for online reinforcement learning.  Acknowledgments  We thank Kumara Kahatapitiya and other lab members of Robotics Lab for valuable discussion. We thank Hanyi Yu and Rui Miao for their helpful feedback.  This work is supported by the National Science Foundation (IIS-2104404 and CNS-2104416). This work is also supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Ministry of Science and ICT (No.2018-0-00205, Development of Core Technology of Robot Task-Intelligence for Improvement of Labor Condition.  References  [1] Rishabh Agarwal, Marlos C Machado, Pablo Samuel Castro, and Marc G Bellemare. Contrastive behavioral similarity embeddings for generalization in reinforcement learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.  [2] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Belle- mare. Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Information Processing Systems (NeurIPS), 34, 2021.  [3] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013.  [4] Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforce- ment learning. In Proceedings of the International Conference on Machine Learning (ICML), pages 449–458. PMLR, 2017.  17  [5] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems (NeurIPS), 33:9912–9924, 2020.  [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9650–9660, 2021.  [7] Lili Chen, Kimin Lee, Aravind Srinivas, and Pieter Abbeel. Improving computational efﬁciency in visual reinforcement learning via stored embeddings. Advances in Neural Information Processing Systems (NeurIPS), 34, 2021.  [8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the International Conference on Machine Learning (ICML), pages 1597–1607. PMLR, 2020.  [9] Xin Chen, Sam Toyer, Cody Wild, Scott Emmons, Ian Fischer, Kuang-Huei Lee, Neel Alex, Steven H Wang, Ping Luo, Stuart Russell, et al. An empirical investigation of representation learning for imitation. In Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.  [10] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15750–15758, 2021.  [11] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum  contrastive learning. arXiv preprint arXiv:2003.04297, 2020.  [12] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised visual transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021.  [13] Rumen Dangovski, Li Jing, Charlotte Loh, Seungwook Han, Akash Srivastava, Brian Che- ung, Pulkit Agrawal, and Marin Soljacic. Equivariant self-supervised learning: Encouraging equivariance in representations. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.  [14] Srijan Das and Michael S Ryoo. Stc-mix: Space, time, channel mixing for self-supervised video  representation. arXiv preprint arXiv:2112.03906, 2021.  [15] Srijan Das and Michael S Ryoo. Viewclr: Learning self-supervised video representation for  unseen viewpoints. arXiv preprint arXiv:2112.03905, 2021.  [16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 248–255. Ieee, 2009.  [17] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In Proceedings of the International Conference on Computer Vision (ICCV), pages 1422–1430, 2015.  [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.  [19] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.  18  [20] Partha Ghosh, Mehdi SM Sajjadi, Antonio Vergari, Michael Black, and Bernhard Schölkopf. From variational to deterministic autoencoders. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.  [21] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.  [22] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems (NeurIPS), 33:21271–21284, 2020.  [23] Zhaohan Daniel Guo, Bernardo Avila Pires, Bilal Piot, Jean-Bastien Grill, Florent Altché, Rémi Munos, and Mohammad Gheshlaghi Azar. Bootstrap latent-predictive representations for multitask reinforcement learning. In Proceedings of the International Conference on Machine Learning (ICML), pages 3875–3886. PMLR, 2020.  [24] David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.  [25] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the International Conference on Machine Learning (ICML), pages 1861–1870. PMLR, 2018.  [26] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.  [27] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In Proceedings of the International Conference on Machine Learning (ICML), pages 2555–2565. PMLR, 2019.  [28] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenyà, Pieter Abbeel, Alexei A Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.  [29] Nicklas Hansen, Hao Su, and Xiaolong Wang. Stabilizing deep q-learning with convnets and vision transformers under data augmentation. Advances in Neural Information Processing Systems (NeurIPS), 34, 2021.  [30] Hado Hasselt. Double q-learning. Advances in Neural Information Processing Systems  (NeurIPS), 23, 2010.  [31] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9729–9738, 2020.  [32] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked  autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.  [33] Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improve- ments in deep reinforcement learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), 2018.  [34] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.  [35] Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep variational reinforcement learning for pomdps. In Proceedings of the International Conference on Machine Learning (ICML), pages 2117–2126. PMLR, 2018.  19  [36] Longlong Jing, Xiaodong Yang, Jingen Liu, and Yingli Tian. Self-supervised spatiotemporal feature learning via video rotation prediction. arXiv preprint arXiv:1811.11387, 2018.  [37] Kumara Kahatapitiya, Zhou Ren, Haoxiang Li, Zhenyu Wu, and Michael S Ryoo. Self- supervised pretraining with classiﬁcation labels for temporal activity detection. arXiv preprint arXiv:2111.13675, 2021.  [38] James Kennedy and Russell Eberhart. Particle swarm optimization. In Proceedings of ICNN’95- international conference on neural networks, volume 4, pages 1942–1948. IEEE, 1995.  [39] Dahun Kim, Donghyeon Cho, and In So Kweon. Self-supervised video representation learning with space-time cubic puzzles. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), volume 33, pages 8545–8552, 2019.  [40] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the  International Conference on Learning Representations (ICLR), 2014.  [41] Tejas D Kulkarni, Ankush Gupta, Catalin Ionescu, Sebastian Borgeaud, Malcolm Reynolds, Andrew Zisserman, and Volodymyr Mnih. Unsupervised learning of object keypoints for perception and control. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019.  [42] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised represen- tations for reinforcement learning. In Proceedings of the International Conference on Machine Learning (ICML), pages 5639–5650. PMLR, 2020.  [43] Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. Advances in Neural Information Processing Systems (NeurIPS), 33:19884–19895, 2020.  [44] Alex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. Advances in Neural Information Processing Systems (NeurIPS), 33:741–752, 2020.  [45] Hankook Lee, Kibok Lee, Kimin Lee, Honglak Lee, and Jinwoo Shin. Improving transferability of representations via augmentation-aware self-supervision. Advances in Neural Information Processing Systems (NeurIPS), 34, 2021.  [46] Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee, John Canny, and Sergio Guadarrama. Predictive information accelerates learning in rl. Advances in Neural Information Processing Systems (NeurIPS), 33:11890–11901, 2020.  [47] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.  [48] Yijiong Lin, Jiancong Huang, Matthieu Zimmer, Yisheng Guan, Juan Rojas, and Paul Weng. Invariant transform experience replay: Data augmentation for deep reinforcement learning. IEEE Robotics and Automation Letters, 5(4):6615–6622, 2020.  [49] Bogdan Mazoure, Remi Tachet des Combes, Thang Long Doan, Philip Bachman, and R Devon Hjelm. Deep reinforcement and infomax learning. Advances in Neural Information Processing Systems (NeurIPS), 33:3686–3698, 2020.  [50] Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shufﬂe and learn: unsupervised learning using temporal order veriﬁcation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 527–544. Springer, 2016.  [51] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.  [52] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In Proceedings of the European Conference on Computer Vision (ECCV), pages 69–84. Springer, 2016.  20  [53] Edwin Olson. Apriltag: A robust and ﬂexible visual ﬁducial system. In 2011 IEEE international  conference on robotics and automation (ICRA), pages 3400–3407. IEEE, 2011.  [54] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive  predictive coding. arXiv preprint arXiv:1807.03748, 2018.  [55] Tian Pan, Yibing Song, Tianyu Yang, Wenhao Jiang, and Wei Liu. Videomoco: Contrastive video representation learning with temporally adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11205– 11214, 2021.  [56] Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Gupta. The unsurpris- ing effectiveness of pre-trained vision models for control. arXiv preprint arXiv:2203.03580, 2022.  [57] AJ Piergiovanni, Anelia Angelova, and Michael S Ryoo. Evolving losses for unlabeled video representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.  [58] Silviu Pitis, Elliot Creager, and Animesh Garg. Counterfactual data augmentation using locally factored dynamics. Advances in Neural Information Processing Systems (NeurIPS), 33:3976– 3990, 2020.  [59] Roberta Raileanu and Rob Fergus. Decoupling value and policy for generalization in reinforce- ment learning. In Proceedings of the International Conference on Machine Learning (ICML), pages 8787–8798. PMLR, 2021.  [60] Roberta Raileanu, Maxwell Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus. Automatic data augmentation for generalization in reinforcement learning. Advances in Neural Information Processing Systems (NeurIPS), 34, 2021.  [61] Kanchana Ranasinghe, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan, and Michael Ryoo. Self-supervised video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.  [62] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.  [63] Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efﬁcient reinforcement learning with momentum predictive representations. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.  [64] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and Google Brain. Time-contrastive networks: Self-supervised learning from video. In 2018 IEEE international conference on robotics and automation (ICRA), pages 1134–1141. IEEE, 2018.  [65] Rutav Shah and Vikash Kumar. Rrl: Resnet as representation for reinforcement learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.  [66] Jinghuan Shang and Michael S Ryoo. Self-supervised disentangled representation learning for third-person imitation learning. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 214–221. IEEE, 2021.  [67] Evan Shelhamer, Parsa Mahmoudieh, Max Argus, and Trevor Darrell. Loss is its own reward: Self-supervision for reinforcement learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.  [68] Samarth Sinha, Ajay Mandlekar, and Animesh Garg. S4rl: Surprisingly simple self-supervision for ofﬂine reinforcement learning in robotics. In Conference on Robot Learning, pages 907–917. PMLR, 2022.  [69] Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation In Proceedings of the International Conference on  learning from reinforcement learning. Machine Learning (ICML), pages 9870–9879. PMLR, 2021.  21  [70] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.  [71] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efﬁcient learners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602, 2022.  [72] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), volume 30, 2016.  [73] Hado P van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in reinforcement learning? Advances in Neural Information Processing Systems (NeurIPS), 32, 2019.  [74] Che Wang, Xufang Luo, Keith Ross, and Dongsheng Li. Vrl3: A data-driven framework for  visual deep reinforcement learning. arXiv preprint arXiv:2202.10324, 2022.  [75] Han Wang, Erfan Miahi, Martha White, Marlos C Machado, Zaheer Abbas, Raksha Ku- maraswamy, Vincent Liu, and Adam White. Investigating the properties of neural network representations in reinforcement learning. arXiv preprint arXiv:2203.15955, 2022.  [76] Kaixin Wang, Bingyi Kang, Jie Shao, and Jiashi Feng. Improving generalization in reinforce- ment learning with mixture regularization. Advances in Neural Information Processing Systems (NeurIPS), 33:7968–7978, 2020.  [77] Xudong Wang, Long Lian, and Stella X Yu. Unsupervised visual attention and invariance for reinforcement learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6677–6687, 2021.  [78] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Duel- ing network architectures for deep reinforcement learning. In Proceedings of the International Conference on Machine Learning (ICML), pages 1995–2003. PMLR, 2016.  [79] Wikipedia contributors. Interquartile mean, 2022. URL https://en.wikipedia.org/wiki/  Interquartile_mean. [Online; accessed 13-May-2022].  [80] Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for  motor control. arXiv preprint arXiv:2203.06173, 2022.  [81] Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving sample efﬁciency in model-free reinforcement learning from images. arXiv preprint arXiv:1910.01741, 2019.  [82] Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.  [83] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2022.  [84] Pei Yingjun and Hou Xinwen. Learning representations in reinforcement learning: An informa-  tion bottleneck approach. arXiv preprint arXiv:1911.05695, 2019.  [85] Tao Yu, Cuiling Lan, Wenjun Zeng, Mingxiao Feng, Zhizheng Zhang, and Zhibo Chen. Playvir- tual: Augmenting cycle-consistent virtual trajectories for reinforcement learning. Advances in Neural Information Processing Systems (NeurIPS), 34, 2021.  [86] Albert Zhan, Philip Zhao, Lerrel Pinto, Pieter Abbeel, and Michael Laskin. A framework for efﬁcient robotic manipulation. In Advances in Neural Information Processing Systems (NeurIPS), 2021.  22  [87] Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.  [88] Jinhua Zhu, Yingce Xia, Lijun Wu, Jiajun Deng, Wengang Zhou, Tao Qin, and Houqiang Li. Masked contrastive representation learning for reinforcement learning. arXiv preprint arXiv:2010.07470, 2020.  A Appendix  A.1 Background on Pairwise Learning  We coin the term “Pairwise” Learning for the frameworks that learn visual representations based on semantic invariance between dual-stream encoder representations. A general pairwise learning method ﬁrst generates multiple augmented views by applying a series of random image augmentations to the input sample, then clusters views with the same semantics in the representation space. Optionally in such frameworks, methods using contrastive losses repel samples with different semantics. Previous works not only have built various self-supervised tasks that beneﬁt representation learning, but also show that learned representations can beneﬁt different downstream tasks.  In this paper, we focus on four representative pairwise learning methods, MoCo [11, 12, 31], BYOL [22], SimSiam [10] and DINO [6]. Speciﬁcally, MoCo takes advantage of contrastive loss and negative samples in the mini-batch, while BYOL, SimSiam and DINO focus on the similarity of the same image across diverse augmentations.  MoCo Momentum Contrast (MoCo) takes advantage of a contrastive loss function InfoNCE [54] with dot product similarity. It starts from two identical encoder networks, an online encoder fq and a momentum encoder fk.  At each training step, a mini-batch of N images x are uniformly sampled from a training set D. Given two distributions of image augmentations T and T (cid:48), two image augmentations t ∼ T and t(cid:48) ∼ T (cid:48) are sampled respectively and applied to x, resulting in 2N samples. Augmented images, v = t(x) and v(cid:48) = t(cid:48)(x), are called views. Then, v and v(cid:48) are fed to two encoders to generate queries q = fq(v) and keys k = fk(v(cid:48)). For each view vi in v and its corresponding query qi = fq(vi), the contrastive loss is formulated as:  LMoCo,qi = − log  sim(qi, ki) j=1 sim(qi, kj)  (cid:80)N  (4)  where sim(qi, ki) = exp(qi · sg[ki]/τ ), sg[·] implies stop gradients and τ is a temperature hyper- parameter. This loss encourages qi to be similar to its corresponding key ki (called positive), but dissimilar to other keys (called negatives) in the mini-batch. The online encoder fq with parameters θq is updated by above contrastive loss. The momentum encoder fk with parameters θk, an Exponential Moving Average (EMA) of fq, is updated by  θk := mθk + (1 − m)θq,  (5)  where m ∈ [0, 1) is a momentum coefﬁcient that controls how fast θk updates towards the online network θq. Finally, fk will be discarded once the training completes.  BYOL Similar to MoCo, in addition to fq and fk, BYOL maintains two identical projection networks gq, gk and one prediction networks pq (See Fig. 2a). BYOL also starts from inputs v and v(cid:48) but calculates the projection z1 = gq(fq(v)) and z2 = gk(fk(v(cid:48))), and tries to regress z2 from z1 using the prediction network pq.  After applying l2-normalization to the prediction pq(z1) and the target projection z2, a mean squared error is measured as:  LBYOL,1 = (cid:107)pq(z1) − sg[z2](cid:107)2  2 = 2 − 2  pq(z1) · sg[z2] (cid:107)pq(z1)(cid:107)2 · (cid:107)sg[z2](cid:107)2  (6)  whose value is low when pq(z1) is close to z2.  23  2 = gk(fk(v)), as LBYOL,2 = (cid:107)pq(z(cid:48)  Similarly, by swapping v and v(cid:48), another symmetric loss can be applied on top of z(cid:48) and z(cid:48) LBYOL,2)/2. The parameters of fk and gk are also the EMA of fq and gq respectively. Finally, fk, gq, gk and pq will be discarded once the training completes.  1 = gq(fq(v(cid:48))) 2. The total loss is LBYOL = (LBYOL,1 +  1) − sg[z(cid:48)  2](cid:107)2  SimSiam SimSiam (Simple Siamese) shares the same architecture as BYOL, while the parameters of the ‘momentum’ branch of SimSiam are always tied to the ‘online’ branch (See Fig. 2b). Therefore, SimSiam only maintains one branch, including an encoder f , a projector g and a predictor p. SimSiam uses negative cosine similarity to encourage the predicted representation h = p(g(f (v)) to be similar to the projected representation of another view g(f (v(cid:48))), as follows:  LSimSiam,1 = −  h (cid:107)h(cid:107)2 sg[g(f (v(cid:48)))] (cid:107)sg[g(f (v(cid:48)))](cid:107)2  (7)  Another symmetric loss term can also be derived as LSimSiam,2 = − h(cid:48) , where (cid:107)h(cid:48)(cid:107)2 h(cid:48) = p(g(f (v(cid:48)))). The total loss is LSimSiam = (LSimSiam,1 + LSimSiam,2)/2. And gq will be discarded once the model is trained.  sg[g(f (v))] (cid:107)sg[g(f (v))](cid:107)2 DINO DINO shares a similar overall structure as MoCo which contains two encoders fq and fk, and fk is the EMA of fq (See Fig. 2c). The outputs of both encoder networks are normalized as probability distributions over K dimensions by applying softmax with a temperature parameter τt, and K is the dimension of fq(v). DINO also maintains a centering vector C with dimension K. Following the formulation of knowledge distillation, a cross-entropy loss is applied to encourage the output distribution of fq to become similar to a centered distribution from fk, as follows:  LDINO,1 = −P (sg[fk(v(cid:48))] − C) · log P (fq(v))  (8)  where P (x) = softmax(x/τt). By swapping v and v(cid:48) in Eq. 8, another loss LDINO,2 which is symmetric to LDINO,1 can be derived. And the total loss is the mean value of LDINO,1 and LDINO,2. After each step of optimization, fk is updated by Eq. 5. C also gets updated in a similar manner: C := mcC + (1 − mc) · mean(fk(v), fk(v(cid:48)))  (9)  Here mc ∈ [0, 1) is another momentum coefﬁcient.  A.2 Implementation Details  Here we present the implementation details in all settings.  A.2.1 General Joint Learning Framework  The general joint learning framework starts from the ofﬁcial implementation of CURL [42] for DMControl and Atari. For different self-supervised learning losses, we only replace the contrastive learning head of CURL with different SSL head and update the loss calculation. All the hyper- parameters are left untouched, except that we use learning rate 10−3 for all DMControl environments. The detailed hyper-parameters can be found at Table 8 (DMControl) and Table 10 (Atari). We keep the most of hyper-parameters from DMControl for the real-world robot experiments. The modiﬁed conﬁguration is listed as Table 9.  We use the ofﬁcial implementations of DrQ [82] and RAD [43] for DMControl benchmark. On Atari we re-implement DrQ (denoting as DrQ*) with the joint learning framework and image augmentation from CURL.  A.2.2 Losses for Self-supervised Learning  Pairwise Learning In this section we replace the contrastive learning head with projectors and encoders depending on the exact loss. All the projectors and predictors are two-layer MLPs with ReLU in the middle. The input dimension which is the output dimension of the encoder, (50 on DMControl and uArm Reacher, and 576 on Atari). The hidden dimension of the MLP is 256 and the output dimension is 128. We use the same encoder EMA update rate (τ = 0.05 for SAC and 0.001  24  Table 8: hyper-parameters used for DMControl with general joint learning framework  Hyperparameter Value  Image augmentation Random crop  Image size before augmentation Image size after augmentation Replay buffer size Number of environment step Initial explore steps Stacked frames Action repeat  (100, 100) (84, 84) 100000 100000 1000 3 2 ﬁnger, spin; walker, walk 8 cartpole, swingup 4 otherwise  Critic target update frequency Actor update freq EMA τ for Q(cid:48), gk EMA τ for fk Discount γ  2 2 0.01 0.05 .99 Initial α 0.1  Convolutional layers in fq 4 32 Number of ﬁlters Fully connected layer in fq 1 Tanh after fq False 50 Image representation dimension q, Ap 3 MLP Hidden units 1024 MLP Non-linearity ReLU  MLP layer of Qi  Optimizer Adam  Learning rate (fq, Qi  (β1, β2) → (fq, Qi  q, Ap) (β1, β2) → (α) q, Ap) Learning rate (α) Batch size  (.9, .999) (.5, .999) 10−3 10−4 512  Evaluation episodes Train with random seeds  10 10  Table 9: Modiﬁed hyper-parameters for real-world robot experiments  Hyperparameter Value  Stacked frames Action repeat Number of environment step Train with random seeds  1 1 200000 5  for Rainbow) to update the projectorgk (if applicable) in the target branch. The applied losses are introduced in Sec. A.1.  Transformation Awareness We use a two-layer MLP with ReLU as the classiﬁer for both rotation classiﬁcation and shufﬂe classiﬁcation. The hidden dimension of the MLP is 1024 and the classiﬁer is supervised by a cross-entropy loss. The output dimension is 4 for four-fold rotation classiﬁcation, and 1 for binary shufﬂe classiﬁcation.  Reconstruction In this section, we follow the ofﬁcial implementation of SAC+AE [81] and apply the same image augmentation from CURL. The decoder has one fully connected layer and the same number of transposed convolutional layers as the convolutional layers in the encoder. When the  25  Table 10: hyper-parameters used for Atari with general joint learning framework  Hyperparameter Value  Image augmentation Random crop (80, 80) →  Image size before augmentation Image size after augmentation Replay buffer size Number of environment step Initial explore steps Stacked frames Frame skip Action repeat  Replication padding (88, 88) → Random crop (84, 84) (84, 84) (84, 84) 100000 400000 1600 4 4 4  Discount γ Priority exponent Priority correction Target update frequency Support of Q distribution EMA τ for fk Reward Clipping  .99 0.5 0.4 → 1 2000 51 bins 0.05 [−1, 1]  Max gradient norm 10  Convolutional layers in fq Number of ﬁlters Image representation dimension  2 (32, 64) 576  Fully connected layer type Noisy Nets  Noisy nets parameter MLP layer of Qq MLP Hidden units MLP Non-linearity ReLU  0.1 2 256  Optimizer Adam  (β1, β2) → (fq, Qq) Learning rate Batch size  (.9, .999) 10−4 32  Evaluation episodes Train with random seeds  10 20  output image from the decoder is smaller than the ground truth, we crop the ground truth to the size of the decoder output from upper left corner.  For MAE we start from augmented SAC+AE and ﬁrst divide the augmented image into non- overlapping patches in the spatial domain with a size of 4 × 4. Then we randomly mask 50% of the patches by setting the pixel value of the masked patches to zero. Finally, the reconstruction loss is modiﬁed to calculate MSE only over the masked patches. Other regularization losses are left untouched.  RL Context Prediction For all kinds of the losses, the dimensions of all the fully connected layers and hidden layers in MLPs are 1024.  A.3 Evolving Losses  We choose PSO (Particle Swarm Optimization) [38] to search the optimal combination for ELo-SAC and ELo-Rainbow. The population of ELo-SAC is 50 and ELo-Rainbow has a population of 30. When calculating the velocity of a particle, the weight of previous velocity (inertia weight) is 0.5 and the cognitive coefﬁcient and social coefﬁcient are 2.  Code for ELo-SAC and ELo-Rainbow will be available soon.  26  A.4 Autonomous Robot Learning Environment  The learning environment needs to automatically generate reward and randomly reset itself for the autonomous operation.  To enable automatic reward generation, we make an automatic calibration framework to locate target in 3D, and calibrate the top-down camera before any experiments. We use AprilTag [53] to locate the robot position in the image plane, and read the 3D robot coordinates directly from the robot. By doing so, we can build a map between 2D image coordinates and 3D robot coordinates. The 2D coordinates of the target is ﬁrst extracted by a simple color threshold. Then, given the constant height of the target, we can obtain 3D target location from its 2D image coordinates according to the 2D↔3D map. Such conversion is critical for not only generating rewards but also randomly resetting the environment.  Regrading the random reset, at the beginning of each episode, the robot arm picks up the target cube given the 3D coordinates converted from calibrated images and releases the cube at a random location with a certain height like throwing a dice. The new location of the target cube will be randomly initialized and saved for generating rewards. After the robot arm moves to a ﬁxed pre-assigned starting point, the environmental reset is done, then the RL agent takes over the control. The RL agent can perform regular online training until the episode ends. Finally, the environment will take over the robot control and repeat the reset process for the next episode.  A.5 Detailed Results  We demonstrate the score distribution of each method in different environments. See DMControl at Fig. 13 and Atari at Fig. 14.  A.6 Manually Balance Two Self-supervised Losses  In this section we further explore the ways to manually combine two self-supervised losses. Extract- AR, Guess-AF and Predict-FR are methods manually designed to combine two individual losses However, Guess-AF and Predict-FR are not better than the single self-supervised loss in their combinations (see Guess-Action and Predict-Reward in Table 4 and Fig. 5). Considering that Extract- AR, Guess-AF and Predict-FR concatenate both the outputs and apply supervision by averaging loss per element of the output, the target with a higher dimension will naturally get more penalty due to the larger number of elements in the output. We further test ‘Balanced’ conﬁguration, where we only modify how the supervision is applied, and this trick bring overall improvements. Take Extract-AR as an example, in ‘Balanced’ setting, we ﬁrst calculate loss regarding action prediction and reward prediction separately, then the total self-supervised loss is the average of both the action prediction loss and the reward prediction loss. By adjusting the combination weights, the ‘Balanced’ trick bring overall improvements on top of all three methods as shown in Table 11. Such observation suggests that we need to carefully design how the two losses are combined, which is getting trickier as the number of combined losses increases.  Table 11: Scores on DMControl improved by manually balancing two self-supervised losses, suggest- ing the importance of weight hyper-parameters when combining multiple losses. Methods in gray are without self-supervised loss for reference. cartpole,swingup ball_in_cup,catch  reacher,easy  walker,walk  cheetah,run  ﬁnger,spin  Agent  SAC-Aug(100)  541.4±306.2 RAD 879.9±82.0 DrQ 914.9±21.2  563.4±235.0 786.4±95.1 663.6±217.5  172.1±64.0 387.9±81.3 345.9±70.0  724.6±154.9 920.0±103.6 935.6±201.3  654.4±222.1 508.8±111.5 692.7±163.1  422.1±250.8 522.1±95.5 523.9±182.2  Extract-AR 822.2±240.5  Extract-AR-Balanced  897.9±113.9(↑75.7)  592.9±124.7 582.3±119.2(↓-10.6)  225.8±60.7 230.1±31.8(↑4.3)  783.0±112.0 881.5±114.2(↑98.5)  645.4±207.0 720.5±136.4(↑75.1)  464.2±141.3 533.8±100.8(↑69.6)  Guess-AF Guess-AF-Balanced  329.8±298.4 918.4±353.5(↑588.6)  140.7±144.0 536.1±190.3(↑395.4)  0.9±22.8 191.2±78.6(↑190.3)  880.0±59.5 842.9±67.9(↓-37.1)  382.9±265.0 462.0±208.7(↑79.1)  494.7±112.7 507.0±128.7(↑12.3)  Predict-FR 750.3±256.0  Predict-FR-Balanced  829.6±241.6(↑79.3)  723.2±167.5 751.0±90.0(↑27.8)  12.4±35.7 216.2±77.6(↑203.8)  861.5±49.2 864.8±72.2(↑3.3)  636.1±201.4 882.1±87.4(↑246.0)  270.0±154.9 472.9±189.7(↑202.9)  A.7 Ablations on Hyper-parameters  In this section we demonstrate that SSL + RL is sensitive to image augmentation magnitude and learning rate.  27  Figure 13: DMControl score distribution  28  Figure 14: Atari score distribution  The left part of Fig. 15 shows how image augmentation signiﬁcantly effects the performance. The image size before random crop can be regarded as an indicator of the magnitude of random crop, larger the size, stronger the augmentation. There is a trend that the score ﬁrst increases and then decreases as the image augmentation getting stronger.  The right part of Fig. 15 shows how the learning rate of self-supervised loss effects the performance. In this group of ablations, we only change the learning rate for SSL and leave the RL part untouched. We run SAC-Aug(100) for reference with its ﬁxed conﬁguration, because it does not have SSL. The results suggest that a smaller learning rate for SSL may improve the performance. Therefore, it is necessary to search for the absolute weights of losses, which is equivalent to searching for the learning rate.  29  Figure 15: Ablations on Cheetah, run, environment step = 400k, batch size = 128, Num. of seeds = 5, conﬁdence interval = 50%  30 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 114156, but `max_length` is set to 10000. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_1195/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">4187612398.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 3&gt;</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_1195/4187612398.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_1195/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2796800581.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">display_answer</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_1195/2796800581.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/autodl-tmp/langchain-ChatGLM/chains/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">local_doc_qa.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">201</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_knowledge_based_answer</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 │   │   </span>prompt = generate_prompt(related_docs, query)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> streaming:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>201 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> result, history <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.llm._call(prompt=prompt,                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   │   │   │   │   │   │   │   │   │   │   │     </span>history=chat_history):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 │   │   │   │   </span>history[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>] = query                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">204 │   │   │   │   </span>response = {<span style=\"color: #808000; text-decoration-color: #808000\">\"query\"</span>: query,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/autodl-tmp/langchain-ChatGLM/models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">chatglm_llm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">78</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call</span>                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 75 │   │   │     </span>stop: Optional[List[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>]] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>:                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 76 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(prompt)                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 77 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.streaming:                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 78 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> inum, (stream_resp, _) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model.stream_chat(                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 79 │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.tokenizer,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 80 │   │   │   │   │   </span>prompt,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 81 │   │   │   │   │   </span>history=history[-<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.history_len:-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.history_len &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> [   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/miniconda3/lib/python3.8/site-packages/torch/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_contextlib.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">35</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generator_context</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 32 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 33 │   │   │   # Issuing `None` to a generator fires it up</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 34 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ctx_factory():                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 35 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>response = gen.send(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 36 │   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 37 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 38 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1311</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">stream_chat</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1308 │   │   │   </span>prompt += <span style=\"color: #808000; text-decoration-color: #808000\">\"[Round {}]\\n问：{}\\n答：\"</span>.format(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(history), query)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1309 │   │   </span>inputs = tokenizer([prompt], return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>)                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1310 │   │   </span>inputs = inputs.to(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1311 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> outputs <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.stream_generate(**inputs, **gen_kwargs):                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1312 │   │   │   </span>outputs = outputs.tolist()[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>][<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(inputs[<span style=\"color: #808000; text-decoration-color: #808000\">\"input_ids\"</span>][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]):]                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1313 │   │   │   </span>response = tokenizer.decode(outputs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1314 │   │   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.process_response(response)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/miniconda3/lib/python3.8/site-packages/torch/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_contextlib.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">35</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generator_context</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 32 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 33 │   │   │   # Issuing `None` to a generator fires it up</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 34 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ctx_factory():                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 35 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>response = gen.send(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 36 │   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 37 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 38 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1386</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">stream_generate</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1383 │   │   </span>unfinished_sequences = input_ids.new(input_ids.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]).fill_(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1384 │   │   </span>scores = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1385 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1386 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>model_inputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prepare_inputs_for_generation(input_ids, **model_kwargs)  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1387 │   │   │   # forward pass to get next token</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1388 │   │   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>(                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1389 │   │   │   │   </span>**model_inputs,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1155</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">prepare_inputs_for_generation</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1152 │   │   │   │   </span>logger.warning_once(<span style=\"color: #808000; text-decoration-color: #808000\">f\"The dtype of attention mask ({</span>attention_mask.dtype  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1153 │   │   │   │   </span>attention_mask = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1154 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> attention_mask <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1155 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>attention_mask = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.get_masks(                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1156 │   │   │   │   │   </span>input_ids,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1157 │   │   │   │   │   </span>device=input_ids.device                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1158 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">683</span> in      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_masks</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 680 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_masks</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, input_ids, device):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 681 │   │   </span>batch_size, seq_length = input_ids.shape                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 682 │   │   </span>context_lengths = [seq.tolist().index(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.bos_token_id) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> seq <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> input  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 683 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attention_mask = torch.ones((batch_size, seq_length, seq_length), device=device)  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 684 │   │   </span>attention_mask.tril_()                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 685 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i, context_length <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(context_lengths):                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 686 │   │   │   </span>attention_mask[i, :, :context_length] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">48.55</span> GiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.70</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.72</span> GiB \n",
       "already allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.76</span> GiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.79</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated memory\n",
       "try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_1195/\u001b[0m\u001b[1;33m4187612398.py\u001b[0m:\u001b[94m3\u001b[0m in \u001b[92m<cell line: 3>\u001b[0m                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_1195/4187612398.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_1195/\u001b[0m\u001b[1;33m2796800581.py\u001b[0m:\u001b[94m5\u001b[0m in \u001b[92mdisplay_answer\u001b[0m                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_1195/2796800581.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/autodl-tmp/langchain-ChatGLM/chains/\u001b[0m\u001b[1;33mlocal_doc_qa.py\u001b[0m:\u001b[94m201\u001b[0m in \u001b[92mget_knowledge_based_answer\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   │   \u001b[0mprompt = generate_prompt(related_docs, query)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m streaming:                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m201 \u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m result, history \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.llm._call(prompt=prompt,                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │     \u001b[0mhistory=chat_history):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mhistory[-\u001b[94m1\u001b[0m][\u001b[94m0\u001b[0m] = query                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m204 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mresponse = {\u001b[33m\"\u001b[0m\u001b[33mquery\u001b[0m\u001b[33m\"\u001b[0m: query,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/autodl-tmp/langchain-ChatGLM/models/\u001b[0m\u001b[1;33mchatglm_llm.py\u001b[0m:\u001b[94m78\u001b[0m in \u001b[92m_call\u001b[0m                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 75 \u001b[0m\u001b[2m│   │   │     \u001b[0mstop: Optional[List[\u001b[96mstr\u001b[0m]] = \u001b[94mNone\u001b[0m) -> \u001b[96mstr\u001b[0m:                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 76 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(prompt)                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 77 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.streaming:                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 78 \u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m inum, (stream_resp, _) \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(\u001b[96mself\u001b[0m.model.stream_chat(                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 79 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.tokenizer,                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 80 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mprompt,                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 81 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhistory=history[-\u001b[96mself\u001b[0m.history_len:-\u001b[94m1\u001b[0m] \u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.history_len > \u001b[94m0\u001b[0m \u001b[94melse\u001b[0m [   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/lib/python3.8/site-packages/torch/utils/\u001b[0m\u001b[1;33m_contextlib.py\u001b[0m:\u001b[94m35\u001b[0m in \u001b[92mgenerator_context\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 32 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 33 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Issuing `None` to a generator fires it up\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 34 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m ctx_factory():                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 35 \u001b[2m│   │   │   │   \u001b[0mresponse = gen.send(\u001b[94mNone\u001b[0m)                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 36 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 37 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 38 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m1311\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mstream_chat\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1308 \u001b[0m\u001b[2m│   │   │   \u001b[0mprompt += \u001b[33m\"\u001b[0m\u001b[33m[Round \u001b[0m\u001b[33m{}\u001b[0m\u001b[33m]\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m问：\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m答：\u001b[0m\u001b[33m\"\u001b[0m.format(\u001b[96mlen\u001b[0m(history), query)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1309 \u001b[0m\u001b[2m│   │   \u001b[0minputs = tokenizer([prompt], return_tensors=\u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m)                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1310 \u001b[0m\u001b[2m│   │   \u001b[0minputs = inputs.to(\u001b[96mself\u001b[0m.device)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1311 \u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m outputs \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.stream_generate(**inputs, **gen_kwargs):                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1312 \u001b[0m\u001b[2m│   │   │   \u001b[0moutputs = outputs.tolist()[\u001b[94m0\u001b[0m][\u001b[96mlen\u001b[0m(inputs[\u001b[33m\"\u001b[0m\u001b[33minput_ids\u001b[0m\u001b[33m\"\u001b[0m][\u001b[94m0\u001b[0m]):]                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1313 \u001b[0m\u001b[2m│   │   │   \u001b[0mresponse = tokenizer.decode(outputs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1314 \u001b[0m\u001b[2m│   │   │   \u001b[0mresponse = \u001b[96mself\u001b[0m.process_response(response)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/lib/python3.8/site-packages/torch/utils/\u001b[0m\u001b[1;33m_contextlib.py\u001b[0m:\u001b[94m35\u001b[0m in \u001b[92mgenerator_context\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 32 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 33 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Issuing `None` to a generator fires it up\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 34 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m ctx_factory():                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 35 \u001b[2m│   │   │   │   \u001b[0mresponse = gen.send(\u001b[94mNone\u001b[0m)                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 36 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 37 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 38 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m1386\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mstream_generate\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1383 \u001b[0m\u001b[2m│   │   \u001b[0munfinished_sequences = input_ids.new(input_ids.shape[\u001b[94m0\u001b[0m]).fill_(\u001b[94m1\u001b[0m)                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1384 \u001b[0m\u001b[2m│   │   \u001b[0mscores = \u001b[94mNone\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1385 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1386 \u001b[2m│   │   │   \u001b[0mmodel_inputs = \u001b[96mself\u001b[0m.prepare_inputs_for_generation(input_ids, **model_kwargs)  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1387 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# forward pass to get next token\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1388 \u001b[0m\u001b[2m│   │   │   \u001b[0moutputs = \u001b[96mself\u001b[0m(                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1389 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m**model_inputs,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m1155\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mprepare_inputs_for_generation\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1152 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogger.warning_once(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mThe dtype of attention mask (\u001b[0m\u001b[33m{\u001b[0mattention_mask.dtype  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1153 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mattention_mask = \u001b[94mNone\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1154 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m attention_mask \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1155 \u001b[2m│   │   │   │   \u001b[0mattention_mask = \u001b[96mself\u001b[0m.get_masks(                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1156 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0minput_ids,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1157 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mdevice=input_ids.device                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1158 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m683\u001b[0m in      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mget_masks\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 680 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mget_masks\u001b[0m(\u001b[96mself\u001b[0m, input_ids, device):                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 681 \u001b[0m\u001b[2m│   │   \u001b[0mbatch_size, seq_length = input_ids.shape                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 682 \u001b[0m\u001b[2m│   │   \u001b[0mcontext_lengths = [seq.tolist().index(\u001b[96mself\u001b[0m.config.bos_token_id) \u001b[94mfor\u001b[0m seq \u001b[95min\u001b[0m input  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 683 \u001b[2m│   │   \u001b[0mattention_mask = torch.ones((batch_size, seq_length, seq_length), device=device)  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 684 \u001b[0m\u001b[2m│   │   \u001b[0mattention_mask.tril_()                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 685 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m i, context_length \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(context_lengths):                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 686 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask[i, :, :context_length] = \u001b[94m1\u001b[0m                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m48.55\u001b[0m GiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m23.70\u001b[0m GiB total capacity; \u001b[1;36m12.72\u001b[0m GiB \n",
       "already allocated; \u001b[1;36m9.76\u001b[0m GiB free; \u001b[1;36m12.79\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory\n",
       "try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可以额外返回参照的知识库源信息\n",
    "history = []\n",
    "resp, history = display_answer(local_doc_qa, query=\"什么是 DenseNet 模型\", vs_path=vs_path, history=history)\n",
    "# print(resp)\n",
    "# resp[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "id": "AD5D4EF28CFE42E1857F4EB65B935B61",
    "jupyter": {
     "outputs_hidden": true
    },
    "notebookId": "644b7c4749a32aea7f079534",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于以下已知信息，简洁和专业的来回答用户的问题，问题是\"它与ResNet的关键区别是什么？\"。如果无法从中得到答案，请说 \"根据已知信息无法回答该问题\" 或 \"没有提供足够的相关信息\"，不允许在答案中添加编造成分，答案请使用中文。已知内容如下: \n",
      "0 2 0 2  p e S 8 2  ]  G L . s c [  2 v 1 4 0 3 0 . 6 0 0 2 : v i X r a  Sample Complexity of Asynchronous Q-Learning: Sharper Analysis and Variance Reduction  Gen Li∗ Tsinghua  Yuting Wei† CMU  Yuejie Chi‡ CMU  Yuantao Gu∗ Tsinghua  Yuxin Chen§ Princeton  June, 2020; Revised: September 2020  Abstract  Asynchronous Q-learning aims to learn the optimal action-value function (or Q-function) of a Markov decision process (MDP), based on a single trajectory of Markovian samples induced by a behavior policy. Focusing on a γ-discounted MDP with state space S and action space A, we demonstrate that the ℓ∞- based sample complexity of classical asynchronous Q-learning — namely, the number of samples needed to yield an entrywise ε-accurate estimate of the Q-function — is at most on the order of  1 µmin(1 − γ)5ε2 +  tmix µmin(1 − γ)  up to some logarithmic factor, provided that a proper constant learning rate is adopted. Here, tmix and µmin denote respectively the mixing time and the minimum state-action occupancy probability of the sample trajectory. The ﬁrst term of this bound matches the complexity in the synchronous case with independent samples drawn from the stationary distribution of the trajectory. The second term reﬂects the cost taken for the empirical distribution of the Markovian trajectory to reach a steady state, which is incurred at the very beginning and becomes amortized as the algorithm runs. Encouragingly, the above bound improves upon the state-of-the-art result Qu and Wierman (2020) by a factor of at least |S||A|. Further, the scaling on the discount complexity can be improved by means of variance reduction.  Keywords: model-free reinforcement learning, asynchronous Q-learning, Markovian samples, variance re- duction, TD learning, mixing time  Contents  1 Introduction  2 Models and background  3 Asynchronous Q-learning on a single trajectory  3.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Theoretical guarantees for asynchronous Q-learning . . . . . . . . . . . . . . . . . . . . . . . . 3.3 A special case: TD learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Adaptive and implementable learning rates  4 Extension: asynchronous variance-reduced Q-learning  4.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Theoretical guarantees for variance-reduced Q-learning . . . . . . . . . . . . . . . . . . . . . .  ∗Department of Electronic Engineering, Tsinghua University, Beijing 100084, China. †Department of Statistics and Data Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA. ‡Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA. §Department of Electrical Engineering, Princeton University, Princeton, NJ 08544, USA.  1  2  4  6 6 6 8 9  10 10 11  5 Related work  6 Analysis of asynchronous Q-learning  6.1 Error decay under constant learning rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Proof of Theorem 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.1 Key decomposition and a recursive formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.2 Recursive analysis 6.3 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  7 Discussion  A Preliminaries on Markov chains  A.1 Concentration of empirical distributions of Markov chains . . . . . . . . . . . . . . . . . . . . A.2 Connection between the mixing time and the cover time . . . . . . . . . . . . . . . . . . . . .  B Cover-time-based analysis of asynchronous Q-learning  C Analysis under adaptive learning rates (proof of Theorem 3)  D Analysis of asynchronous variance-reduced Q-learning  D.1 Per-epoch analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . γ . . . . . . . . . . . . . . . . . . . . . . . . . . k∞ γ . . . . . . . . . . . . . . . . . . . . . . . . . . k∞ ≤ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  Q⋆ Q⋆ D.2 How many epochs are needed?  D.1.1 Phase 1: when D.1.2 Phase 2: when  > 1/√1 1/√1  Q Q  − −  − −  k k  E Proofs of technical lemmas  E.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Proof of Lemma 2 and Lemma 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Proof of Lemma 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.5 Proof of Lemma 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.6 Proof of Lemma 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  12  13 13 14 14 15 16  17  18 18 19  21  22  23 24 24 25 27  27 27 29 30 30 32 33  1 Introduction  Model-free algorithms such as Q-learning (Watkins and Dayan, 1992) play a central role in recent break- throughs of reinforcement learning (RL) (Mnih et al., 2015). In contrast to model-based algorithms that decouple model estimation and planning, model-free algorithms attempt to directly interact with the envi- ronment — in the form of a policy that selects actions based on perceived states of the environment — from the collected data samples, without modeling the environment explicitly. Therefore, model-free algorithms are able to process data in an online fashion and are often memory-eﬃcient. Understanding and improving the sample eﬃciency of model-free algorithms lie at the core of recent research activity (Dulac-Arnold et al., 2019), whose importance is particularly evident for the class of RL applications in which data collection is costly and time-consuming (such as clinical trials, online advertisements, and so on).  The current paper concentrates on Q-learning — an oﬀ-policy model-free algorithm that seeks to learn the optimal action-value function by observing what happens under a behavior policy. The oﬀ-policy feature makes it appealing in various RL applications where it is infeasible to change the policy under evaluation on the ﬂy. There are two basic update models in Q-learning. The ﬁrst one is termed a synchronous setting, which hypothesizes on the existence of a simulator (or a generative model); at each time, the simulator generates an independent sample for every state-action pair, and the estimates are updated simultaneously across all state-action pairs. The second model concerns an asynchronous setting, where only a single sample trajectory following a behavior policy is accessible; at each time, the algorithm updates its estimate of a single state-action pair using one state transition from the trajectory. Obviously, understanding the asynchronous  2  Paper  Sample complexity  Learning rate  Even-Dar and Mansour (2003)  Even-Dar and Mansour (2003)  Beck and Srikant (2012)  Qu and Wierman (2020)  (tcover) (1  1 1−γ γ)4ε2  − 1  ω +  1 1−ω  tcover 1 γ  −  t1+3ω cover  γ)4ε2  (1  −  (cid:0)  linear: 1 t  polynomial: 1  tω , ω  ( 1 2 , 1)  t3 (cid:1) (cid:0) cover|S||A| γ)5ε2 (1 −  (cid:1)  constant:  rescaled linear:  This work (Theorem 1)  This work (Theorem 2)  1  µmin(1  −  −  γ)  constant: min  (cid:8) constant: min  ∈ γ)4ε2 t2 cover  (1  − |S||A|  1  1 µmin (1−γ) µmin (1−γ) ,tmix { γ)4ε2 , 1 γ2 tmix  }  t+max  (1  −  (1  γ)4ε2 γ2  −  , 1  (cid:9)  tmix  µ2  −  γ)5ε2  min(1 γ)5ε2 + tmix  µmin(1  tcover  γ)5ε2  (1  −  Table 1: Sample complexity of asynchronous Q-learning to compute an ε-optimal Q-function in the ℓ norm, where we hide all logarithmic factors. With regards to the Markovian trajectory induced by the behavior policy, we denote by tcover, tmix, and µmin the cover time, mixing time, and minimum state-action occupancy probability of the associated stationary distribution, respectively.  ∞  (cid:8)  (cid:9)  setting is considerably more challenging than the synchronous model, due to the Markovian (and hence non-i.i.d.) nature of its sampling process.  Focusing on an inﬁnite-horizon Markov decision process (MDP) with state space  , A this work investigates asynchronous Q-learning on a single Markovian trajectory. We ask a fundamental question:  and action space  S  How many samples are needed for asynchronous Q-learning to learn the optimal Q-function?  Despite a considerable amount of prior work exploring this algorithm (ranging from the classical work Jaakkola et al. (1994); Tsitsiklis (1994) to the very recent paper Qu and Wierman (2020)), it remains un- clear whether existing sample complexity analysis of asynchronous Q-learning is tight. As we shall elucidate — between the state-of-the-art sample momentarily, there exists a large gap — at least as large as complexity bound for asynchronous Q-learning Qu and Wierman (2020) and the one derived for the syn- chronous counterpart Wainwright (2019a). This raises a natural desire to examine whether there is any bottleneck intrinsic to the asynchronous setting that signiﬁcantly limits its performance.  |S||A|  Our contributions. This paper develops a reﬁned analysis framework that sharpens our understanding about the sample eﬃciency of classical asynchronous Q-learning on a single sample trajectory. Setting the stage, consider an inﬁnite-horizon MDP with state space (0, 1). What we have access to is a sample trajectory of the MDP induced by a stationary behavior policy. In contrast to the synchronous setting with i.i.d. samples, we single out two parameters intrinsic to the Markovian sample trajectory: (i) the mixing time tmix, which characterizes how fast the trajectory disentangle itself from the initial state; (ii) the smallest state-action occupancy probability µmin of the stationary distribution of the trajectory, which captures how frequent each state-action pair has been at least visited.  , and a discount factor γ  , action space  A  ∈  S  With these parameters in place, our ﬁndings unveil that: the sample complexity required for asynchronous  Q-learning to yield an ε-optimal Q-function estimate — in a strong ℓ  sense — is at most1  1  −  µmin(1  (cid:16)  γ)5ε2 +  tmix µmin(1  −  γ)  O  e  ∞  .  (cid:17)  (1)  The ﬁrst component of (1) is consistent with the sample complexity derived for the setting with independent samples drawn from the stationary distribution of the trajectory (Wainwright, 2019a). In comparison, the second term of (1) — which is unaﬀected by the accuracy level ε — is intrinsic to the Markovian nature  1Let X := (cid:0)|S|, |A|, f ≤ C1g. The notation  1  ε (cid:1). The notation f (X ) = O(g(X )) means there exists a universal constant C1 > 0 such that  1−γ , 1 O(·) is deﬁned analogously except that it hides any logarithmic factor. e  3  of the trajectory; in essence, this term reﬂects the cost taken for the empirical distribution of the sample trajectory to converge to a steady state, and becomes amortized as the algorithm runs. In other words, the behavior of asynchronous Q-learning would resemble what happens in the setting with independent samples, as long as the algorithm has been run for reasonably long. In addition, our analysis framework readily yields another sample complexity bound  O  tcover  (1  γ)5ε2  ,  (2)  − where tcover stands for the cover time — namely, the time taken for the trajectory to visit all state-action pairs at least once. This facilitates comparisons with several prior results based on the cover time.  (cid:16)  (cid:17)  e  Furthermore, we leverage the idea of variance reduction to improve the scaling with the discount com- γ . We demonstrate that a variance-reduced variant of asynchronous Q-learning attains ε-accuracy  plexity 1 1 − using at most  O  µmin(1  1 γ)3 min {  1, ε2  +  tmix µmin(1  γ)  − (cid:16) (Wainwright, samples, matching the complexity of its synchronous counterpart if ε 2019b). Moreover, by taking the action space to be a singleton set, the aforementioned results immediately -based sample complexity guarantees for temporal diﬀerence (TD) learning (Sutton, 1988) on lead to ℓ ∞ Markovian samples.  γ)√tmix  min  1,  −  ≤  (cid:17)  (cid:8)  (cid:9)  e  }  (1  −  1  (3)  Comparisons with past work. A large fraction of the classical literature focused on asymptotic conver- gence analysis of asynchronous Q-learning (e.g. Jaakkola et al. (1994); Szepesvári (1998); Tsitsiklis (1994)); these results, however, did not lead to non-asymptotic sample complexity bounds. The state-of-the-art sample complexity analysis was due to the recent work Qu and Wierman (2020), which derived a sample , our result (1) improves complexity bound . Given the obvious lower bound 1/µmin  tmix  O  (cid:0)  e  In upon that of Qu and Wierman (2020) by a factor at least on the order of addition, we note that several prior work Beck and Srikant (2012); Even-Dar and Mansour (2003) developed sample complexity bounds in terms of the cover time tcover of the sample trajectory; our result strengthens these bounds by a factor of at least t2 3. The interested reader is referred to Table 1 for 3 more precise comparisons, and to Section 5 for discussions of further related work.  cover|S||A| ≥ |S|  |S||A|  tmix,  min  |A|  (cid:9)  (cid:8)  (1  (cid:1)  −  .  1 γ)4ε2  µ2  min(1  γ)5ε2  −  ≥ |S||A|  ≤  S  A  n and  )) the probability simplex over the set  n ∈ i ≤ ≤ w (resp. z ≤  Notation. Denote by ∆( ) (resp. ∆( Rn, we overload the notation √ z = [zi]1 i · ≤ √z := [√zi]1 z ]1 zi| | w) means zi ≥ z wi (resp. zi ≤ all-one vector, I the identity matrix, and 1 P . Throughout this paper, we use c, c0, c1, Pij |  ). For any vector (resp. to denote entry-wise operations, such that n and w = [wi]1 n, the notation i i ≤ ≤ ≤ n. Additionally, we denote by 1 the ≥ the indicator function. For any matrix P = [Pij ], we denote to denote universal constants that do not k1 := maxi k depend either on the parameters of the MDP or the target levels (ε, δ), and their exact values may change P from line to line.  n. For any vectors z = [ai]1 i  wi) for all 1  := [  and  ≤ ≤  | · |  {·}  · ·  i ≤  j |  A  ≤  S  ≤  |  |  2 Models and background  ,  A  A  and  , P, r, γ). Here,  denote respectively the (ﬁnite) state space and action space, whereas γ  This paper studies an inﬁnite-horizon MDP with discounted rewards, as represented by a quintuple = M (0, 1) ( S S ) to represent the probability transition kernel of indicates the discount factor. We use P : s, a) denotes the probability of transiting , P (s′ the MDP, where for each state-action pair (s, a) to state s′ from state s when action a is executed. The reward function is represented by r : [0, 1], such that r(s, a) denotes the immediate reward from state s when action a is taken; for simplicity, we assume throughout that all rewards lie within [0, 1]. We focus on the tabular setting which, despite its basic form, is not yet well understood. See Bertsekas (2017) for an in-depth introduction of this model.  S ∈ S × A  S × A →  S × A →  ∆(  ∈  |  4  ∆(  S →  Q-function and the Bellman operator. An action selection rule is termed a policy and represented by ), which maps a state to a distribution over the set of actions. A policy is said to be a mapping π : A stationary if it is time-invariant. We denote by ∞t=0 a sample trajectory, where st (resp. at) denotes st, at, rt} the state (resp. the action taken), and rt = r(st, at) denotes the reward received at time t. It is assumed throughout that the rewards are deterministic and depend solely upon the current state-action pair. We denote by V π :  R the value function of a policy π, namely,  {  S →  s ∀  ∈ S  :  V π(s) := E  ∞  \"  t=0 X  γtr(st, at)  s0 = s  ,  #  (cid:12) (cid:12)  which is the expected discounted cumulative reward received when (i) the initial state is s0 = s, (ii) the actions are taken based on the policy π (namely, at ∼ 0) and the trajectory is generated based on the transition kernel (namely, st+1 ∼ for any π. The action-value function (also Q-function) Qπ :  ≥ st, at)). It can be easily veriﬁed that 0 R of a policy π is deﬁned by  π(st) for all t  V π(s)  P (  ≤  ≤  |  −  γ  1  1  S × A →  (s, a)  ∀  ∈ S × A  :  Qπ(s, a) := E  γtr(st, at)  s0 = s, a0 = a  ,  ∞  # 1). where the actions are taken according to the policy π except the initial action (i.e. at ∼ As is well-known, there exists an optimal policy — denoted by π⋆ — that simultaneously maximizes V π(s) and Qπ(s, a) uniformly over all state-action pairs (s, a) ). Here and throughout, we shall denote by V ⋆ := V π⋆  ( S × A the optimal value function and the optimal Q-function, respectively.  and Q⋆ := Qπ⋆  π(st) for all t  t=0 X  ≥  ∈  \"  (cid:12) (cid:12)  , which is a mapping from R|S|×|A| to itself, is deﬁned such that the  In addition, the Bellman operator  (s, a)-th entry of  (Q) is given by  T  T  It is well known that the optimal Q-function Q⋆ is the unique ﬁxed point of the Bellman operator.  (Q)(s, a) := r(s, a) + γE s′  T  P (  |  ∼  s,a)  max a′ ∈A  h  Q(s′, a′)  .  i  Sample trajectory and behavior policy. ∞t=0 under a given stationary policy πb — called a behavior policy. The behavior policy generated by the MDP is deployed to help one learn the “behavior” of the MDP under consideration, which often diﬀers from the optimal policy being sought. Given the stationarity of πb, the sample trajectory can be viewed as a sample path of a time-homogeneous Markov chain over all state-action pairs. Throughout this paper, we impose the following assumption regarding uniform ergodicity (Paulin, 2015).  Imagine we have access to a sample trajectory  st, at, rt}  M  {  Assumption 1. The Markov chain induced by the stationary behavior policy πb is uniformly ergodic.  There are several properties concerning the behavior policy and its resulting Markov chain that play a crucial role in learning the optimal Q-function. Speciﬁcally, denote by µπb the stationary distribution (over all state-action pairs) of the aforementioned behavior Markov chain, and deﬁne  ∈S×A Intuitively, µmin reﬂects an information bottleneck — the smaller µmin is, the more samples are needed in order to ensure all state-action pairs are visited suﬃciently many times. In addition, we deﬁne the associated mixing time of the chain as  µmin := min  µπb(s, a).  (4)  (s,a)  tmix := min  t  n  (cid:12) (cid:12) (cid:12)  max  (s0,a0)  ∈S×A  dTV  P t( ·|  s0, a0), µπb  1 4  ≤  (cid:0)  (cid:1)  o  ,  (5)  where P t( s0, a0) denotes the distribution of (st, at) conditional on the initial state-action pair (s0, a0), and ·| dTV(µ, ν) stands for the total variation distance between two distributions µ and ν (Paulin, 2015). In words, the mixing time tmix captures how fast the sample trajectory decorrelates from its initial state. Moreover, we deﬁne the cover time associated with this Markov chain as follows  tcover := min  t  n  |  min  (s0,a0)  ∈S×A  5  s0, a0  P  Bt| (cid:0)  1 2  ,  o  ≥  (cid:1)  (6)  Bt| (cid:0)  (cid:1)  where time t, and P  Bt denotes the event such that all (s, a)  denotes the probability of  s0, a0  ∈ S × A  have been visited at least once between time 0 and  Bt conditional on the initial state (s0, a0).  ∞t=0 generated by the behavior policy πb, we aim to Goal. Given a single sample trajectory compute/approximate the optimal Q-function Q⋆ in an ℓ sense. This setting — in which a state-action pair can be updated only when the Markovian trajectory reaches it — is commonly referred to as asynchronous Q- learning (Qu and Wierman, 2020; Tsitsiklis, 1994) in tabular RL. The current paper focuses on characterizing, in a non-asymptotic manner, the sample eﬃciency of classical Q-learning and its variance-reduced variant.  st, at, rt}  ∞  {  3 Asynchronous Q-learning on a single trajectory  3.1 Algorithm  The Q-learning algorithm (Watkins and Dayan, 1992) is arguably one of the most famous oﬀ-policy algo- rithms aimed at learning the optimal Q-function. Given the Markovian trajectory ∞t=0 gener- ated by the behavior policy πb, the asynchronous Q-learning algorithm maintains a Q-function estimate Qt :  R at each time t and adopts the following iterative update rule  st, at, rt}  {  S × A →  Qt(st  1, at 1) = (1 − Qt(s, a) = Qt  −  ηt)Qt − 1(s, a),  1(st  −  1, at − (s, a)  1) + ηtTt(Qt 1, at = (st  1)(st − 1)  ∀  −  −  −  −  1, at  −  1)  (7)  −  for any t operator w.r.t. the t-th sample, that is,  ≥  0, whereas ηt denotes the learning rate or the step size. Here  Tt denotes the empirical Bellman  Tt(Q)(st  −  1, at  −  1) := r(st  −  1, at  −  1) + γ max ∈A  a′  Q(st, a′).  (8)  It is worth emphasizing that at each time t, only a single entry — the one corresponding to the sampled state-action pair (st 1) — is updated, with all remaining entries unaltered. While the estimate Q0 can be initialized to arbitrary values, we shall set Q0(s, a) = 0 for all (s, a) unless otherwise noted. The corresponding value function estimate Vt :  R at time t is thus given by  1, at  −  −  S → :  s ∀  ∈ S  Vt(s) := max ∈A  a  Qt(s, a).  (9)  The complete algorithm is described in Algorithm 1.  Algorithm 1: Asynchronous Q-learning ηt}  1 input parameters: learning rates 2 initialization: Q0 = 0. 3 for t = 1, 2, 4  · ·  {  Draw action at Update Qt according to (7).  πb(st  −  , T do 1 ∼  −  5  , number of iterations T .  1) and next state st ∼  P (  |  st  −  1, at  −  1).  3.2 Theoretical guarantees for asynchronous Q-learning  We are in a position to present our main theory regarding the non-asymptotic sample complexity of asyn- chronous Q-learning, for which the key parameters µmin and tmix deﬁned respectively in (4) and (5) play a vital role. The proof of this result is provided in Section 6.  Theorem 1 (Asynchronous Q-learning). For the asynchronous Q-learning algorithm detailed in Algorithm 1, there exist some universal constants c0, c1 > 0 such that for any 0 < δ < 1 and 0 < ε  1  γ , one has  ≤  1  −  (s, a)  ∀  ∈ S × A  :  QT (s, a)  |  −  Q⋆(s, a)  | ≤  ε  6  6  with probability at least 1  −  δ, provided that the iteration number T and the learning rates ηt ≡ c0 µmin (cid:26) c1  1 γ)2ε  |S||A| δ  tmix 1 γ)5ε2 + γ 1 (cid:27) γ)4ε2 (1 γ2  − −  min  log  log  (1  (1  −  −  (cid:16)  (cid:17)  (cid:16)  (cid:17)  T  ,  ,  .  T  ≥  η =  η obey  (10a)  (10b)  log  T  |S||A| δ  (cid:26)  1 tmix (cid:27)  Theorem 1 delivers a ﬁnite-sample/ﬁnite-time analysis of asynchronous Q-learning, given that a ﬁxed -based sample complexity required for Algorithm 1  learning rate is adopted and chosen appropriately. The ℓ to attain ε accuracy is at most  ∞  (cid:0)  (cid:1)  A few implications are in order.  µmin(1  1  −  γ)5ε2 +  tmix µmin(1  −  γ)  .  (cid:17)  O  (cid:16)  e  (11)  Dependency on the minimum state-action occupancy probability µmin. Our sample complexity bound (11) scales linearly in 1/µmin, which is in general unimprovable. Consider, for instance, the ideal scenario where state-action occupancy is nearly uniform across all state-action pairs, in which case 1/µmin is on the order of , and this dependency matches the known minimax lower bound Azar et al. (2013) derived for the setting with independent samples. In comparison, Qu and Wierman (2020, Theorem 7) depends at least quadratically on 1/µmin, which is at least  . In such a “near-uniform” case, the sample complexity scales linearly with  times larger than our result (11).  |S||A|  |S||A|  |S||A|  1 Dependency on the discount complexity 1 γ)5ε2 , which co- 1 − incides with both Chen et al. (2020); Wainwright (2019a) (for the synchronous setting) and Beck and Srikant (2012); Qu and Wierman (2020) (for the asynchronous setting) with either a rescaled linear learning rate or a constant learning rate. This turns out to be the sharpest scaling known to date for the classical form of Q-learning.  γ . The sample size bound (11) scales as  (1  −  Dependency on the mixing time tmix. The second additive term of our sample complexity (11) depends linearly on the mixing time tmix and is (almost) independent of the target accuracy ε. The inﬂuence of this mixing term is a consequence of the expense taken for the Markovian trajectory to reach a steady state, which is a one-time cost that can be amortized over later iterations if the algorithm is run for reasonably long. Put 1 γ)4ε2 ), another way, if the behavior chain mixes not too slowly with respect to ε (in the sense that tmix then the algorithm behaves as if the samples were independently drawn from the stationary distribution of 1 γ)5ε2 in Qu and Wierman (2020) (cf. Table 1) are the trajectory. In comparison, the inﬂuences of tmix and multiplicative regardless of the value of ε, thus resulting in a much higher sample complexity. For instance, if ε = O times larger than our result (modulo some log factor).  , then the sample complexity result therein is at least tmix  µmin ≥  |S||A|  γ)2√tmix  tmix  ≤  (1  (1  (1  −  −  −  1  (cid:0)  (cid:1)  ∞  Schedule of learning rates. An interesting aspect of our analysis lies in the adoption of a time-invariant error decays linearly — down to some error ﬂoor whose value is dic- learning rate, under which the ℓ tated by the learning rate. Therefore, a desired statistical accuracy can be achieved by properly setting the learning rate based on the target accuracy level ε and then determining the sample complexity accord- ingly. In comparison, classical analyses typically adopted a (rescaled) linear or a polynomial learning rule Even-Dar and Mansour (2003); Qu and Wierman (2020). While the work Beck and Srikant (2012) studied Q-learning with a constant learning rate, their bounds were conservative and fell short of revealing the op- timal scaling. Furthermore, we note that adopting time-invariant learning rates is not the only option that enables the advertised sample complexity; as we shall elucidate in Section 3.4, one can also adopt carefully designed diminishing learning rates to achieve the same performance guarantees.  In addition, our analysis framework immediately leads to another sample complexity guarantee stated in terms of the cover time tcover (cf. (6)), which facilitates comparisons with several past work Beck and Srikant  7  with probability at least 1  −  (2012); Even-Dar and Mansour (2003). The proof follows essentially that of Theorem 1, with a sketch provided in Section B.  Theorem 2. For the asynchronous Q-learning algorithm detailed in Algorithm 1, there exist some universal constants c0, c1 > 0 such that for any 0 < δ < 1 and 0 < ε  1  1  −  ≤ QT (s, a)  γ , one has Q⋆(s, a)  (s, a)  :  ∀  ∈ S × A  − δ, provided that the iteration number T and the learning rates ηt ≡ log T  c0tcover  | ≤  T  |  ,  ≥  (1  −  η =  γ)5ε2 log2 c1  (cid:16) min  |S||A| δ (1  log  T  |S||A| δ  (1  − .  (cid:17) γ)4ε2 γ2  −  (cid:16) , 1  ε  (cid:17)  1 γ)2ε  η obey  (12a)  (12b)  (cid:26) -based sample complexity of classical asynchronous Q-  (cid:27)  In a nutshell, this theorem tells us that the ℓ  (cid:1)  (cid:0)  learning is bounded above by  O  ∞  tcover  (1  γ)5ε2  ,  (13)  (cid:16)  − which scales linearly with the cover time. This improves upon the prior result Even-Dar and Mansour (2003) 3). (resp. Beck and Srikant (2012)) by an order of at least t3.29 See Table 1 for detailed comparisons. We shall further make note of some connections between tcover and O(tmix/µmin) for uniformly tmix/µmin to help compare Theorem 1 and Theorem 2: (1) in general, tcover = ergodic chains; (2) one can ﬁnd some cases where tmix/µmin = O(tcover). Consequently, while Theorem 1 does not strictly dominate Theorem 2 in all instances, the aforementioned connections reveal that Theorem 1 is tighter for the worst-case scenarios. The interested reader is referred to Section A.2 for details.  cover|S||A| ≥ |S|  3.29 (resp. t2  cover ≥ |S|  |A|  |A|  3.29  (cid:17)  e  e  3  e  3.3 A special case: TD learning  In the special circumstance that the set of allowable actions to a Markov reward process (MRP), where the state transition kernel P : of transitioning between diﬀerent states, and r : immediate reward in state s). The goal is to estimate the value function V : st, rt} maintains an estimate Vt :  is a singleton, the corresponding MDP reduces ) describes the probability S [0, 1] denotes the reward function (so that r(s) is the R from the trajectory  The Q-learning procedure in this special setting reduces to the well-known TD learning algorithm, which R at each time t and proceeds according to the following iterative update2  ∞t=0, which arises commonly in the task of policy evaluation for a given deterministic policy.  S →  S →  S →  ∆(  A  {  S → Vt(st  1) = (1 Vt(s) = Vt  −  ηt)Vt  − 1(s),  −  −  1) + ηt (r(st 1(st − 1. = st s ∀  −  −  1) + γVt  −  1(st)) ,  (14)  As usual, ηt denotes the learning rate at time t, and V0 is taken to be 0. Consequently, our analysis for asynchronous Q-learning with a Markovian trajectory immediately leads to non-asymptotic ℓ guarantees for TD learning, stated below as a corollary of Theorem 1. A similar result can be stated in terms of the cover time as a corollary to Theorem 2, which we omit for brevity.  ∞  Corollary 1 (Asynchronous TD learning). Consider the TD learning algorithm (14). There exist some universal constants c0, c1 > 0 such that for any 0 < δ < 1 and 0 < ε  1  γ , one has  s ∀  ∈ S  :  VT (s)  |  −  V (s)  | ≤  1  −  ≤ ε  with probability at least 1  T  −  ≥  η =  δ, provided that the iteration number T and the learning rates ηt ≡ c0 |S| δ µmin (cid:26) c1  1 γ)5ε2 + (1  1 γ)2ε  log  log  (1  (1  −  (cid:17)  (cid:16)  (cid:17)  T  ,  tmix γ 1 (cid:27) − γ)4ε2 γ2  ,  −  (cid:16) 1 tmix (cid:27)  .  − min  (cid:26)  log  T  |S| δ  (cid:0)  (cid:1)  η obey  (15a)  (15b)  2When A = {a} is a singleton, the Q-learning update rule (7) reduces to the TD update rule (14) by relating Q(s, a) = V (s).  8  6  The above result reveals that the ℓ  ∞  sample complexity for TD learning is at most  1  −  µmin(1  (cid:16)  γ)5ε2 +  tmix µmin(1  −  γ)  ,  (cid:17)  (16)  O  e  provided that an appropriate constant learning rate is adopted. We note that prior ﬁnite-sample analysis on asynchronous TD learning typically focused on (weighted) ℓ2 estimation errors with linear function approx- imation (Bhandari et al., 2018; Srikant and Ying, 2019), and it is hence diﬃcult to make fair comparisons. The recent papers Khamaru et al. (2020); Mou et al. (2020) develop ℓ guarantees for TD learning, with their focus on the synchronous settings with i.i.d. samples rather than Markovian samples.  ∞  3.4 Adaptive and implementable learning rates  The careful reader might already notice that the learning rates recommended in (10b) depend on the mixing time tmix — a parameter that might be either a priori unknown or diﬃcult to estimate. Fortunately, it is feasible to adopt a more adaptive learning rate schedule which does not rely on prior knowledge of tmix and which is still capable of achieving the performance advertised in Theorem 1.  Learning rates. quantities for all (s, a)  : ∈ S × A  In order to describe our new learning rate schedule, we need to keep track of the following  Kt(s, a): the number of times that the sample trajectory visits (s, a) during the ﬁrst t iterations. In addition, we maintain an estimate  µmin,t of µmin, computed recursively as follows  ,  1 b |S||A| 1, µmin,t mins,a Kt(s, a)/t, b  −  µmin,t =    b    mins,a Kt(s, a) = 0; 2 < mins,a Kt(s,a)/t 1 otherwise.  bµmin,t−1  < 2;  With the above quantities in place, we propose the following learning rate schedule:  ηt = min  1, cη exp  log  n  (cid:16)j  log t  µmin,t(1  γ)γ2t  −  ,  k(cid:17)o  (17)  (18)  If  denotes the nearest integer less than or equal where cη > 0 is some suﬃciently large constant, and x b ⌋ ⌊ µmin,t forms a reliable estimate of µmin, then one can view (18) as a sort of “piecewise constant to x. approximation” of the rescaled linear stepsizes γ)γ2t . Clearly, such learning rates are fully data-driven and do no rely on any prior knowledge about the Markov chain (like tmix and µmin) or the target accuracy ε.  cη log t  µmin(1  b  −  Performance guarantees. Encouragingly, our theoretical framework can be extended without diﬃculty to accommodate this adaptive learning rate choice. Speciﬁcally, for the Q-function estimates  Qt =  Qt, Qt  −  1,  (cid:26)  = ηt,  if ηt+1 6 otherwise,  (19)  where Qt is provided by the Q-learning steps (c.f. (7)). We then are ensured of the following theoretical guarantees whose proof is deferred to Appendix C.  b  b  Theorem 3. Consider asynchronous Q-learning with learning rates (18). There exists some universal constant C > 0 such that: for any 0 < δ < 1 and 0 < ε  1  γ , one has  ≤  1  −  (s, a)  ∀  ∈ S × A  :  QT (s, a)  Q⋆(s, a)  −  ε  ≤  with probability at least 1  δ, provided that  −  (cid:12) (cid:12) b  C max  T  ≥  1  −  γ)5ε2 ,  tmix µmin(1  −  γ)  log  o  (cid:16)  |S||A| δ  µmin(1  n  9  (cid:12) (cid:12)  T  (cid:17)  log  T γ)2ε  −  .  (cid:17)  (1  (cid:16)  (20)  (21)  4 Extension: asynchronous variance-reduced Q-learning  1  1  As pointed out in prior literature, the classical form of Q-learning (7) often suﬀers from sub-optimal de- γ . For instance, in the synchronous setting, the minimax lower pendence on the discount complexity 1 γ)3 (see, Azar et al. (2013)), while the sharpest known upper bound for vanilla bound is proportional to (1 1 Q-learning scales as γ)5 ; see detailed discussions in Wainwright (2019a). To remedy this issue, recent work proposed to leverage the idea of variance reduction to develop accelerated RL algorithms in the syn- chronous setting (Sidford et al., 2018a; Wainwright, 2019b), as inspired by the seminal SVRG algorithm (Johnson and Zhang, 2013) that originates from the stochastic optimization literature. In this section, we adapt this idea to asynchronous Q-learning and characterize its sample eﬃciency.  (1  −  −  −  4.1 Algorithm  In order to accelerate the convergence, it is instrumental to reduce the variability of the empirical Bellman operator Tt employed in the update rule (7) of classical Q-learning. This can be achieved via the following means. Simply put, assuming we have access to (i) a reference Q-function estimate, denoted by Q, and (ii) an estimate of  (Q), the variance-reduced Q-learning update rule is given by  (Q), denoted by  T  T  Qt(st  1, at  1) = (1  −  − Qt(s, a) = Qt  e ηt)Qt  − 1(s, a),  −  −  1(st  −  1, at  − (s, a)  ∀  1) + ηt  = (st  1)  Tt(Qt − (cid:16) 1), 1, at −  −  − Tt(Q) +  T  e  (Q)  (st  −  1, at  −  1),  (cid:17)  (22)  where Tt denotes the empirical Bellman operator at time t (cf. (8)). The empirical estimate puted using a set of samples; more speciﬁcally, by drawing N consecutive sample transitions from the observed trajectory, we compute  (Q) can be com- T (si, ai, si+1) { e  }0 ≤  i<N  (Q)(s, a) = r(s, a) +  T  γ  1  N − i=0  1  {  P  (si, ai) = (s, a) } N (si, ai) = (s, a) − i=0 }  1 {  1  maxa′ Q(si+1, a′)  .  (23)  e  Compared with the classical form (7), the original update term Tt(Q) + chosen properly.  − (Q), in the hope of achieving reduced variance as long as Q (which serves as a proxy to Q⋆) is  1) has been replaced by  Tt(Qt  Tt(Qt  1)  T  −  −  P  For convenience of presentation, we introduce the following notation  e  Q = Vr-q-run-epoch( Q, N, tepoch )  (24)  to represent the above-mentioned update rule, which starts with a reference point Q and operates upon a total number of N + tepoch consecutive sample transitions. The ﬁrst N samples are employed to construct (Q) via (23), with the remaining samples employed in tepoch iterative updates (22); see Algorithm 3. To T achieve the desired acceleration, the proxy Q needs to be periodically updated so as to better approximate the truth Q⋆ and hence reduce the bias. It is thus natural to run the algorithm in a multi-epoch manner. e Speciﬁcally, we divide the samples into contiguous subsets called epochs, each containing tepoch iterations and using N + tepoch samples. We then proceed as follows  Qepoch  m = Vr-q-run-epoch( Qepoch  m  1 , N, tepoch ), m = 1, . . . , M,  −  (25)  where M is the total number of epochs, and Qepoch denotes the output of the m-th epoch. The whole procedure is summarized in Algorithm 2. Clearly, the total number of samples used in this algorithm is given by M (N + tepoch). We remark that the idea of performing variance reduction in RL is certainly not new, and has been explored in a number of recent work Du et al. (2017); Khamaru et al. (2020); Sidford et al. (2018a,b); Wainwright (2019b); Xu et al. (2020).  m  10  6  Algorithm 2: Asynchronous variance-reduced Q-learning  1 input parameters: number of epochs M , epoch length tepoch, recentering length N , learning rate η. 2 initialization: set Qepoch 3 for each epoch m = 1,  0. , M do  0 ← · · ·  /  /* Call Algorithm 3. Qepoch  m = Vr-q-run-epoch( Qepoch  m  4  1 , N, tepoch) .  −  4.2 Theoretical guarantees for variance-reduced Q-learning  This subsection develops a non-asymptotic sample complexity bound for asynchronous variance-reduced Q- learning on a single trajectory. Before presenting our theoretical guarantees, there are several algorithmic parameters that we shall specify; for given target levels (ε, δ), choose  ηt ≡  η =  N  ≥  tepoch  ≥  log c1 (cid:0) µmin c2 µmin  c0  tepoch  |S||A| δ  min  (cid:26)  (1  γ)2  − γ2  ,  1 (cid:1) 1, ε2 γ)3 min { tmix γ 1  γ)3 +  −  (1  (cid:16)  − 1  (1  −  + tmix  } log  ,  1 tmix (cid:27) log  (cid:17)  (cid:16)  1 γ)2ε  (1  −  tepoch |S||A| δ  ,  log  (cid:17) tepoch |S||A| δ  (26a)  (26b)  (26c)  ,  (cid:16)  (cid:17) where c0 > 0 is some suﬃciently small constant, c1, c2 > 0 are some suﬃciently large constants, and we recall the deﬁnitions of µmin and tmix in (4) and (5), respectively. Note that the learning rate (26a) chosen (which happens if here could be larger than the choice (10b) for the classical form by a factor of O tmix is not too large), allowing the algorithm to progress more aggressively.  1 γ)2  (cid:16)  (cid:16)  (cid:17)  (cid:17)  (1  −  (cid:0)  (cid:1)  Theorem 4 (Asynchronous variance-reduced Q-learning). Let Qepoch be the output of Algorithm 2 with parameters chosen according to (26). There exists some constant c3 > 0 such that for any 0 < δ < 1 and 0 < ε  M  1  γ , one has  ≤  1  −  with probability at least 1  −  (s, a)  ∀  ∈ S × A  :  Qepoch  M (s, a)  |  −  Q⋆(s, a)  | ≤  ε  δ, provided that the total number of epochs exceeds  The proof of this result is postponed to Section D.  M  ≥  c3 log  ε(1  1  −  γ)2 .  (27)  In view of Theorem 4, the ℓ  based sample complexity for variance-reduced Q-learning to yield ε accuracy  ∞ — which is characterized by M (N + tepoch) — can be as low as  µmin(1  −  (cid:16)  1 γ)3 min  +  tmix µmin(1  −  γ)  .  (cid:17)  1, ε2  {  }  (28)  O  e  Except for the second term that depends on the mixing time, the ﬁrst term matches Wainwright (2019b) derived for the synchronous settings with independent samples. In the range ε ], the  1,  1 γ)3 matches the minimax lower bound derived  (0, min {  ∈  1 γ)√tmix }  (1  −  sample complexity reduce to in Azar et al. (2013) for the synchronous setting.  µmin(1  γ)3ε2  O  −  ; the scaling  (1  −  1  (cid:0)  e  (cid:1)  Once again, we can immediately deduce guarantees for asynchronous variance-reduced TD learning by reducing the action space to a singleton (similar to Section 3.3), which extends the analysis Khamaru et al. (2020) to Markovian noise. We do not elaborate on this here as it is not the main focus of the current paper.  11  Algorithm 3:  function Q = Vr-q-run-epoch( Q, N, tepoch)  1 Draw N new consecutive samples from the sample trajectory; compute 2 Set s0 ← 3 for t = 1, 2, 4  current state, and Q0 ← , tepoch do 1 ∼  1) and next state st ∼  1, at  1).  P (  · ·  Q.  st  |  −  −  5  Draw action at − Update Qt according to (22). Qtepoch.  πb(st  −  6 return: Q  ←  (Q) according to (23).  T  e  5 Related work  The Q-learning algorithm and its variants. The Q-learning algorithm, originally proposed in Watkins (1989), has been analyzed in the asymptotic regime by Borkar and Meyn (2000); Jaakkola et al. (1994); Szepesvári (1998); Tsitsiklis (1994) since more than two decades ago. Additionally, ﬁnite-time perfor- mance of Q-learning and its variants have been analyzed by Beck and Srikant (2012); Chen et al. (2020); Even-Dar and Mansour (2003); Kearns and Singh (1999); Qu and Wierman (2020); Wainwright (2019a) in the tabular setting, by Bhandari et al. (2018); Cai et al. (2019); Chen et al. (2019); Du et al. (2020, 2019); Fan et al. (2019); Weng et al. (2020a,b); Xu and Gu (2020); Yang and Wang (2019) in the context of function approximations, and by Shah and Xie (2018) with nonparametric regression. In addition, Azar et al. (2011); Devraj and Meyn (2020); Ghavamzadeh et al. (2011); Sidford et al. (2018a); Strehl et al. (2006); Wainwright (2019b) studied modiﬁed Q-learning algorithms that might potentially improve sample complexities and ac- celerate convergence. Another line of work studied Q-learning with sophisticated exploration strategies such as UCB exploration (e.g. Bai et al. (2019); Jin et al. (2018); Wang et al. (2020)), which is beyond the scope of the current work.  1  ∞  ∞  guarantees avail- guarantees for Q-learning. We now expand on non-asymptotic ℓ Finite-sample ℓ able in prior literature, which are the most relevant to the current work. An interesting aspect that we shall highlight is the importance of learning rates. For instance, when a linear learning rate (i.e. ηt = 1/t) is adopted, the sample complexity results derived in past work Even-Dar and Mansour (2003); Szepesvári In the synchronous setting, (1998) exhibit an exponential blow-up in Beck and Srikant (2012); Chen et al. (2020); Even-Dar and Mansour (2003); Wainwright (2019a) studied the ﬁnite-sample complexity of Q-learning under various learning rate rules; the best sample complexity known to date is , achieved via either a rescaled linear learning rate (Chen et al., 2020; Wainwright, 2019a) or a constant learning rate (Chen et al., 2020). When it comes to asynchronous Q-learning (in its classical form), our work provides the ﬁrst analysis that achieves linear scaling with 1/µmin or tcover; see Ta- ble 1 for detailed comparisons. Going beyond classical Q-learning, the speedy Q-learning algorithm provably (Azar et al., 2011) in the asynchronous setting, whose update achieves a sample complexity of In comparison, our analysis of the variance-reduced rule takes twice the storage of classical Q-learning. Q-learning algorithm achieves a sample complexity of O  γ , which is clearly undesirable.  when ε < 1.  |S||A| −  γ)4ε2  γ)5ε2  tcover  O  O  e  e  (1  (1  (cid:1)  (cid:0)  (cid:0)  (cid:1)  −  −  1  1  γ)3ε2 + tmix  µmin(1  µmin(1  −  γ)  −  (cid:1)  (cid:0)  e  Finite-sample guarantees for model-free algorithms. Convergence properties of several model-free RL algorithms have been studied recently in the presence of Markovian data, including but not limited to TD learning and its variants (Bhandari et al., 2018; Dalal et al., 2018a,b; Doan et al., 2019; Gupta et al., 2019; Kaledin et al., 2020; Lee and He, 2019; Lin et al., 2020; Srikant and Ying, 2019; Xu et al., 2020, 2019), Q- learning (Chen et al., 2019; Xu and Gu, 2020), and SARSA (Zou et al., 2019). However, these recent papers risk, where the latter is often more relevant typically focused on the (weighted) ℓ2 error rather than the ℓ in the context of RL. In addition, Khamaru et al. (2020); Mou et al. (2020) investigated the ℓ bounds of (variance-reduced) TD learning, although they did not account for Markovian noise.  ∞  ∞  Finite-sample guarantees for model-based algorithms. Another contrasting approach for learning the optimal Q-function is the class of model-based algorithms, which has been shown to enjoy minimax- optimal sample complexity in the synchronous setting. More precisely, it is known that by planning over  12  O  an empirical MDP constructed from samples, we are guaranteed to ﬁnd not only an ε-optimal Q-function but also an ε-optimal policy (Agarwal et al., 2019; Azar et al., 2013; Li et al., 2020). It is worth emphasizing that the minimax optimality of model-based approach has been shown to hold for the entire ε-range; in comparison, the sample optimality of the model-free approach has only been shown for a smaller range of accuracy level ε in the synchronous setting. We also remark that existing sample complexity analysis for model-based approaches might be generalizable to Markovian data.  |S||A| −  γ)3ε2  e  (1  (cid:1)  (cid:0)  6 Analysis of asynchronous Q-learning  This section is devoted to establishing Theorem 1. Before proceeding, we ﬁnd it convenient to introduce R|S||A|×|S||A| be a diagonal matrix obeying some matrix notation. Let Λt ∈ Λt  (s, a), (s, a)  1, at  (29)  1),  :=  −  −  η, 0,  (  if (s, a) = (st otherwise,  (cid:0)  (cid:1)  R|S|) to represent where η > 0 is the learning rate. In addition, we use the vector Qt ∈ our estimate Qt (resp. Vt) in the t-th iteration, so that the (s, a)-th (resp. sth) entry of Qt (resp. Vt) is R|S| represent the optimal given by Qt(s, a) (resp. Vt(s)). Similarly, let the vectors Q⋆ R|S||A| stand for Q-function Q⋆ and the optimal value function V ⋆, respectively. We also let the vector r the reward function r, so that the (s, a)-th entry of r is given by r(s, a). In addition, we deﬁne the matrix Pt ∈ {  R|S||A| (resp. Vt ∈  |S||A|×|S| such that  R|S||A| and V ⋆  0, 1  ∈  ∈  ∈  }  Pt  (s, a), s′  :=  (cid:0)  (cid:1)  if (s, a, s′) = (st  1, 0, otherwise.  (  1, at  −  1, st),  (30)  −  Clearly, this set of notation allows us to express the Q-learning update rule (7) in the following matrix form  6.1 Error decay under constant learning rates  (cid:0)  (cid:1)  (cid:0)  Qt =  I  Λt  1 + Λt  Qt  −  −  r + γPtVt  −  .  1  (cid:1)  (31)  The main step of the analysis is to establish the following result concerning the dynamics of asynchronous Q-learning. In order to state it formally, we ﬁnd it convenient to introduce several auxiliary quantities  tframe :=  443tmix µmin  4  log  |S||A| δ  T  ,  tth := max  (  2 log  1 γ)2ε  (cid:16) (1 − ηµmin  (cid:17) , tframe  ,  )  µframe :=  µmintframe,  1 2 ρ := (1  γ)  1  (1  η)µframe  .  − With these quantities in mind, we have the following result. Theorem 5. Consider the asynchronous Q-learning algorithm in Algorithm 1 with ηt ≡ and any ε following relation holds uniformly for all t  γ ], there exists a universal constant c > 0 such that with probability at least 1  T (deﬁned in (10a))  (0,  −  −  ∈  (cid:0)  (cid:1)  −  η. For any δ  1  1  −  Qt −  k  Q⋆  k∞ ≤  ρ)k k  (1  −  ≤ Q0 − 1 −  Q⋆ γ  k∞  +  cγ  −  1  γ k  V ⋆  provided that 0 < η log  T  |S||A| δ  < 1. Here, we deﬁne k := max  T  |S||A| δ  + ε,  (cid:17)  η log  k∞r t tth 0, − tframe  (cid:16) .  (32a)  (32b)  (32c)  (32d)  (0, 1) ∈ 6δ, the  (33)  In words, Theorem 5 asserts that the ℓ (cid:0)  estimation error decays linearly — in a blockwise manner — to some error ﬂoor that scales with √η. This result suggests how to set the learning rate based on the target accuracy level, which in turn allows us to pin down the sample complexity under consideration. In what follows, we shall ﬁrst establish Theorem 5, and then return to prove Theorem 1 using this result.  (cid:5)(cid:9)  (cid:8)  ∞  (cid:4)  (cid:1)  13  6.2 Proof of Theorem 5  6.2.1 Key decomposition and a recursive formula  The starting point of our proof is the following elementary decomposition  ∆t := Qt −  Q⋆ =  I  =  =  =  I (cid:0) I (cid:0) I (cid:0)  Λt Λt Λt Λt  −  −  −  −  (cid:1)  Qt − Qt Qt (cid:1)(cid:0) ∆t (cid:1)(cid:0) −  −  1 + Λt Q⋆ (cid:0) 1 − Q⋆ 1 − − 1 + γΛt  Q⋆  −  −  1 r + γPtVt (cid:1) 1 − − P V ⋆ PtVt 1 − − V ⋆ + γΛtPt  Q⋆  (cid:1)  r + γPtVt + Λt + γΛt (cid:0) P Pt − (cid:0) (cid:0)  (cid:1)  (cid:1)  (cid:1)  Vt (cid:1)  V ⋆  1 −  (34)  −  for any t > 0, where the ﬁrst line results from the update rule (31), and the penultimate line follows from the Bellman equation Q⋆ = r + γP V ⋆ (see Bertsekas (2017)). Applying this relation recursively gives  (cid:0)  (cid:1)  (cid:0)  (cid:1)  t  t  ∆t = γ  I  i=1 X  j=i+1 Y  (cid:0)  Λj  Λi  −  (cid:1) =:β1,t  Pi − (cid:0)  t  t  P  V ⋆  + γ  I  (cid:1)  i=1 X  j=i+1 Y  (cid:0)  |  {z  | } Applying the triangle inequality, we obtain ∆t| ≤ | | ]1  where we recall the notation these terms separately.  zi|  := [  i ≤  z  ≤  |  |  |  +  β1,t|  β3,t| β2,t| n for any vector z = [zi]1  +  |  |  ,  Λj  ΛiPi  −  Vi  −  1 −  (cid:0)  (cid:1) =:β2,t  {z  (cid:1)  }  t  V ⋆  +  I  Λj  ∆0  . (35)  −  j=1 Y  (cid:0)  (cid:1)  =:β3,t  |  {z  }  (36)  n. In what follows, we shall look at  i ≤  ≤ First of all, given that I  −  Pi  Vi  −  1 −  V ⋆  ∞ ≤ k  Λj and Λj are both non-negative diagonal matrices and that Pik1k  k∞ ≤ k  1 −  1 −  1 −  V ⋆  V ⋆  Q⋆  k∞  k∞  Qi  Vi  Vi  =  =  k  k  −  −  −  ∆i −  (cid:1)(cid:13) we can easily see that (cid:13)  (cid:13) (cid:13)  (cid:0)  β2,t  γ  ≤  t  i=1 X  ∆i −  k  1k∞  t  I  j=i+1 Y  (cid:0)  Λj  Λi1.  −  (cid:1)  (cid:12) (cid:12)  (cid:12) (cid:12)  Next, the term β1,t can be controlled by exploiting some sort of statistical independence across diﬀerent transitions and applying the Bernstein inequality. This is summarized in the following lemma, with the proof deferred to Section E.1.  Lemma 1. Consider any ﬁxed vector V ⋆ that for any 0 < δ < 1, one has  ∈  R|S|. There exists some universal constant c > 0 such  ,  1k∞  (37)  with probability at least 1  −  δ, provided that 0 < η log  < 1. Here, we deﬁne  1 ∀  ≤  t  ≤  T :  t  t  γ  i=1 X  j=i+1 Y  (cid:0)  Λj  Λi  I  −  (cid:1)  Pi − (cid:0)  P  V ⋆  (cid:1)  V ⋆  τ1k  ≤  k∞  1  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  T  |S||A| δ  (cid:0) |S||A| δ  (cid:1) T  .  (cid:16)  (cid:17)  τ1 := cγ  η log  r  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  (38)  (39)  Additionally, we develop an upper bound on the term β3,t, which follows directly from the concentration of the empirical distribution of the Markov chain (see Lemma 5). The proof is deferred to Section E.2.  Lemma 2. For any δ > 0, recall the deﬁnition of tframe in (32a). Suppose that T > tframe and δ one has 0 < η < 1. Then with probability exceeding 1  −  t  I  j=1 Y  (cid:0)  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  Λj  ∆0  −  (cid:1) t  ≥  ≥  uniformly over all t obeying T  (1  (1  1 2 tµmin  η)  ∆0  ≤  ≤  −  (cid:12) (cid:12) (cid:12) (cid:12) tframe and all vector ∆0 ∈ (cid:12)  (cid:12) (cid:12)  (cid:12) (cid:12)  1 2 tµmin  η)  ∆0k∞  k  1  −  R|S||A|.  (40)  14  Moreover, in the case where t < tframe, we make note of the straightforward bound  t  (cid:1) Λj is a diagonal non-negative matrix whose entries are bounded by 1  (cid:0)  j=1 Y  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  Λj  ∆0  I  −  ∆0k∞  1,  ≤ k  (41)  η < 1.  −  given that I  −  Substituting the preceding bounds into (36), we arrive at  1, ∞ 1 2 tµmin η) (cid:13) − (cid:13) 2δ, where tframe is deﬁned in (32a). The rest of the proof is thus dedicated to  Λj)Λi1 + τ1k Λj)Λi1 + τ1k  1 + 1 + (1 (cid:13) (cid:13)  t < tframe t tframe  V ⋆ V ⋆  k∞ k∞  − −  ∆0  ∆0  (42)  1,  ≤  ≤  T  ∞  (cid:13) (cid:13)  (cid:13) (cid:13)  t j=i+1(I t j=i+1(I Q Q −  based on the above recursive formula (42).  |  1  1  ∞  γ γ  ∆t| ≤ (  ∆i − ∆i (cid:13) − (cid:13) (cid:13) (cid:13)  t i=1 t (cid:13) P i=1 (cid:13) ∞ (cid:13) P with probability at least 1 (cid:13) ∆t| bounding 6.2.2 Recursive analysis  |  A crude bound. We start by observing the following recursive relation  ∆t| ≤  |  γ  t  i=1 X  ∆i −  1  (cid:13) (cid:13)  (cid:13) (cid:13)  t  ∞  j=i+1 Y  (I  Λj)Λi1 + τ1k  V ⋆  −  k∞  1 +  ∆0k∞  k  1,  1  t  ≤  ≤  T,  (43)  which is a direct consequence of (42). In the sequel, we invoke mathematical induction to establish, for all 1  T , the following crude upper bound  t  ≤  ≤  ∆t  ∞ ≤  V ⋆  τ1k  k∞ 1  + k γ  −  (cid:13) (cid:13)  (cid:13) (cid:13)  ∆0k∞  ,  (44)  which implies the stability of the asynchronous Q-learning updates.  Towards this, we ﬁrst observe that (44) holds trivially for the base case (namely, t = 0). Now suppose  that the inequality (44) holds for all iterations up to t  1. In view of (43) and the induction hypotheses,  −  (I  Λj)Λi1 + τ1k  V ⋆  −  k∞  1 +  ∆0k∞  k  1,  (45)  V ⋆  γ  ∆t| ≤  |  τ1k (cid:0)  + γ  k∞ 1 −  ∆0  ∞  t  t  (cid:13) (cid:13)  (cid:1)  (cid:13) (cid:13)  i=1 j=i+1 X Y t j=i+1(I Λj)Λi, and denote by N j  where we invoke the fact that the vector t matrix Mi := j=i+1(I between the i-th and the j-th iterations (including i and j). Then the diagonal entries of Mi satisfy  Λj)Λi1 is non-negative. Next, deﬁne the diagonal i (s, a) the number of visits to the state-action pair (s, a)  Q  −  −  Q  Mi((s, a), (s, a)) =  η)N t  i+1(s,a),  η(1 0,  −  (  if (s, a) = (si if (s, a) = (si  −  −  1, ai 1, ai  −  −  1), 1).  Letting e(s,a) ∈ easily verify that  R|S||A| be a standard basis vector whose only nonzero entry is the (s, a)-th entry, we can  t  j=i+1 Y  (I  −  and  Λj)Λi1 = Mi1 = Mie(si−1,ai−1) = η(1  −  η)N t  i+1(si−1,ai−1)e(si−1,ai−1)  (46a)  t  t  i=1 X  j=i+1 Y  Λj)Λi1 =  (I  −  t  i=1 X  η(1  −  η)N t  i+1(si−1,ai−1)e(si−1,ai−1)  t  =  X(s,a)  ∈S×A  (  i=1 X  η)N t  i+1(s,a) 1  η(1  −  (si  −  1, ai  −  1) = (s, a)  (cid:8)  (cid:9)  e(s,a)  )  15  6  X(s,a) Combining the above relations with the inequality (45), one deduces that  ∈S×A  j=0 X  j=0 X  ≤  ∞  η(1  η)je(s,a) =  ∞  η(1  −  −  η)j 1 = 1.  (46b)  ∆t  ∞ ≤  (cid:13) (cid:13)  (cid:13) (cid:13)  V ⋆  γ(τ1k  + γ  k∞ 1 −  ∆0  )  ∞  (cid:13) (cid:13)  (cid:13) (cid:13)  V ⋆  + τ1k  k∞  +  ∆0  ∞  V ⋆  τ1k  =  k∞ 1  ∆0  + γ (cid:13) (cid:13)  −  ,  ∞  (cid:13) (cid:13)  (cid:13) (cid:13)  (cid:13) (cid:13)  thus establishing (44) for the t-th iteration. This induction analysis thus validates (44) for all 1  t  ≤  ≤  T .  Reﬁned analysis. Now, we strengthen the bound (44) by means of a recursive argument. To begin with, γ)ε for any t > tth, where we it is easily seen that the term (1 − remind the reader of the deﬁnition of tth in (32b) and the fact that γ . It is assumed that T > tth. To facilitate our argument, we introduce a collection of auxiliary quantities ut as follows  is bounded above by (1 ∆0k∞  ∆0k∞  k∞ ≤  1 2 tµmin  Q⋆  − =  η)  k  k  k  −  1  1  ∆0k∞ u0 = k γ 1 − vtk∞  ut =  k  ,  ,  vt =  γ γ  (  t i=1 t i=1  P P  Q Q  t j=i+1(I t j=i+1(I  Λj)Λi1ui − Λj)Λi1ui −  1 + 1,  − −  ∆0k∞  k  1,  t  for 1 ≤ ≤ for t > tth.  tth,  (47a)  (47b)  These auxiliary quantities are useful as they provide upper bounds on lemma. The proof is deferred to Section E.3.  ∆tk∞  k  , as asserted by the following  ut}  {  (48)  Lemma 3. Recall the deﬁnition (39) of τ1 in Lemma 1. With probability at least 1 deﬁned in (47) satisfy  −  2δ, the quantities  ∆tk∞ ≤ The preceding result motivates us to turn attention to bounding the quantities  + ut + ε.  −  k  . Towards this end, we resort to a frame-based analysis by dividing the iterations [1, t] into contiguous frames each comprising tframe (cf. (32a)) iterations. Further, we deﬁne another auxiliary sequence:  ut}  {  V ⋆ τ1k k∞ γ 1  wk := (1  ∆0k∞ ρ)k k γ 1 −  −  = (1  −  ρ)k k  Q0 − 1 −  Q⋆ γ  k∞  ,  (49)  where we remind the reader of the deﬁnition of ρ in (32d). The connection between precise as follows, whose proof is postponed to Section E.4.  wk}  {  and  ut}  {  is made  Lemma 4. For any δ  (0, 1  2 ), with probability at least 1  ∈  2δ, one has  −  ut ≤  wk,  with k = max  0,  (cid:26)  j  t tth − tframe  .  k(cid:27)  (50)  Combining Lemmas 3-4, we arrive at  Qt −  k  Q⋆  k∞  =  ∆tk∞ ≤  k  V ⋆ τ1k k∞ γ 1  −  + wk + ε  ≤  ρ)k  (1  −  k 1  Q0 − γ −  Q⋆  k∞  +  V ⋆ τ1k k∞ γ 1  −  + ε,  which ﬁnishes the proof of Theorem 5.  6.3 Proof of Theorem 1  Now we return to complete the proof of Theorem 1. To control that the ﬁrst term of (33) obeys  ∆tk∞  k  to the desired level, we ﬁrst claim  (1  −  ∆0k∞ ρ)k k 1 −  γ ≤  ε  16  (51)  whenever  ∆0k∞ k γ) ε(1 − provided that η < 1/µframe. Furthermore, by taking the learning rate as  2 γ)ηµmin  tth + tframe +  log  (1  −  ≥  (cid:18)  t  ,  (cid:19)  η = min  (  (1  γ)4ε2 − c2γ2 log |S||A|  δ  ,  T  1 µframe )  ,  one can easily verify that the second term of (33) satisﬁes  where the last step follows since We have thus concluded the proof, as long as the claim (51) can be justiﬁed.  γ . Putting the above bounds together ensures  k∞ ≤  k  −  1  V ⋆  cγ  −  1  γ k  V ⋆  k∞r 1  η log  T  |S||A| δ  (cid:16)  ε,  ≤  (cid:17)  (52)  (53)  (54)  ∆tk∞ ≤  k  3ε.  Proof of the inequality (51). Observe that (1 log( k∆ 0k∞ ε(1−γ) ) ρ  , which would hold as long as (according to the deﬁnition (50) of k)  ρ)k  k  ∆0k∞ 1 −  γ ≤  −  exp(  −  ρk) k  ∆0k∞ 1 −  γ ≤  ε holds true whenever k  ≥  In addition, if η < 1/µframe, then one has (1  η)µframe  −  1  −  ≤  tth + tframe +  t  ≥  tframe ρ  log  ∆0k∞ k γ) ε(1 −  .  (55)  (cid:18) ηµframe/2, thus guaranteeing that  (cid:19)  ρ = (1  γ)  1  (1  −  −  −  η)µframe )  (1  −  ≥  γ)  1  As a consequence, the condition (55) would hold as long as  (cid:0)  1 +  −  ηµframe 2  =  1 2  (1  −  (cid:17)  γ)ηµframe.  (cid:16)  tth + tframe +  t  ≥  2tframe γ)ηµframe  −  (1  log  ε(1  (cid:18)  where we have made use of the simple bound  k  7 Discussion  1  γ)2  (cid:19) − ∆0k∞  =  tth + tframe +  ≥  2 γ)ηµmin  log  ∆0k∞ k γ) ε(1 −  ,  (cid:19)  (cid:18)  (1  −  Q⋆  k  k∞ ≤  1  1  −  γ with Q0 = 0.  This work develops a sharper ﬁnite-sample analysis of the classical asynchronous Q-learning algorithm, highlighting and reﬁning its dependency on intrinsic features of the Markovian trajectory induced by the behavior policy. Our sample complexity bound strengthens the state-of-the-art result by an order of at least . A variance-reduced variant of asynchronous Q-learning is also analyzed, exhibiting improved scaling  |S||A| with the discount complexity 1 1 −  γ .  −  (1  Our ﬁndings and the analysis framework developed herein suggest a couple of directions for future in- vestigation. For instance, our improved sample complexity of asynchronous Q-learning has a dependence 1 γ)5 on the discount complexity, which is inferior to its model-based counterpart. In the synchronous of setting, Wainwright (2019a) demonstrated an empirical lower bound It would be important to determine the exact scaling in this regard. In addition, it would be interesting to see whether the techniques developed herein can be exploited towards understanding model-free algorithms with more sophisticated exploration schemes Dann and Brunskill (2015). Finally, asynchronous Q-learning on a single Markovian trajectory is closely related to coordinate descent with coordinates selected according to a Markov chain; one would naturally ask whether our analysis framework can yield improved convergence guarantees for general Markov-chain-based optimization algorithms Doan et al. (2020); Sun et al. (2020).  1 γ)4 for Q-learning.  (1  −  17  Acknowledgements  Y. Wei is supported in part by the NSF grant CCF-2007911 and DMS-2015447. Y. Chi is supported in part by the grants ONR N00014-18-1-2142 and N00014-19-1-2404, ARO W911NF-18-1-0303, NSF CCF-1806154 and CCF-2007911. Y. Chen is supported in part by the grants AFOSR YIP award FA9550-19-1-0030, ONR N00014-19-1-2120, ARO YIP award W911NF-20-1-0097, ARO W911NF-18-1-0303, NSF CCF-1907661, IIS- 1900140 and DMS-2014279, and the Princeton SEAS Innovation Award. We thank Shicong Cen, Chen Cheng and Cong Ma for numerous discussions about reinforcement learning.  A Preliminaries on Markov chains  For any two probability distributions µ and ν, denote by dTV(µ, ν) the total variation distance between µ and ν (Brémaud, 2013). For any time-homogeneous and uniformly ergodic Markov chain (X0, X1, X2, ) with transition kernel P , ﬁnite state space x) denote the distribution of Xt conditioned on X0 = x. Then the mixing time tmix of this Markov chain is deﬁned by  and stationary distribution µ, we let P t( ·|  · ·  X  tmix(ǫ) := min  t  tmix := tmix(1/4).  n  (cid:12) (cid:12) (cid:12)  max x ∈X  dTV  P t( ·|  x), µ  (cid:0)  ≤  ǫ  ;  o  (cid:1)  (56a)  (56b)  A.1 Concentration of empirical distributions of Markov chains  We ﬁrst record a result concerning the concentration of measure of the empirical distribution of a uniformly ergodic Markov chain, which makes clear the role of the mixing time.  Lemma 5. Consider the above-mentioned Markov chain. For any 0 < δ < 1, if t  y ∀  ∈ X  :  PX1=y  x (∃  ∈ X  t  1  :  Xi = x  {  } ≤  tµ(x)  ) ≤  1 2  i=1 X Proof. To begin with, consider the scenario when X1 ∼ the chain). Then (Paulin, 2015, Theorem 3.4) tells us that: for any given x  PX1∼  µ  t  (  i=1 X  1  {  Xi = x  } ≤  tµ(x)  τ  −  ) ≤  2 exp  2 exp  ≤  −  (cid:18)  −  (cid:18)  µ (namely, X1 follows the stationary distribution of  Ω and any τ  0,  ≥  ∈ τ 2γps 8(t + 1/γps)µ(x) + 20τ  (cid:19)  τ 2/tmix 16(t + 2tmix)µ(x) + 40τ  ,  (cid:19)  (58)  where γps stands for the so-called pseudo spectral gap as deﬁned in Paulin (2015, Section 3.1). Here, the ﬁrst inequality relies on the fact VarXi µ(x), while the last inequality results ] = µ(x)(1 µ(x)) 1/(2tmix) that holds for uniformly ergodic chains (cf. Paulin (2015, Proposition 3.4)). from the fact γps Consequently, for any t  0, continue the bound (58) to obtain  Xi = x }  tmix and any τ  µ[1  −  ≤  ≥  {  ∼  ≥  (58)  ≤  2 exp  −  (cid:18)  τ 2 48tµ(x)tmix + 40τ tmix (cid:19) tµ(x)tmix log 2  10  2 max  exp  (cid:26) , 80tmix log 2  −  (cid:18)  |X |δ  ≤  |X |δ  τ  , exp  τ 2 96tµ(x)tmix (cid:19) . As a result, by taking τ = 10  80tmix (cid:19)(cid:27)  −  ≤  (cid:18)  δ  ,  |X |  21 tµ(x) and  provided that τ ≥ applying the union bound, we reach  max  q  443tmix µmin  log 4  |X |δ  , then  (57)  ≥  δ.  ≥  (cid:8) t  PX1∼  µ  x (∃  :  ∈ X  i=1 X  1  {  Xi = x  } ≤  11 21  tµ(x)  as long as 10 441tmix µmin  log 2  max  21 tµ(x) ≥ q |X |δ with µmin := minx  10  (cid:8)  tµ(x)tmix log 2 µ(x).  |X |δ  ∈X  t  µ  (  i=1 X  1  {  Xi = x  } ≤  11 21  tµ(x)  ) ≤  δ,  (59)  |X |δ  for all x  ∈ X  , or equivalently, when t  ≥  (cid:9)  PX1∼  ) ≤  Xx ∈X , 80tmix log 2  (cid:9)  18  Next, we move on to the case when X1 takes an arbitrary state y  (cf. (56a)), we know that  . From the deﬁnition of tmix( ·  )  ∈ X  dTV  P tmix(δ)( ·|  y), µ  sup y  ∈X  δ.  ≤  (cid:17)  (cid:16)  This together with the deﬁnition of dTV (cf. Paulin (2015, Equation (1.1))) reveals that: for any event that can be fully determined by  tmix(δ), one has  B  P  {B |  X1 = y  P  {B |  P  {B |  } − Xtmix(δ) = s  ≥  Xτ }τ µ  }  { X1 ∼ P  }  {  P  {  Xtmix(δ) = s  X1 = y  |  P  {  } −  Xtmix(δ) = s  |  (cid:12) = (cid:12)  Xs ∈S  (cid:12) (cid:12) (cid:12) Xs ∈S (cid:12) (cid:12) (cid:12) and hence  ≤  Xs ∈S µ X1 ∼  δ,  ≤  }  (cid:12) (cid:12) (cid:12)  Xtmix(δ) = s  (cid:12) (cid:12)  X1 = y  |  } −  P  Xtmix(δ) = s  P  {  }  Xtmix(δ) = s  X1 ∼  |  µ  }  {B |  (cid:12) (cid:12) (cid:12)  sup y  ∈X  PX1=y  x (∃  :  ∈ X  t  Xi=tmix(δ) t  PX1∼  µ  ≤  x (∃  :  ∈ X  Xi=tmix(δ) log 2  1  {  Xi = x  } ≤  11 21  (t  −  tmix(δ))µ(x)  )  1  {  Xi = x  } ≤  11 21  (t  −  tmix(δ))µ(x)  )  + δ  2δ,  ≤  (60)  with the proviso that t  tmix(δ) + 441tmix µmin To ﬁnish up, we recall from Paulin (2015, Section 1.1) that tmix(δ)  |X |δ .  ≥  the above constraint on t necessarily implies that 11 tmix(δ) + 441tmix |X |δ , one has µmin  log 2  21 (t  tmix(δ))  −  ≥  2tmix log 2  ≤  1 2 t. To conclude, if t  δ , which together with |X |δ ≥  443tmix µmin  log 2  ≥  sup y  ∈X  PX1=y  x (∃  :  ∈ X  t  i=1 X  as claimed.  1  {  Xi = x  } ≤  1 2  tµ(x)  ) ≤  sup y  ∈X  PX1=y  x (∃  ∈ X  t  1  {  : Xi=tmix(δ)  Xi = x  } ≤  1 2  tµ(x)  ) ≤  2δ.  A.2 Connection between the mixing time and the cover time  Lemma 5 combined with the deﬁnition (6) immediately reveals the following upper bound on the cover time:  tcover = O  tmix µmin  (cid:16)  log  |X |  .  (cid:17)  (61)  In addition, while a general matching converse bound (namely, tmix/µmin = come up with some special examples for which the bound (61) is provably tight.  O(tcover)) is not available, we can  e  Example. Consider a time-homogeneous Markov chain with state space transition matrix  :=  1,  {  · ·  ,  X  |X |}  and probability  P =  1  (cid:16)  −  q(k + 1) 2  I  |X |  q  +  k1  1⊤ |X |  /2  |X |  (cid:17)  |X | h 1. Suppose q(k + 1) < 2 and  for some quantities q > 0 and k  ≥  1  1⊤ |X |  /2  |X |  ∈  i  R|X |×|X |  3. Then this chain obeys  |X | ≥  (62)  tcover  tmix  ≥  8 log 2 + 4 log 1 µmin  µmin  (cid:0)  (cid:1)  .  19  Proof. As can be easily veriﬁed, this chain is reversible, whose stationary distribution vector µ  R|X | obeys  ∈  µ =  2  (k + 1)  |X | (cid:20)  k1 1  |X |  |X |  .  /2 /2 (cid:21)  As a result, the minimum state occupancy probability of the stationary distribution is given by  µmin := min  1  x  ≤  ≤|X |  µx =  2  (k + 1)  |X |  .  (63)  In addition, the reversibility of this chain implies that the matrix P d := D 1 symmetric and has the same set of eigenvalues as P (Brémaud, 2013). A little algebra yields  2 P D−  1  2 with D := diag [µ] is  P d =  1  (cid:16)  q(k + 1) 2  −  I  |X |  +  (cid:17)  q |X | \"  k1 √k1  |X |  /2  /21⊤ |X | /21⊤ |X |  /2  |X |  √k1 1  /21⊤ |X | /2  |X | /21⊤ |X |  |X |  /2  ,  #  allowing us to determine the eigenvalues  λi}1  ≤  {  i ≤|X |  as follows  λ1 = 1  and  λi = 1  q(k + 1) 2  −  > 0 (i  2).  ≥  We are now ready to establish the lower bound on the cover time. First of all, the well-known connection  between the spectral gap and the mixing time gives Paulin (2015, Proposition 3.3)  tmix  ≤  2 log 2 + log 1 µmin  2(1  λ2)  −  =  2 log 2 + log 1 µmin q(k + 1)  .  (64)  In addition, let (x0, x1, for the stationary distribution. Consider the last state — denoted by occupancy probability µmin. For any integer t > 0 one has  ) be the corresponding Markov chain, and assume that x0 ∼  µ, where µ stands , which enjoys the minimum state  |X |  · ·  P  =  xl 6 {  ,  0  l  t  }  ≤  ≤  ∀  |X |  (i) = P  =  x0 6 {  |X |}  t  Yl=1 t  P  =  xl 6 n  P  min =  j:j  |X |  Yl=1  =  x0 6  ,  |X |  · ·  , xl  =  1 6  −  |X |  o  =  t  1 = j  xl  −  |X |  (cid:12) (cid:12)  (cid:9)  |X |  (cid:12) (cid:12) (cid:12) xl 6 (cid:8) q  P  (ii)  ≥  =  x0 6 {  |X |}  (iii) =  (iv)  ≥  1  (cid:16) 1  (cid:16)  −  −  2  2  (k + 1)  (k + 1)  1 |X | (cid:17) (cid:18) 1 |X | (cid:17) (cid:18)  −  −  |X | (cid:19) 2qt  |X | (cid:19)  ,  where (i) follows from the chain rule, (ii) relies on the Markovian property, (iii) results from the construction (62), and (iv) holds as long as  3 and if t < |X |8q , then one necessarily has  2 . Consequently, if  t < 1  q  |X |  P  1 |X | (cid:17) (cid:18) This taken collectively with the deﬁnition of tcover (cf. (6)) reveals that  (k + 1)  xl 6 {  } ≥  |X |  =  ≤  ≤  −  (cid:16)  ∀  0  1  t  l  ,  2qt  −  |X | (cid:19)  >  1 2  .  |X | ≥  2  tcover  |X | 8q ≥  ≥  tmix  8 log 2 + 4 log 1 µmin  µmin  ,  where the last inequality is a direct consequence of (63) and (64).  (cid:0)  (cid:1)  20  6  B Cover-time-based analysis of asynchronous Q-learning  In this section, we sketch the proof of Theorem 2. Before continuing, we recall the deﬁnition of tcover in (6), and further introduce a quantity  tcover,all := tcover log  .  (65)  T δ  There are two useful facts regarding tcover,all that play an important role in the analysis.  Lemma 6. Deﬁne the event  Kl :=  (s, a)  ∈ S × A  ∃  n T tcover,all ⌋  and set L :=  ⌊  Proof. See Section E.6.  . Then one has P  s.t. it is not visited within iterations  ltcover,all, (l + 1)tcover,all  ,  L l=0 Kl  nS  δ.  ≤  o  (cid:0)  (cid:3) o  In other words, Lemma 6 tells us that with high probability, all state-action pairs are visited at least . The next result is an immediate  once in every time frame (ltcover,all, (l + 1)tcover,all] with 0 ≤ ⌊ consequence of Lemma 6; the proof can be found in Section E.2.  T /tcover,all  ≤  ⌋  l  Lemma 7. For any δ > 0, recall the deﬁnition of tcover,all in (65). Suppose that T > tcover,all and 0 < η < 1. Then with probability exceeding 1  δ one has  − t  Λj  ∆0  I  −  (1  η)  −  ≤  ∆0k∞  k  1  (66)  t 2tcover,all  uniformly over all t obeying T  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≥  j=1 Y  t  ≥  (cid:1)  (cid:12) (cid:12) (cid:0) (cid:12) (cid:12) tcover,all and all vector ∆0 ∈ (cid:12)  R|S||A|.  With the above two lemmas in mind, we are now positioned to prove Theorem 2. Repeating the analysis  of (42) (except that Lemma 2 is replaced by Lemma 7) yields  ∆t| ≤ (  |  γ  γ  t i=1 t i=1  ∆i − ∆i (cid:13) (cid:13) − (cid:13) (cid:13)  1  ∞  t j=i+1(I t j=i+1(I Q  −  Λj)Λi1 + τ1k Λj)Λi1 + τ1k  V ⋆ V ⋆  k∞  ∆0  1 + 1 + (1 (cid:13) (cid:13)  ∞ η) (cid:13) (cid:13)  1, t 2tcover,all  1  P  (cid:13) (cid:13) (cid:13) P with probability at least 1 (cid:13) by tcover,all (resp. Section 6.2.2 to establish a convergence guarantee based on the cover time. More speciﬁcally, deﬁne  Q 2δ. This observation resembles (42), except that tframe (resp. µmin) is replaced ). As a consequence, we can immediately use the recursive analysis carried out in  1 tcover,all  k∞  (cid:13) (cid:13)  (cid:13) (cid:13)  −  −  −  ≤  ∞  ∞  T  t  ≤  ∆0  1,  tcover,all  t < tcover,all  (67)  (68)  ρ := (1  −  γ)  1  (cid:16)  e  tcover,all 2tcover,all  (1  −  −  η)  = (1  −  γ)  1  (1  −  −  (cid:17)  (cid:16)  1 2  η)  .  (cid:17)  Replacing ρ by  ρ in Theorem 5 reveals that with probability at least 1  6δ,  −  e k  Qt −  Q⋆  k∞ ≤  ρ)k k  (1  −  Q0 − 1 −  Q⋆ γ  k∞  +  cγ  −  1  V ⋆  γ k  η log  k∞r  (cid:16)  T  |S||A| δ  + ε  (cid:17)  holds for all t  ≤  T , where k := max  e 0,  t tth − tcover,all  and we abuse notation to deﬁne  (cid:8)  (cid:5)(cid:9) (cid:4) tth := 2tcover,all log  1 γ)2ε  −  .  (1  Repeating the proof of the inequality (51) yields  (1  −  ∆0k∞ ρ)k k 1 −  γ ≤  ε,  e  21  1  ε(1  −  γ)2  , with the proviso that η < 1/2.  In addition, setting  whenever t (1  ≥ γ)4 − c2γ2ε2 log  η =  |S||A|T δ  tth + tcover,all + 2tcover,all guarantees that  (1  −  γ)η log  (cid:0)  (cid:0)  (cid:1)  1  In conclusion, we have  cγ  V ⋆  γ k  − Qt −  k  k∞r Q⋆  k∞ ≤  η log  |S||A| δ  (cid:16)  (cid:17) 3ε as long as  (cid:1)  T  cγ  ≤  (1  γ)2  −  η log  r  (cid:16)  T  |S||A| δ  ε.  ≤  (cid:17)  (cid:16) for some suﬃciently large constant c′ > 0. This together with the deﬁnition (65) completes the proof.  (cid:17)  (cid:17)  (cid:16)  −  c′tcover,all (1  γ)5ε2 log  t  ≥  T  log  |S||A| δ  ε(1  γ)2  ,  1  −  C Analysis under adaptive learning rates (proof of Theorem 3)  Useful preliminary facts. We ﬁrst make note of several useful properties about ηt. Invoking the concentration result in Lemma 5, one can easily show that with probability at least 1  1 2  µmin < min s,a  Kt(s, a) t  < 2µmin  δ,  −  (69)  holds simultaneously for all t obeying T taken collectively with the update rule (17) of that  ≥  µmin,t stabilizes as t grows; more precisely, there exists some quantity c′  t & tmix log( |S||A|t  δ  )  . In addition, this concentration result µmin,t (in particular, the second case of (17)) implies  µmin  [1/4, 4] such that  ∈  (70)  b  holds simultaneously for all t obeying T  ≥  c′µmin  b µmin,t ≡ t & tmix log( |S||A|t b  δ  µmin  )  .  For any t obeying t & tmix log( |S||A|t simpliﬁes to  µmin(1  δ γ)  −  )  (so that  bµmin,t(1  log t  −  γ)γ2t is small enough), the learning rate (18)  ηt = cη exp  log  (cid:16)j  log t  c′µmin(1  γ)γ2t  −  .  k(cid:17)  (71)  Clearly, there exists a sequence of endpoints t1 < t2 < t3 < . . . as well as a threshold kth such that: for any k  kth one has  ≥  2tk < tk+1 < 3tk  and  ηt = η(k) :=  αk log tk+1  µmin(1  −  γ)γ2tk+1  ,  tk < t  ∀  ≤  tk+1  (72)  (73)  for some constant αk > 0; in words, (73) provides a concrete expression for the piecewise constant learning rate, where the tk’s form the change points.  Combining (73) with the deﬁnition of  Qt (cf. (17)), one can easily check that for t & tmix log( |S||A|t  µmin(1  δ γ)  −  b Qt = Qtk ,  tk < t  ∀  ≤  tk+1,  )  ,  (74)  so that in the sequel, which can be easily accomplished by invoking Theorem 1.  Qt remains ﬁxed within each time segment (tk, tk+1]. As a consequence, we only need to analyze Qtk  b  b  22  A crude bound. Given that 0 < ηt ≤ Qt  max  (1  Qtk∞ ≤  k  ηt) k  −  1k∞  −  thus leading to the following crude bound  (cid:8)  ≤ + ηt(1 + γ  ≤ Qt  −  k  ),  1k∞  Qt  −  k  1k∞  Qt  −  1k∞  ≤ k  + γ,  (cid:9)  1 and 0  r(s, a)  1, the update rule (7) of Qt implies that  Qt −  k  Q⋆  k∞ ≤  t +  Q0k∞  k  +  k  Q⋆  k∞ ≤  t +  2  −  1  γ ≤  3t,  for any t >  1  −  1  .  γ  Reﬁned analysis. Deﬁne  εk :=  s  tk  ck,0 log( |S||A| µmin(1  δ γ)5γ2tk  ) log tk  ,  −  (75)  (76)  where ck,0 = αk (73) of ηt together with the deﬁnition (76) implies that  −  1/c1 > 0 is some constant, and c1 > 0 is the constant stated in Theorem 1. The property  ηt =  c1(1 − log( |S||A|  γ)4ε2 k tk )  δ  =  c1  log( |S||A|  δ  tk  )  min  (1  n  γ)4ε2 k,  −  1 tmix  ,  o  t ∀  ∈  (tk  −  1, tk],  as long as (1  γ)4ε2  k ≤  −  1/tmix, or more explicitly,  tk ≥  ck,0tmix log( |S||A| δ γ)γ2 µmin(1  tk  ) log tk  −  .  (77)  In addition, condition (72) further tells us that  tk −  tk  1 ≍  −  tk =  max  ≍  ck,0 log  tk  |S||A|  log tk  δ γ)5γ2ε2 (cid:1) k  µmin(1 (cid:0) 1  −  µmin(1  γ)5ε2 k  −  n  ,  tmix µmin(1  γ)  log  o  (cid:16)  tk  |S||A| δ  log tk  (cid:17)  −  under the sample size condition (77). Now suppose that ck,0 is suﬃciently large (which can be guaranteed by adjusting the constant cη in (18)). Invoking Theorem 1 with an initialization Qtk−1 (which clearly satisﬁes the crude bound (75)) ensures that  with probability at least 1  Qtk − δ, with the proviso that  k  Q⋆  εk  k∞ ≤  − c5 µmin (cid:26)  (1  tk ≥  − for some large enough constant c5 > 0.  1 γ)5ε2 k  +  tmix γ 1  −  (cid:27)  log  (cid:16)  tk  |S||A| δ  (cid:17)  log  tk γ)2εk  −  (cid:17)  (1  (cid:16)  (78)  (79)  Finally, taking tkmax to be the largest change point that does not exceed T , we see from (72) that ε. These T . By choosing the constant c5 to be suﬃciently large, we can ensure that εkmax ≤  1 3 T immediately conclude the proof of the theorem under the sample size condition (21).  tkmax ≤  ≤  D Analysis of asynchronous variance-reduced Q-learning  This section aims to establish Theorem 4. We carry out an epoch-based analysis, that is, we ﬁrst quantify the progress made over each epoch, and then demonstrate how many epochs are suﬃcient to attain the desired accuracy. In what follows, we shall overload the notation by deﬁning  tframe :=  443tmix µmin  log  tepoch  4  |S||A| δ  ,  (cid:17)  (cid:16)  23  (80a)  tth := max  ( γ)  1  − − (cid:0) µmintframe.  ρ := (1 1 2  µframe :=  2 log  1 γ)2ε  (1 − ηµmin  , tframe  ,  )  η)µframe  ,  (1  −  (cid:1)  (80b)  (80c)  (80d)  D.1 Per-epoch analysis  We start by analyzing the progress made over each epoch. Before proceeding, we denote by [0, 1]|S||A|×|S| a matrix corresponding to the empirical probability transition kernel used in (23) from N new sample R|S||A| to represent the reference Q-function, and introduce the transitions. Further, we use the vector Q vector V  R|S| to represent the corresponding value function so that V (s) := maxa Q(s, a) for all s For convenience, this subsection abuses notation to assume that an epoch starts with an estimate Q0 = Q,  ∈ S  ∈  ∈  ∈  e  .  P  and consists of the subsequent  tepoch := tframe + tth +  8 log 2 1 − γ)ηµmin  γ  (1  −  (81)  iterations of variance-reduced Q-learning updates, where tframe and tth are deﬁned in (80a) and (80b), respec- tively. In the sequel, we divide all epochs into two phases, depending on the quality of the initial estimate Q in each epoch.  D.1.1 Phase 1: when Recalling the matrix notation of Λt and Pt in (29) and (30), respectively, we can rewrite (22) as follows  > 1/√1  k∞  −  −  γ  k  Q  Q⋆  Following similar steps as in the expression (34), we arrive at the following error decomposition  (cid:0)  (cid:1)  (cid:16)  (cid:17)  e  Qt =  I  Λt  1 + Λt  Qt  −  −  r + γPt(Vt  −  1 −  V ) + γ  P V  .  (82)  r + γPt(Vt  V ) + γ  P V  Q⋆  Θt := Qt −  Q⋆ =  I  =  (cid:0) I  =  (cid:0) I  =  (cid:0) I  −  −  −  −  Λt  1 + Λt  Qt  −  (cid:1)  Λt  Qt  −  1 −  Λt  Λt  (cid:1)(cid:0)  Qt  Θt (cid:1)(cid:0) −  1 − − 1 + γΛt  (cid:16) Q⋆  + Λt  Q⋆  (cid:1)  (cid:16) + γΛt  (cid:1) P  P  −  −  1 − r + γPt(Vt  Pt(Vt  − (cid:16) V + γΛt  1 −  which once again leads to a recursive relation  (cid:1)  (cid:0)  (cid:0)  e  (cid:1)  − (cid:17) V ) + γ e  −  1 − V ) +  P V  Pt − (cid:0)  P  (V ⋆ e (cid:1)  −  Q⋆  (cid:17)  P V  − P V ⋆ e −  (cid:17) V ) + γΛtPt  Vt  −  1 −  V ⋆  ,  (83)  (cid:0)  (cid:1)  t  t  Θt =γ  I  i=1 X  j=i+1 Y  (cid:0)  |  + γ  (cid:1) =:h0,t  t  {z  t  (cid:0)  e  I  i=1 X  j=i+1 Y  (cid:0)  Λj  Λi  P  −  −  P  V  + γ  (cid:1)  t  t  I  i=1 X  j=i+1 Y  (cid:0)  Λj  Λi  −  P  (V ⋆  Pi −  V )  −  (cid:0) (cid:1) =:h1,t  (cid:1)  Λj  | } ΛiPi  −  Vi  −  1 −  (cid:1) =:h2,t  (cid:0)  V ⋆  +  t  {z I  (cid:1)  j=1 Y  (cid:0)  (cid:1)  =:h3,t  Λj  Θ0  .  −  }  (84)  This identity takes a very similar form as (35) except for the additional term h0,t.  {z  {z  |  }  |  }  Let us begin by controlling the ﬁrst term, towards which we have the following lemma. The proof is  postponed to Section E.5.  Lemma 8. Suppose that probability greater than 1  δ, one has  P is constructed using N consecutive sample transitions. If N > tframe, then with  − e h0,tk∞ ≤  k  4 log  6N  |S||A| δ N µmin (cid:0)  γ  s  (cid:1)  (cid:13) (cid:13)  V ⋆  V  −  +  1  ∞  (cid:13) (cid:13)  24  γ  −  γ s  4 log  6N  |S||A| δ N µmin (cid:0)  .  (cid:1)  (85)  Inheriting the results from Lemma 1 and Lemma 2, we are guaranteed that  h1,t  ≤  V ⋆  τ2k − (1 η) − Θ0k∞  (cid:12) (cid:12) |  (cid:12) (cid:12) h3,t| ≤   k  1,  V  k∞ 1 2 tµmin  1 Θ0k∞  k  with probability at least 1  2δ, where    −  τ2 := c′γ  η log  r  1,  if tframe  t  ≤  ≤  tepoch  if t < tframe  tepoch |S||A| δ  (cid:0)  (cid:1)  for some constant c′ > 0 (similar to (39)). In addition, the term h2,t can be bounded in the same way as β2,t in (37). Therefore, repeating the same argument as for Theorem 5, we conclude that with probability at least 1  δ,  −  Θtk∞ ≤  k  (1  −  Θ0k∞ ρ)k k γ 1 −  +  τ + ξ = (1  Q  ρ)k k  −  Q⋆ γ  − 1  −  k∞  +  τ + ξ  (86)  holds simultaneously for all 0 < t  tepoch, where k = max  e  0,  tth,ξ t − tframe  and  e  ≤  τ :=  cγ  −  1  e  tth,ξ := max  (1  s  γ   2 log   − 1 γ)2ξ  (1 − ηµmin  (  log N  |S||A| δ γ)2N µmin  V ⋆  +  k  −  V  k∞   r    , tframe  )  (cid:8)  (cid:4)  η log  (cid:5)(cid:9)  tepoch |S||A| δ  +  s  (cid:17)  4 log  6N  |S||A| δ N µmin (cid:0)  ,  (cid:1)           (cid:16)  for some constant c > 0.  Let C > 0 be some suﬃcient large constant. Setting ηt ≡ tframe, C log N |S||A| γ)3µmin }  max {  ≥  and ensuring N  (1  δ  −  , we can easily demonstrate that  η = min  (1  γ)2  − |S||A|tepoch δ  Cγ2 log  n  ,  1 µframe  , ξ =  o  1 16√1  −  γ  Θtk∞ ≤  k  (1  −  Q  ρ)k k  Q⋆ γ  − 1  k∞  +  1 8√1  −  +  γ  1 4 k  V ⋆  V  .  k∞  −  As a consequence, if tepoch  tframe + tth,ξ +  ≥  which in turn implies that  − 8 log 2 (1  1−γ γ)ηµmin  −  , one has  ρ)k  (1  −  ≤  1 8  (1  −  γ),  Θtepochk∞ ≤  k  1 8 k  Q  −  Q⋆  +  k∞  1  8  (1  γ)  −  +  1 4 k  V ⋆  V  −  k∞ ≤  1 2  max  1 √1  −  n  ,  γ  Q  k  −  Q⋆  k∞  ,  o  where the last step invokes the simple relation  p  Qtepoch −  k  Q⋆  k∞ ≤  k 1 2  V ⋆  V  −  max  n  k∞ ≤ k 1 √1  γ  ,  −  Q  −  Q  Q⋆  k∞  . Thus, we conclude that  Q⋆  −  k∞  .  o  k  (87)  (88)  D.1.2 Phase 2: when  Q  k  −  Q⋆  k∞ ≤  1/√1  γ  −  The analysis of Phase 2 follows by straightforwardly combining the analysis of Phase 1 and that of the synchronous counterpart in Wainwright (2019b). For the sake of brevity, we only sketch the main steps.  25  Following the proof idea of Wainwright (2019b, Section B.2), we introduce an auxiliary vector  Q which is the unique ﬁx point to the following equation, which can be regarded as a population-level Bellman equation with proper reward perturbation, namely,  b  Q = r + γP (  V  V ) + γ  P V .  −  (89)  V  R|S| represents the value function corresponding to  Here, as usual, equation when the reward vector r is replaced by r + γ( Wainwright (2019b, Lemma 4) (except that we need to apply the measure concentration of performed in the proof of Lemma 8 due to Markovian data), we reach  Q. This can be viewed as a Bellman P )V . Repeating the arguments in the proof of P in the manner  −  P  ∈  b  b  e  b  e  b  Q⋆  Q  −  c′  s  (1  ∞ ≤  (cid:13) (cid:13) b  (cid:13) (cid:13)  log |S||A|δ  γ)3N µmin ≤  −  ε  e  (90)  with probability at least 1 Q⋆ constructed in the algorithm, as we don’t have access to the probability transition matrix P . b  δ for some constant c′ > 0, provided that N  − ≥ Q only serves as a helper in the proof and is never explicitly  γ. It is worth noting that  In addition, we claim that  γ)3ε2 and that  k∞ ≤  1/√1  −  −  k  (1  −  δ  Q  (c′)2 log |S||A|  Under this claim, the triangle inequality yields  b  Q  Qtepoch − (cid:13) (cid:13)  ∞ ≤ (cid:13) (cid:13)  Q  k  −  Q⋆ 8  b  k∞  + k  Q  −  Q⋆ 8  k∞  + ε.  (91)  Qtepoch −  k  Q⋆  Q  Qtepoch − k∞ ≤ k 1 Q⋆ Q b 8 k k∞  −  ≤  k∞  +  k 17 8  +  Q  Q⋆  −  k∞ ≤  1 8 k  Q  −  Q⋆  k∞  +  9 8 k  Q  −  Q⋆  k∞  + ε  ε, b  b  (92)  where the last inequality follows from (90).  Proof of the inequality (91). Recalling the variance-reduced update rule (82) and using the Bellman-type equation (89), we obtain  Θt := Qt − b  Q =  I  =  (cid:0) I  b  =  (cid:0) I  Λt  Λt  Λt  −  −  −  r + γPt(Vt  V ) + γ  P V  (Qt  −  1 −  Q) + Λt  (cid:1) (Qt  (cid:1) Θt −  1 − − 1 + γΛt b  Q) + Λt b  (cid:16)  γPt(Vt  −  (cid:16) (Pt − (cid:16)  P )(  V  −  −  1 − V )  γP (  V  e −  1 − − V ) + Pt(Vt  b 1 −  −  (cid:0)  (cid:1)  Adopting the same expansion as before (see (35)), we arrive at  b  b  V )  (cid:17) .  (cid:17)  b  γP (  V  r  −  V )  −  −  γ  P V  b  e  − V )  (cid:17)  (93)  t  t  Θt = γ  I  i=1 X  j=i+1 Y  (cid:0)  b  −  Λj  Λi  Pi − (cid:0) (cid:1) =:ϑ1,t  P  V (  −  V )  + γ  (cid:1)  b  t  t  I  i=1 X  j=i+1 Y  (cid:0)  Λj  ΛiPi  −  Vi  −  1 −  V  +  (cid:1) =:ϑ2,t  (cid:0)  (cid:1)  b  t  I  j=1 Y  (cid:0)  Λj  Θ0  .  −  (cid:1)  b  =:ϑ3,t  | Inheriting the results in Lemma 1 and Lemma 2, we can demonstrate that, with probability at least 1  {z  {z  {z  }  }  |  |  } 2δ,  −  ϑ1,t  cγ  V  k  ≤  −  V  (cid:12) (cid:12) |  (cid:12) (cid:12) ϑ3,t| ≤      (1 η) b − Θ0k∞ b  k  1,  k∞r 1 2 tµmin  η log  k  (cid:16) 1, Θ0k∞ b  tepoch |S||A| δ  (cid:17) if tframe  1;  t  ≤  ≤  tepoch,  if t < tframe.  26  Repeating the same argument as for Theorem 5, we reach  Q Θtk∞ ≤ − 1 − b b for some constant c > 0, where k = max  ρ)k k  (1  −  k  By taking η = c5 min  (1  γ)2 − |S||A|tepoch δ  γ2 log  (cid:8)  Q k∞ γ  +  1  cγ  t tth − tframe  0, 1 (cid:4) µframe  { ,  } (cid:5)  (cid:9)  for some large constant c6 > 0, we obtain  tepoch  ≥  tth + tframe +  c6 γ)ηµmin  log  (1  1  −  γ)2  (1  −  V  V  η log  −  −  γ k  k∞r with tth deﬁned in (80b).  (cid:16)  b  tepoch |S||A| δ  + ε  (cid:17)  for some suﬃciently small constant c5 > 0 and ensuring that  ≤ where the last line follows by the triangle inequality.  b  k  Θtepochk∞ ≤ b  Q  k  Q − 8  k∞  + ε  Q  k  −  Q⋆ 8  b  k∞  + k  Q  −  Q⋆ 8  k∞  + ε,  D.2 How many epochs are needed?  We are now ready to pin down how many epochs are needed to achieve ε-accuracy.  In Phase 1, the contraction result (88) indicates that, if the algorithm is initialized with Q0 = 0 at the very beginning, then it takes at most log2  Q⋆ ε,  k max  k∞ 1 √1  γ  ! ≤  log2  1 √1  γ  −  + log2  (cid:17)  1  −  γ)  (cid:17)  ε(1  (cid:16)  (cid:16)  epochs to yield level ε > 1 √1 −  Q  − 1 √1 { γ , then the algorithm terminates in this phase.  (cid:8) max  (cid:9) γ , ε  k∞ ≤  Q⋆  −  k  }  −  (so as to enter Phase 2). Clearly, if the target accuracy Suppose now that the target accuracy level ε γ . Once the algorithm enters Phase 2, the dynamics − can be characterized by (92). Given that Q is also the last iterate of the preceding epoch, the property (92) provides a recursive relation across epochs. Standard recursive analysis thus reveals that: within at most  ≤  1 √1  (cid:17) epochs (with c7 > 0 some constant), we are guaranteed to attain an ℓ  (cid:16)  (cid:16)  (cid:17)  c7 log  1 ε√1  c7 log  γ  ≤  −  1  −  ε(1  γ)  estimation error at most 3ε.  ∞  To summarize, a total number of O the proof.  log  1  ε(1  −  (cid:0)  γ) +log 1  1  −  epochs are suﬃcient for our purpose. This concludes  γ  (cid:1)  E Proofs of technical lemmas  E.1 Proof of Lemma 1  Fix any state-action pair (s, a)  , and let us look at β1,t(s, a), namely, the (s, a)-th entry of  ∈ S × A  t  t  β1,t = γ  Λj  Λi  I  −  P  V ⋆.  Pi − (cid:0)  (cid:1)  (cid:1)  i=1 X  j=i+1 Y  (cid:0)  For convenience of presentation, we abuse the notation to let Λj(s, a) denote the (s, a)-th diagonal entry of the diagonal matrix Λj, and Pt(s, a) (resp. P (s, a)) the (s, a)-th row of Pt (resp. P ). In view of the deﬁnition (35), we can write  t  t  β1,t(s, a) = γ  1  i=1 X  j=i+1 Y  (cid:0)  Λj(s, a)  Λi(s, a)  Pi(s, a)  −  (cid:1)  (cid:0)  P (s, a)  V ⋆.  −  (cid:1)  (94)  27  As it turns out, it is convenient to study this expression by deﬁning  tk(s, a) := the time stamp when the trajectory visits (s, a) for the k-th time  (95)  and  k { namely, the total number of times — during the ﬁrst t iterations — that the sample trajectory visits (s, a). With these in place, the special form of Λj (cf. (29)) allows us to rewrite (94) as  Kt(s, a) := max  tk(s, a)  (96)  ≤  }  t  ,  |  β1,t(s, a) = γ  Kt(s,a)  (1  −  Xk=1  η)Kt(s,a)  kη  −  Ptk+1(s, a)  P (s, a)  V ⋆.  −  (cid:0)  (cid:1)  (97)  where we suppress the dependency on (s, a) and write tk := tk(s, a) to streamline notation. The main step thus boils down to controlling (97).  Towards this, we claim that: there exists some constant c > 0 such that with probability at least 1  K  (1  −  η)K  kη  −  Ptk+1(s, a)  P (s, a)  −  V ⋆  ≤  c  η log  r  (cid:16)  T  |S||A| δ  V ⋆  k∞  k  (cid:17)  Xk=1  (cid:12) (cid:12) (cid:0) (cid:12) (cid:12) holds simultaneously for all (s, a) (cid:12) Recognizing the trivial bound Kt(s, a) the expression (97), we arrive at  ∈ S × A ≤ ≤  t  (cid:1)  (cid:12) (cid:12) (cid:12) (cid:12) K (cid:12)  < 1. and all 1 T (by construction (96)) and substituting the bound (98) into  T , provided that 0 < η log  |S||A| δ  ≤  ≤  T  (cid:1)  (cid:0)  δ,  −  (98)  (s, a)  ∀  ∈ S × A  :  |  β1,t(s, a)  | ≤  c  η log  r  T  |S||A| δ  (cid:16)  k (cid:17)  V ⋆  ,  k∞  (99)  thus concluding the proof of this lemma. It remains to validate the inequality (98).  Proof of the inequality (98). Before proceeding, we introduce some additional notation. Let VarP (V )⋆ ∈ R|S||A| be a vector whose (s, a)-th entry is given by the variance of V ⋆ w.r.t. the transition probability Ps,a( ·  ) from state s when action a is taken, namely,  (s, a)  ∀  , ∈ S × A  VarP (V ⋆)  (s,a) :=  Ps,a(s′)  V ⋆(s′)  (cid:2)  (cid:3)  Xs′ ∈S  (cid:0)  (cid:1)  2  −  (cid:16) Xs′ ∈S  Ps,a(s′)V ⋆(s′) (cid:17)  2  .  We ﬁrst make the observation that: for any ﬁxed integer K > 0, the following vectors  Ptk+1(s, a)  {  1  |  ≤  k  ≤  K  }  are identically and independently distributed. To justify this observation, let us denote by Ps,a( · transition probability from state s when action a is taken. For any i1,  , one obtains  (100)  ) the  P  1 stk+1 = ik ( K) } ∀ { 1 stk+1 = ik ( ∀  ≤  ≤  =  P  k  {  = P  1 stk+1 = ik ( ∀ {  k  K 1) and tK = m and sm+1 = iK}  −  ≤  ≤  , iK ∈ S  · · 1) and stK +1 = iK}  k  k  ≤  ≤  K  ≤  −  K  ≤  −  P  1 stk+1 = ik ( ∀ {  1) and tK = m  P  }  sm+1 = iK | {  sm = s, am = a  }  P  1 stk+1 = ik ( ∀  {  ≤  k  ≤  K  −  1) and tK = m  }  m>0 X  (i) =  m>0 X = Ps,a(iK)  m>0 X  = Ps,a(iK)P  − where (i) holds true from the Markov property as well as the fact that tK is an iteration in which the trajectory visits state s and takes action a. Invoking the above identity recursively, we arrive at  ≤  ≤  1 stk+1 = ik ( ∀ {  k  K  ,  1) }  P  1 stk+1 = ik ( ∀ {  ≤  k  K) }  ≤  =  Ps,a(ij),  K  j=1 Y  28  (101)  meaning that the state transitions happening at times distribution Ps,a( ·  ). This clearly demonstrates the independence of  , tK} {  · ·  t1,  {  Ptk+1(s, a)  1  k  K  . }  ≤  ≤  |  With the above observation in mind, we resort to the Bernstein inequality to bound the quantity of  are independent, each following the  interest (which has zero mean). To begin with, the variance parameter can be characterized by  Var  K  \" Xk=1  (1  −  η)K  kη  −  Ptk+1(s, a)  P (s, a)  V ⋆  −  (cid:0)  (cid:1)  K  #  (1  K  =  Xk=1 = η2  −  (1  η)2K  2kη2Var  −  Ptk+1(s, a)  (cid:2)(cid:0) 2kVarPs,a  V ⋆  η)2K  −  P (s, a)  V ⋆  −  (cid:1)  (cid:3)  −  Xk=1 η2VarPs,a  ≤  (cid:2) η)j =  −  (cid:3)  1  −  η2 (1  η)  −  VarPs,a  V ⋆  (cid:2)  (cid:3)  V ⋆  ∞  (1  j=0 X (cid:3) =: σ2  K .  In addition, each term in the summation clearly satisﬁes  (cid:2)  (cid:3)  = ηVarPs,a  (cid:2) V ⋆  (1  −  η)K  kη  −  Ptk (s, a)  P (s, a)  V ⋆  −  V ⋆  2η  k  ≤  k∞  =: D,  1  k  ≤  ≤  K.  As a consequence, invoking the Bernstein inequality implies that  (cid:0)  (cid:1)  (cid:12) (cid:12)  (cid:12) (cid:12)  K  Xk=1  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  η)K  kη  −  Ptk (s, a)  (1  −  P (s, a)  −  (cid:0) V ⋆  2 ∞  k  ≤  ˜c  η r  k  log  T  |S||A| δ  V ⋆  (cid:1)  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) + 2η  ˜c  ≤  σ2 K log  r  T  |S||A| δ  (cid:16)  + D log  (cid:17)  (cid:16)  T  |S||A| δ  !  (cid:17)  V ⋆  k  k∞  log  T  |S||A| δ  (cid:16)  !  (cid:17)  (cid:17)  ≤  3˜c  η log  r  |S||A| δ  (cid:16)  (cid:16) T  k (cid:17) δ  V ⋆  k∞  (102)  2  with probability exceeding 1 V ⋆ k k ∞ all 1 ≤ ≤ over all (s, a)  , and the last line holds if 0 < η log K  and all 1  |S||A|  −  ∈ S × A  ≤  ≤  T then reveals that: with probability at least 1 K  T . This concludes the proof.  −  (cid:0)  (cid:1)  T , where the second line relies on the simple bound VarPs,a  V ⋆  < 1. Taking the union bound over all (s, a)  ≤ and (cid:3) δ, the inequality (102) holds simultaneously  (cid:2) ∈ S × A  T  |S||A| δ  E.2 Proof of Lemma 2 and Lemma 7  Proof of Lemma 2. Let β3,t = of β3,t (resp. ∆0). From the deﬁnition of β3,t, it is easily seen that  −  I  t j=1  Λj  ∆0. Denote by β3,t(s, a) (resp. ∆0(s, a)) the (s, a)-th entry  Q  (cid:0)  (cid:1)  β3,t(s, a) |  |  = (1  −  η)Kt(s,a)  ∆0(s, a)  ,  (103)  where Kt(s, a) denotes the number of times the sample trajectory visits (s, a) during the iterations [1, t] (cf. (96)). By virtue of Lemma 5 and the union bound, one has, with probability at least 1  δ, that  (cid:12) (cid:12)  (cid:12) (cid:12)  −  and all t obeying 443τmix simultaneously over all (s, a) µmin relation (103) establishes that, with probability greater than 1  ∈ S × A  |S||A| δ  log 4 δ,  −  Kt(s, a)  tµmin/2  ≥  (104)  T  t  ≤  ≤  T . Substitution into the  holds uniformly over all (s, a)  ∈ S × A  β3(s, a)  (1  1 2 tµmin  η)  ∆0(s, a)  |  | ≤  − (cid:12) and all t obeying 443τmix (cid:12) µmin  log 4  (cid:12) (cid:12) |S||A| δ  T  t  ≤  ≤  T , as claimed.  .  (105)  29  Proof of Lemma 7. The proof of this lemma is essentially the same as that of Lemma 2, except that we use instead the following lower bound on Kt(s, a) (which is an immediate consequence of Lemma 6)  for all t > tcover,all. Therefore, replacing tµmin with t/tcover,all in the above analysis, we establish Lemma 7.  Kt(s, a)  t tcover,all  ≥  j  ≥  k  t 2tcover,all  (106)  E.3 Proof of Lemma 3  We prove this fact via an inductive argument. The base case with t = 0 is a consequence of the crude bound (44). Now, assume that the claim holds for all iterations up to t 1, and we would like to justify it for the t-th iteration as well. Towards this, deﬁne  −  h(t) :=  ∆0k∞ , k γ)ε, (1 ( −  if t tth, ≤ if t > tth.  (107)  Recall that (1 η) induction hypotheses indicates that  (1  ≤  −  −  1 2 tµmin  γ)ε for any t  tth. Therefore, combining the inequality (42) with the  ≥  ∆t| ≤  |  γ  = γ  t  t  i=1 X t  j=i+1 Y t  i=1 X  j=i+1 Y  Λj)Λi1  (I  −  V ⋆ τ1k k∞ γ 1  − (cid:18)  + ui  −  1 + ε  (cid:19)  V ⋆  + τ1k  k∞  1 + h(t)1  (I  −  Λj)Λi1ui −  1 + γ  t  t  i=1 X  j=i+1 Y  Λj)Λi1  (I  −  V ⋆ τ1k k∞ γ 1  −  (cid:18)  + ε  (cid:19)  Taking this together with the inequality (46b) and rearranging terms, we obtain  V ⋆  + τ1k  k∞  1 + h(t)1.  ∆t| ≤  |  =  =  ≤  t  t  γ  j=i+1 Y  i=1 X V ⋆ τ1k k∞ γ 1  − V ⋆ τ1k k∞ 1 γ − V ⋆ τ1k k∞ γ 1  −  (I  −  Λj)Λi1ui −  1 +  V ⋆ k∞ γ  γτ1k 1 −  1 + γε1 + τ1k  V ⋆  k∞  1 + h(t)1  1 + γε1 + γ  t  t  i=1 X  j=i+1 Y  (I  −  Λj)Λi1ui −  1 + h(t)1  1 + γε1 + vt + (1  γ)ε 1 {  −  t > tth  1  }  1 + ε1 + vt,  where we have used the deﬁnition of vt in (47). This taken collectively with the deﬁnition ut = establishes that  ∆tk∞ ≤  k  as claimed. This concludes the proof.  E.4 Proof of Lemma 4  V ⋆ τ1k k∞ γ 1  −  + ε + ut  (108)  vtk∞  k  We shall prove this result by induction over the index k. To start with, consider the base case where k = 0 γ) = w0. In fact, and t < tth + tframe. By deﬁnition, it is straightforward to see that u0 ≤ k − repeating our argument for the crude bound (see Section 6.2.2) immediately reveals that  ∆0k∞  /(1  t ∀  ≥  0 :  ut ≤  ∆0k∞ k γ 1 −  = w0,  (109)  thus indicating that the inequality (50) holds for the base case. inequality (50) holds up to k  1, and would like to extend it to the case with all t obeying  In what follows, we assume that the  −  t tth − tframe  = k.  (cid:4)  (cid:5)  30  Let us focus on the case when t = tth + ktframe; the case with t = tth + ktframe + j (1  j < tframe) follows from an analogous argument and is omitted for brevity. In view of the deﬁnition of vt (cf. (47)) as well as our induction hypotheses, one can arrange terms to derive  ≤  vtth+ktframe = γ  = γ  γ  ≤  tth+ktframe  tth+ktframe  (I  −  Λj)Λi1ui −  1  tth+ktframe  j=i+1 Y  i: max  ⌊ (cid:8)  i−1−tth X tframe ⌋  ,0  =s  j=i+1 Y  tth+ktframe  (cid:9)  i=1 X 1  −  k  s=0 ( X  k  1  −  s=0 ( X  i: max  i−1−tth X tframe ⌋  ,0  =s  j=i+1 Y  (I  −  Λj)Λi1ui −  1  )  Λj)Λi1  (I  −  ws,  )  (110)  Λj)Λi1.  −  ⌊ (cid:8) where the last inequality follows from our induction hypotheses and the non-negativity of (I  (cid:9)  Given any state-action pair (s, a)  , let us look at the (s, a)-th entry of vtth+ktframe — denoted by vtth+ktframe (s, a), towards which it is conevnient to pause and introduce some notation. Recall that N j i (s, a) has been used to denote the number of visits to the state-action pair (s, a) between iteration i and iteration j (including i and j). To help study the behavior in each timeframe, we introduce the following quantities  ∈ S × A  N k s  −  1  := N j  i (s, a)  with i = tth + stframe + 1, j = tth + ktframe  (111)  for every s and the (k  1; in words, N k s  k 1)-th frame. Lemma 5 tells us that, with probability at least 1  −  −  1  stands for the total number of visits to (s, a) between the s-th frame  2δ,  −  ≤ −  N k  1 − s ≥  (k  −  s)µframe  with µframe =  1 2  µmintframe,  (112)  which actually holds uniformly over all state-action pairs (s, a). Armed with this set of notation, it is straightforward to use the expression (110) to verify that  vtth+ktframe (s, a)  k  1  −  s=0 X 1 k −  γ  ≤  = γ  n  (1  s=0 (cid:16) X η)N k−1 A litter algebra further leads to  where we denote αs := (1  −  s  η  (1  η)N k−1 s −  1 + (1  η)N k−1 s −  2 +  −  + (1  −  · ·  −  η)N k−1  s+1  ws  o  η)N k−1  s+1  −  (1  −  −  η)N k−1  s  ws =: γ  (cid:17)  k  1  −  s=0 X  for any s  k  −  ≤  1 and αk := 1.  (αs+1 −  αs) ws,  (113)  γ  k  1  −  s=0 X  (αs+1 −  αs) ws = γ(αkwk  α0w0) + γ  1 −  −  k  1  −  s=1 X  αs (ws  −  1 −  ws) .  (114)  Thus, in order to control the quantity vtth+ktframe (s, a), it suﬃces to control the right-hand side of (114), for which we start by bounding the last term. Plugging in the deﬁnitions of ws and αs yields  1 γ − ∆0k∞  k  k  1  −  s=1 X  αs (ws  −  1 −  ws) =  k  1  −  s=1 X  η)N k−1  s  (1  (1  −  ρ)s  −  1ρ  ρ  ≤  −  k  1  −  s=1 X  (1  −  η)(k  −  s)µframe (1  ρ)s  −  1,  −  where the last inequality resuls from the fact (112). Additionally, direct calculation yields  k  −  1 (1  ρ  s=1 X  −  η)(k  −  s)µframe (1  ρ)s  −  1 = ρ(1  η)(k  −  1)µframe  −  −  k  1  −  s=1 (cid:16) X  1  s  −  ρ − η)µframe  1  −  (1  (cid:17)  31  = ρ(1  = ρ(1  ρ(1  ≤  −  −  −  where the last inequality makes use of the fact that  η)(k  −  1)µframe  1  − 1  k  1  −  1 ρ − η)µframe 1 ρ (cid:1) − η)µframe η)(k − η)µframe  − −  − (1 (1 1  −  η)µframe  (1  −  (1  (cid:0) − 1  −  ρ)k (1  −  − − ρ) − ρ)k (1  − −  −  (1 ρ)  η)µframe (1  η)µframe  (1  −  1)µframe  ,  (115)  ρ)  (1  −  −  (1  −  η)µframe = 1  = γ  (1  γ)(1  − 1 {  − (1  −  −  (1  − η)µframe  −  =  }  η)µframe ) γ  γ  1  −  (1  − 0.  − ρ  ≥  η)µframe  (116)  Combining the inequalities (113), (114) and (115) and using the fact α0w0 ≥  0 give  k  1  −  γ  ≤  s=1 X ∆0 1 ≤ (cid:13) (cid:13)  ∞ γ (cid:13) − (cid:13)  vtth+ktframe (s, a)  αs (ws  −  1 −  ws) + γαkwk  1  −  γρ(1  (cid:26)  −  η)µframe  (1  1  −  ρ)k (1  (1 ρ)  − −  −  (cid:27) wk. Note that the observation (116) implies  −  η)µframe  −  + γ(1  ρ)k  −  1  .  (117)  We are now ready to justify vtth+ktframe (s, a)  ≤  γ  (1  ρ(1 ρ)  η)µframe (1  −  − −  −  η)µframe  ρ(1  = γ  η)µframe γ ρ  − γ 1  −  = (1  γ)(1  −  −  η)µframe .  This combined with the bound (117) yields  vtth+ktframe (s, a)  ∆0 ∞ γ 1 ≤ (cid:13) (cid:13) − (cid:13) (cid:13) ∆0 1 ≤ (cid:13) (cid:13) = (1  ∞ γ (cid:13) − (cid:13) ρ)k  −  γ)(1  (1  −  −  η)µframe (1  −  ρ)k  −  1 + γ(1  ρ)k  −  1  −  γ)(1  −  −  η)µframe  (1  ρ)k  −  1  −  = wk,  (cid:1)  (cid:9)  (118)  (cid:8) γ + (1  (cid:0)  ∆0 1 (cid:13) (cid:13)  ∞ γ (cid:13) − (cid:13)  where the last line follows from the deﬁnition of ρ (cf. (32d)). Since the above inequality holds for all state-action pair (s, a), we conclude that  utth+ktframe =  vtth+ktframe  wk.  ∞ ≤  (119)  We have thus ﬁnished the proof for the case when t = tth + ktframe. As mentioned before, the case with 1) can be justiﬁed using the same argument. As a consequence, we have = k, which together with the induction argument  t = tth +ktframe +j (j = 1, . . . , tframe established the inequality (50) for all t obeying completes the proof of this lemma.  tth t − tframe  −  (cid:13) (cid:13)  (cid:13) (cid:13)  (cid:4)  (cid:5)  E.5 Proof of Lemma 8 Recalling that 0  t i=1  t j=i+1(I  ≤  Λj)Λi1  −  ≤  1 (cf. (46b)), we obtain  P Q h0,tk∞ ≤  k  γ  t  t  I  i=1 X  j=i+1 Y  (cid:0)  Λj  Λi  −  (cid:1)  (cid:13) (cid:13) (cid:13)  As a result, it remains to upper bound  1 (cid:13) (cid:13) (cid:13)  P (  −  P  V  γ  P (  P  V  −  .  ∞  ∞ ≤  (120)  e  (cid:13) (cid:13) .  (cid:1)  (cid:13) (cid:13)  (cid:13) (cid:13)  e  (cid:1)  (cid:13) (cid:13)  P (  −  P  V  (cid:13) (cid:13)  e  (cid:1)  ∞  (cid:13) (cid:13) 32  Suppose that  P is constructed using N consecutive sample transitions. Without loss of generality, assume  that these N sample transitions are the transitions between the following N + 1 samples  e  (s0, a0), (s1, a1), (s2, a2),  , (sN , aN ).  · ·  Then the (s, a)-th row of  P — denoted by  P (s, a) — is given by  1 KN (s, a)  N e  −  1  i=0 X  e Pi+1(s, a)V 1  {  (si, ai) = (s, a) }  =  1 KN (s, a)  KN (s,a)  i=1 X  Pti+1(s, a)V ,  (121)  P (s, a) =  e  where Pi is deﬁned in (30), and Pi(s, a) denotes its (s, a)-th row. Here, KN (s, a) denotes the total number of visits to (s, a) during the ﬁrst N time instances (cf. (96)), and tk := tk(s, a) denotes the time stamp when the trajectory visits (s, a) for the k-th time (cf (95)).  In view of our derivation for (101), the state transitions happening at time t1, t2, for any given integer k > 0. This together with the Hoeﬀding inequality implies that  · ·  , tk are independent  P  (  k  1 k (cid:12) i=1 (cid:12) X (cid:12) (cid:12) (cid:12)  Pti+1(s, a)  −  (cid:0)  P (s, a)  V  (cid:12) (cid:12) (cid:1) (cid:12) (cid:12) one has (cid:12)  δ  −  |S||A|  Consequently, with probability at least 1  τ  ≥  ) ≤  2 exp  −  (cid:26)  2  kτ 2 V  k  k  .  2 ∞ (cid:27)  (122)  k  Pti+1(s, a)  2 log  2N  |S||A| δ  P (s, a)  V  −  ≤ s  k  (cid:0)  V  ,  1  k  ≤  ≤  N.  (cid:1)  ∞  i=1 X Recognizing the simple bound KN (s, a) k is replaced by KN (s, a). Conditioning on these KN (s, a), applying the union bound over all (s, a) we obtain  N , the above inequality holds for each state-action pair (s, a) when , ∈ S × A  (cid:13) (cid:13)  (cid:13) (cid:13)  ≤  (cid:1)  (cid:0)  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  1 k  (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  P (  −  P )V  ∞ ≤  max  (s,a)  s  ∈S×A  2 log  2N  |S||A| δ  KN (s, a)  (cid:0)  (cid:1)  V  ∞  (123)  with probability at least 1 In addition, for any N  (cid:13) (cid:13) − ≥ (s, a) is visited at least N µmin/2 times, namely, KN (s, a) yields  e δ. tframe, Lemma 5 guarantees that with probability 1  (cid:13) (cid:13)  (cid:13) (cid:13)  (cid:13) (cid:13)  ≥  2δ, each state-action pair 1 2 N µmin for all (s, a). This combined with (124)  −  P (  −  P )V  ∞ ≤ s  (cid:13) (cid:13)  e  (cid:13) (cid:13)  ≤ s  ≤ s  4 log  2N  |S||A| δ N µmin (cid:0)  4 log  2N  |S||A| δ N µmin (cid:0)  4 log  2N  |S||A| δ N µmin (cid:0)  (cid:1)  (cid:1)  (cid:1)  V  ∞  (cid:13) (cid:13)  (cid:13) (cid:13) V  V ⋆  +  V ⋆  ∞  ∞  −  (cid:0)(cid:13) (cid:13) V  (cid:13) (cid:13)  ∞  +  V ⋆  −  (cid:13) (cid:13) 1  (cid:13) (cid:13)  γ s  1  −  (cid:1) 4 log  2N  |S||A| δ N µmin (cid:0)  (124)  (cid:1)  (cid:13) (cid:13)  (cid:13) (cid:13)  with probability at least 1 last inequality follows from  −  3δ, where the second inequality follows from the triangle inequality, and the V ⋆ γ . Putting this together with (120) concludes the proof.  1  1  ∞ ≤  −  E.6 Proof of Lemma 6  (cid:13) (cid:13)  (cid:13) (cid:13)  For notational convenience, set tl := tcoverl, and deﬁne  Hl :=  (s, a)  ∃  n  ∈ S × A  that is not visited within  tl, tl+1  (cid:0)  (cid:3) o  33  for any integer l  ≥  0. In view of the deﬁnition of tcover, we see that for any given (s′, a′)  , ∈ S × A  {Hl | Consequently, for any integer L > 0, one can invoke the Markovian property to obtain  } ≤  (stl, atl) = (s′, a′)  P  1 2  .  (125)  P  = P  {H1 ∩ · · · ∩ HL} = P  {H1 ∩ · · · ∩ HL  −  {H1 ∩ · · · ∩ HL P 1} {HL |  {HL | H1 ∩ · · · ∩ HL  1} − (stl , atl) = (s′, a′) }  P  1}  −  P  (stl , atl) = (s′, a′) {  | H1 ∩ · · · ∩ HL  1}  −  Xs′,a′  P  P  1 2  1 2  ≤  =  P  (stl, atl ) = (s′, a′) {  | H1 ∩ · · · ∩ HL  1}  −  {H1 ∩ · · · ∩ HL  1}  −  {H1 ∩ · · · ∩ HL  1}  −  Xs′,a′ ,  where the inequality follows from (125). Repeating this derivation recursively, we deduce that  P  {H1 ∩ · · · ∩ HL} ≤  1 2L .  This tells us that  P  (s, a)  {∃  ∈ S × A  that is not visited between (0, tcover,all]  P  } ≤  which in turn establishes the advertised result by applying the union bound.  H1 ∩ · · · ∩ Hlog2 n  1 2log2  T δ  =  δ T  ,  T δ  ≤  o  References  Agarwal, A., Kakade, S., and Yang, L. F. (2019). Model-based reinforcement learning with a generative  model is minimax optimal. arXiv preprint arXiv:1906.03804.  Azar, M. G., Munos, R., Ghavamzadeh, M., and Kappen, H. (2011). Reinforcement learning with a near  optimal rate of convergence. Technical report, INRIA.  Azar, M. G., Munos, R., and Kappen, H. J. (2013). Minimax PAC bounds on the sample complexity of  reinforcement learning with a generative model. Machine learning, 91(3):325–349.  Bai, Y., Xie, T., Jiang, N., and Wang, Y.-X. (2019). Provably eﬃcient q-learning with low switching cost.  In Advances in Neural Information Processing Systems, pages 8002–8011.  Beck, C. L. and Srikant, R. (2012). Error bounds for constant step-size Q-learning. Systems & control letters,  61(12):1203–1208.  Bertsekas, D. P. (2017). Dynamic programming and optimal control (4th edition). Athena Scientiﬁc.  Bhandari, J., Russo, D., and Singal, R. (2018). A ﬁnite time analysis of temporal diﬀerence learning with  linear function approximation. In Conference On Learning Theory, pages 1691–1692.  Borkar, V. S. and Meyn, S. P. (2000). The ODE method for convergence of stochastic approximation and  reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447–469.  Brémaud, P. (2013). Markov chains: Gibbs ﬁelds, Monte Carlo simulation, and queues, volume 31. Springer  Science & Business Media.  Cai, Q., Yang, Z., Lee, J. D., and Wang, Z. (2019). Neural temporal-diﬀerence and q-learning converges to  global optima. In Advances in Neural Information Processing Systems, pages 11312–11322.  Chen, Z., Maguluri, S. T., Shakkottai, S., and Shanmugam, K. (2020). Finite-sample analysis of stochastic  approximation using smooth convex envelopes. arXiv preprint arXiv:2002.00874.  34  Chen, Z., Zhang, S., Doan, T. T., Maguluri, S. T., and Clarke, J.-P. (2019). Performance of Q-learning with  linear function approximation: Stability and ﬁnite-time analysis. arXiv preprint arXiv:1905.11425.  Dalal, G., Szörényi, B., Thoppe, G., and Mannor, S. (2018a). Finite sample analyses for TD(0) with function  approximation. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence.  Dalal, G., Thoppe, G., Szörényi, B., and Mannor, S. (2018b). Finite sample analysis of two-timescale stochastic approximation with applications to reinforcement learning. In Conference On Learning Theory, pages 1199–1233.  Dann, C. and Brunskill, E. (2015). Sample complexity of episodic ﬁxed-horizon reinforcement learning. In  Advances in Neural Information Processing Systems, pages 2818–2826.  Devraj, A. M. and Meyn, S. P. (2020). Q-learning with uniformly bounded variance: Large discounting is  not a barrier to fast learning. arXiv preprint arXiv:2002.10301.  Doan, T., Maguluri, S., and Romberg, J. (2019). Finite-time analysis of distributed TD(0) with linear function approximation on multi-agent reinforcement learning. In International Conference on Machine Learning, pages 1626–1635.  Doan, T. T., Nguyen, L. M., Pham, N. H., and Romberg, J. (2020). Convergence rates of accelerated markov  gradient descent with applications in reinforcement learning. arXiv preprint arXiv:2002.02873.  Du, S. S., Chen, J., Li, L., Xiao, L., and Zhou, D. (2017). Stochastic variance reduction methods for policy evaluation. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1049–1058. JMLR. org.  Du, S. S., Lee, J. D., Mahajan, G., and Wang, R. (2020). Agnostic Q-learning with function approximation in deterministic systems: Tight bounds on approximation error and sample complexity. arXiv preprint arXiv:2002.07125.  Du, S. S., Luo, Y., Wang, R., and Zhang, H. (2019). Provably eﬃcient Q-learning with function approxima- tion via distribution shift error checking oracle. In Advances in Neural Information Processing Systems, pages 8058–8068.  Dulac-Arnold, G., Mankowitz, D., and Hester, T. (2019). Challenges of real-world reinforcement learning.  arXiv preprint arXiv:1904.12901.  Even-Dar, E. and Mansour, Y. (2003). Learning rates for Q-learning. Journal of machine learning Research,  5(Dec):1–25.  Fan, J., Wang, Z., Xie, Y., and Yang, Z. (2019). A theoretical analysis of deep Q-learning. arXiv preprint  arXiv:1901.00137.  Ghavamzadeh, M., Kappen, H. J., Azar, M. G., and Munos, R. (2011). Speedy Q-learning. In Advances in  neural information processing systems, pages 2411–2419.  Gupta, H., Srikant, R., and Ying, L. (2019). Finite-time performance bounds and adaptive learning rate selection for two time-scale reinforcement learning. In Advances in Neural Information Processing Systems, pages 4706–4715.  Jaakkola, T., Jordan, M. I., and Singh, S. P. (1994). Convergence of stochastic iterative dynamic program-  ming algorithms. In Advances in neural information processing systems, pages 703–710.  Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. (2018). Is Q-learning provably eﬃcient? In Advances  in Neural Information Processing Systems, pages 4863–4873.  Johnson, R. and Zhang, T. (2013). Accelerating stochastic gradient descent using predictive variance reduc-  tion. In Advances in neural information processing systems, pages 315–323.  35  Kaledin, M., Moulines, E., Naumov, A., Tadic, V., and Wai, H.-T. (2020). Finite time analysis of linear  two-timescale stochastic approximation with Markovian noise. arXiv preprint arXiv:2002.01268.  Kearns, M. J. and Singh, S. P. (1999). Finite-sample convergence rates for Q-learning and indirect algorithms.  In Advances in neural information processing systems, pages 996–1002.  Khamaru, K., Pananjady, A., Ruan, F., Wainwright, M. J., and Jordan, M. I. (2020). Is temporal diﬀerence  learning optimal? an instance-dependent analysis. arXiv preprint arXiv:2003.07337.  Lee, D. and He, N. (2019). Target-based temporal diﬀerence learning. arXiv preprint arXiv:1904.10945.  Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y. (2020). Breaking the sample size barrier in model-based reinforcement learning with a generative model. arXiv preprint arXiv:2005.12900, accepted to Neural Information Processing Systems.  Lin, Y., Qu, G., Huang, L., and Wierman, A. (2020). Distributed reinforcement learning in multi-agent  networked systems. arXiv preprint arXiv:2006.06555.  Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540):529–533.  Mou, W., Li, C. J., Wainwright, M. J., Bartlett, P. L., and Jordan, M. I. (2020). On linear stochas- arXiv preprint  tic approximation: Fine-grained Polyak-Ruppert and non-asymptotic concentration. arXiv:2004.04719.  Paulin, D. (2015). Concentration inequalities for Markov chains by Marton couplings and spectral methods.  Electronic Journal of Probability, 20.  Qu, G. and Wierman, A. (2020). Finite-time analysis of asynchronous stochastic approximation and Q-  learning. Conference on Learning Theory.  Shah, D. and Xie, Q. (2018). Q-learning with nearest neighbors.  In Advances in Neural Information  Processing Systems, pages 3111–3121.  Sidford, A., Wang, M., Wu, X., Yang, L., and Ye, Y. (2018a). Near-optimal time and sample complexities for solving Markov decision processes with a generative model. In Advances in Neural Information Processing Systems, pages 5186–5196.  Sidford, A., Wang, M., Wu, X., and Ye, Y. (2018b). Variance reduced value iteration and faster algorithms for solving Markov decision processes. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 770–787. SIAM.  Srikant, R. and Ying, L. (2019). Finite-time error bounds for linear stochastic approximation and TD  learning. In Conference on Learning Theory, pages 2803–2830.  Strehl, A. L., Li, L., Wiewiora, E., Langford, J., and Littman, M. L. (2006). PAC model-free reinforcement  learning. In Proceedings of the 23rd international conference on Machine learning, pages 881–888.  Sun, T., Sun, Y., Xu, Y., and Yin, W. (2020). Markov chain block coordinate descent. Computational  Optimization and Applications, pages 1–27.  Sutton, R. S. (1988). Learning to predict by the methods of temporal diﬀerences. Machine learning, 3(1):9–  44.  Szepesvári, C. (1998). The asymptotic convergence-rate of Q-learning. In Advances in Neural Information  Processing Systems, pages 1064–1070.  Tsitsiklis, J. N. (1994). Asynchronous stochastic approximation and Q-learning. Machine learning, 16(3):185–  202.  36  Wainwright, M. J. (2019a). Stochastic approximation with cone-contractive operators: Sharp ℓ  Q-learning. arXiv preprint arXiv:1905.06265.  bounds for  ∞  Wainwright, M. J. arXiv:1906.04697.  (2019b).  Variance-reduced Q-learning is minimax optimal.  arXiv preprint  Wang, Y., Dong, K., Chen, X., and Wang, L. (2020). Q-learning with UCB exploration is sample eﬃcient  for inﬁnite-horizon MDP. In International Conference on Learning Representations.  Watkins, C. J. and Dayan, P. (1992). Q-learning. Machine learning, 8(3-4):279–292.  Watkins, C. J. C. H. (1989). Learning from delayed rewards.  Weng, B., Xiong, H., Zhao, L., Liang, Y., and Zhang, W. (2020a). Momentum Q-learning with ﬁnite-sample  convergence guarantee. arXiv preprint arXiv:2007.15418.  Weng, W., Gupta, H., He, N., Ying, L., and Srikant, R. (2020b). Provably-eﬃcient double Q-learning. arXiv  preprint arXiv:2007.05034.  Xu, P. and Gu, Q. (2020). A ﬁnite-time analysis of Q-learning with neural network function approximation.  accepted to International Conference on Machine Learning.  Xu, T., Wang, Z., Zhou, Y., and Liang, Y. (2020). Reanalysis of variance reduced temporal diﬀerence  learning. ICLR, arXiv preprint arXiv:2001.01898.  Xu, T., Zou, S., and Liang, Y. (2019). Two time-scale oﬀ-policy TD learning: Non-asymptotic analysis over  Markovian samples. In Advances in Neural Information Processing Systems, pages 10633–10643.  Yang, L. and Wang, M. (2019). Sample-optimal parametric Q-learning using linearly additive features. In  International Conference on Machine Learning, pages 6995–7004.  Zou, S., Xu, T., and Liang, Y. (2019). Finite-sample analysis for SARSA with linear function approximation.  In Advances in Neural Information Processing Systems, pages 8665–8675.  37Meta-Learning Dynamics Forecasting Using Task Inference  Rui Wang * 1 Robin Walters * 2 Rose Yu 1  1 2 0 2  b e F 0 2  ]  G L . s c [  1 v 1 7 2 0 1 . 2 0 1 2 : v i X r a  Abstract  Current deep learning models for dynamics fore- casting struggle with generalization. They can only forecast in a speciﬁc domain and fail when applied to systems with different parameters, ex- ternal forces, or boundary conditions. We pro- pose a model-based meta-learning method called DyAd which can generalize across heterogeneous domains by partitioning them into separate sub- domains, each with a different task. DyAd has two parts: a prediction network which learns the shared dynamics of the entire domain, and an encoder which infers the parameters of the task. The encoder adapts the prediction network dur- ing inference time using adaptive instance nor- malization and a new layer, AdaPad, speciﬁcally designed for boundary conditions. The encoder can also use any weak supervision signals that can help distinguish different tasks, allowing the in- corporation of additional domain knowledge. Our model outperforms a variety of state-of-the-art approaches on both turbulent ﬂow and real-world ocean data forecasting tasks.  1. Introduction  Learning dynamical systems with deep neural networks has shown great success in a wide range of systems from ﬂuid mechanics to neural dynamics (Tompson et al., 2017; Chen et al., 2018; Kolter & Manek, 2019; Zoltowski et al., 2020; Li et al., 2021). However, the main limitation of previous work modeling dynamics with neural networks is very lim- ited generalizability. Most approaches treat dynamics data as a time series and train on past data in order to predict fu- ture data. Thus a new model must be trained to predict each speciﬁc system. It is imperative to develop generalizable deep learning models for dynamical systems that can learn and generalize well over a large heterogeneous domain.  Equal contribution 1Computer Science and Engineering, Uni- versity of California San Diego, USA 2Khoury College of Com- puter Science, Northeastern University, Boston, USA. Correspon- dence to: Rose Yu <roseyu@eng.ucsd.edu>.  Figure 1. Comparison of DyAd applied to two inputs of ﬂuid turbu- lence, one with small external forcing and one with larger external forces. The encoder infers the time-shift invariant characteristic variable z which is used to adapt the prediction network.  Meta-learning (Thrun & Pratt, 1998; Baxter, 1998; Finn et al., 2017), or learning to learn, improves generalization by learning multiple tasks from the environment. The recent development in meta-learning has been successfully applied to few-shot classiﬁcation (Munkhdalai & Yu, 2017), active learning (Yoon et al., 2018), and reinforcement learning (Gupta et al., 2018). However, meta-learning in the context of forecasting high-dimensional physical dynamics has not been studied before. The challenges with meta-learning dy- namical systems are unique in that (1) we need to efﬁciently infer the latent parameters of the dynamical system from observed time series data, and (2) we need to account for changes in unknown initial and boundary conditions.  Our approach is inspired by the fact that similar dynamical systems may share time-invariant characteristic variables. Even the slightest change in these variables may lead to vastly different phenomena. For example, in climate science, ﬂuids are governed by a set of differential equations called Navier-Stokes equations. Some variables such as kinematic viscosity, which measures a ﬂuid’s internal resistance to deformation, and external forces, such as gravity, determine the ﬂow characteristics. By inferring these variables, we can model diverse system behavior from smoothly ﬂowing water to atmospheric turbulence.  Meta-Learning Dynamics Forecasting Using Task Inference  We propose a model-based meta-learning method, called DyAd, which can rapidly adapt to dynamics systems with different parameters. DyAd has two parts, an encoder g and a prediction network f . The encoder maps different dynam- ical systems to time-invariant characteristic variables such as constants of motion, boundary conditions, and external forces which vary from system to system. The prediction network f then takes the characteristic variables and the past system states to forecast the future system state. Due to the information in the characteristic variable, the predic- tion network has the ﬂexibility to adapt to a wide range of systems with heterogeneous dynamics.  Unlike gradient-based meta-learning techniques such as MAML (Finn et al., 2017), DyAd automatically adapts during inference using an encoder and does not require any retraining. Similar to other model-based meta-learning methods such as MetaNets (Munkhdalai & Yu, 2017), we employ a two-part design with an adaptable learner which receives task-speciﬁc weights. However, for time series forecasting, since input and output come from the same do- main, a support set of additional labeled data is unnecessary to deﬁne the task. The encoder can infer the task directly from query input.  Our contributions include:  A new model-based meta-learning method (DyAd) for forecasting in dynamical systems which adapts to each input using task inference.  An encoder capable of extracting the time-invariant part of a dynamical system using time-shift invariant model structure and loss.  A new layer AdaPad designed for boundary conditions. • Improved accuracy on heterogeneous domains such as ﬂuid ﬂow and sea temperature prediction relative to models trained separately on homogeneous domains or on the entire domain but without task inference.  Good generalization to new tasks with parameters out-  side the training distribution.  2. Methods  want to learn a map f such that:  f : (xt−l+1, . . . , xt) −→ (xt+1, . . . xt+h)  (2)  Here l is the length of the input series and h is the forecasting horizon in the output.  Existing approaches for dynamics forecasting only predict future data for a speciﬁc system as a single task. The re- sulting model often generalizes poorly to different system dynamics. Thus a new model must be trained to predict for systems with different dynamics.  To perform meta-learning, we learn multiple forecasting tasks simultaneously where each task is identiﬁed by some parameters c ⊂ ψ, representing time-invariant characteris- tics such as constants of motion, external forces, and bound- ary conditions. Here we use c for a subset of parameters because we usually do not have the full knowledge of the system dynamics. The parameters c only partially describe the characteristics in the data.  Consider, for example, the domain of ﬂuid dynamics. Here, xt is the velocity ﬁeld of the ﬂuid ﬂow at time t. Different types of ﬂuid ﬂows may be classiﬁed and labeled according to their degree of turbulence by taking c to be Reynolds number, average vorticity, average magnitude, or a vector of all three. Fluid ﬂows with different Reynolds numbers may have very different dynamics and characteristics.  Formally, let µ be the distribution over X × Y representing the function f : X → Y where X = Rd×l and Y = Rd×h. Our main assumption is that the domain X may be parti- tioned into separate tasks X = ∪c∈CXc labeled by different task parameter c ∈ C. Note that the space C may be either discreet or continuous. Denote the task map g : X → C taking x ∈ Xc to c. Let µc be the conditional distribution of (x, y) ∼ µ such that g(x) = c.  During training, the model is presented with data from a subset of tasks {ck} ∼ C. Our goal is to learn the func- tion f : X → Y over the whole domain X which can thus generalize across all tasks c ∈ C.  2.1. Meta-learning in dynamics forecasting  2.2. DyAd: Dynamic Adaptation Network  A dynamical system is governed by a set of differential equations:  (cid:8)ξi(x, ˙x, ¨x, . . . ; ψ) = 0(cid:9)  (1)  where x ∈ Rd is a d-dimensional state of the system and ψ are the parameters. Oftentimes, the dynamics parameter ψ can represent different system coefﬁcients, external forces or boundary conditions.  The problem of dynamics forecasting is that given a set of series from the system in (1), {(x1, x2, . . . , xt)(i)}n i=1, we  We propose a model-based meta-learning approach for dy- namics forecasting. Our approach infers the forecasting task as a latent variable and uses it to adapt the prediction.  Task Inference. Given multiple forecasting tasks, two ap- proaches to modeling f are either to learn all tasks at once or one task at a time. If the training set S = {(x(i), y(i))} has good and uniform coverage of the different classes in X , that is, if S is a sampled from µ i.i.d and is large enough, then a single high capacity neural network may model f well. However, if distribution µ is highly heterogeneous for different c and if the training set is not i.i.d., then a single  Meta-Learning Dynamics Forecasting Using Task Inference  allows generalization to tasks not in the training set.  Our characteristic latent variable z bears afﬁnity with the “style” vector in style transfer techniques. Rather than aes- thetic style in images, our latent variable represents the characteristics of the dynamics that are time-invariant. In- terestingly, ﬁnding characteristic numbers such as Reynolds number is also a central topic in ﬂuid mechanics. In the style-transfer literature, a generative network is guided by the use of an external style though adaptive instance normal- ization between convolutional layers (Karras et al., 2019; Huang & Belongie, 2017). We use adaptive instance normal- ization to incorporate the latent variable zc into ResNet to create an adaptable prediction network fc = f (·, zc).  Partial Disentanglement. The time-invariant characteristic latent variable may be viewed as partial disentanglement of the system state. As suggested by (Locatello et al., 2019; Nie et al., 2020), our disentanglement method is guided by inductive bias and weak supervision. Unlike complete disentanglement, as in e.g. (Massague et al., 2020), in which the latent system state is factored into time-invariant and time-varying components (¯z, ˜z), our latent variable is only ¯z. Nonetheless, ¯z provides a strong signal to the prediction network which is useful for generalization.  2.3. Time-shift Invariant Encoder  We assume that c are parameters of the system which are time-invariant, such as constants of motion or ﬁxed parame- ters of the dynamical system. In order to enforce this induc- tive bias, we encode time-invariance both in the architecture and training objective of the encoder.  The encoder is implemented using 4 layers of 3D convolu- tion as seen in Figure 2. We convolve across two spatial and one temporal dimension. After this there is a global mean- pooling layer and fully connected layer which outputs the  Figure 3. Detail of one block of the prediction network.  Figure 2. Detail of the DyAd encoder. The conv3D layers are shift equivariant and global mean pooling is shift invariant. The network is approximately invariant to spatial and temporal shifts.  model may struggle with generalization.  We propose to learn the function f in two stages, that is, by ﬁrst inferring the task c from the input data x and then learn- ing different specialized prediction functions fc : Xc → Y for each task c ∈ C. We introduce a latent variable zc as a high-dimensional representation for the task c and infer this latent variable from the input data x. As system dynamics are encoded in x, we can use the same single input x to infer the task c and then predict the output label ˆy = fc(x).  Model-Based Adaptation. Care must be taken with how the functions fc are implemented. A naive approach in which each fc has its own weights and is trained separately would be highly impractical if the number of tasks |C| is large. Each fc would have very little training data and the combined model would have a large number of weights. In essence, treating each c completely separately fails to take advantage of the similarity between tasks fc.  Our proposal strikes a balance between these two extremes. As shown in Figure 1, our model consists of two parts: an encoder g and prediction network f . The encoder maps the input x to a high-dimensional latent variable zc that characterizes the dynamics of the system, hence the task c. We then use zc to adapt the prediction network f to the speciﬁc dynamics of the domain c, i.e., model y = fc(x) as y = f (x, zc). In this way, we efﬁciently train different speciﬁc mappings fc for each c. Moreover, the encoder  Meta-Learning Dynamics Forecasting Using Task Inference  estimate of the latent variable ˆzc. One last fully connected layer computes the additional output ˆc = W ˆzc.  Since convolutions are equivariant to shift (up to bound- ary frames) and mean pooling is invariant to shift, the encoder is shift-invariant. That is, enc(x1, . . . , xt, 0) = enc(0, x1, . . . , xt). In practice, shifting the time sequence forward one frame will add one new frame at the beginning and drop one frame at the end. This creates some change in output value of the encoder. Thus, practically, the encoder is only approximately shift-invariant.  Following 3D convolutions, we apply BatchNorm, LeakyReLU, and max-pooling. Max-pooling further re- duces the theoretical shift-invariance of the network since 2x2x2 max-pooling is perfectly equivariant to shifts of size 2 and only approximately invariant to shifts of size 1.  2.4. Prediction Network  As shown in Figure 4, our prediction network is similar to ResNet but includes layers which incorporate the charac- teristic latent variable. We use two specialized layers, adap- tive instance normalization AdaIN and adaptive padding AdaPad to specialize the prediction network to speciﬁc dy- namical systems. AdaIN has been used in style transfer networks to guide generative networks. Here, AdaIN may adapt for speciﬁc coefﬁcients and external forces. We also introduce a new layer AdaPad(x, z) which is well-suited for encoding the boundary conditions of dynamical systems.  AdaIN. We employ adaptive instance normalization AdaIN, which has proven effective in the style transfer literature as way to use a style vector to guide a convolutional net- work (Dumoulin et al., 2016; Ghiasi et al., 2017; Huang & Belongie, 2017; Dumoulin et al., 2018; Karras et al., 2019). Denote the channels of input x by xi and let µ(xi) and σ(xi) be the mean and standard deviation of channel i. For each AdaIN layer, a particular style is computed s = (µi, σi)i = Az + b, where the linear map A and bias b are learned weights. Adaptive instance normalization is then deﬁned  yi = σi  xi − µ(xi) σ(xi)  + µi.  In essence, the channels are renormalized to the style s.  For dynamics prediction, the characteristic latent variable z encodes data analogous to the various coefﬁcients of a differential equation and external forces on the system. In numerical simulation of a differential equation these coefﬁ- cients enter as scalings of different terms in the equation and the external forces are added to the combined force equation. Thus in our context AdaIN, which scales channels and adds a global vector, is well-suited to injecting this information.  AdaPad.  To compliment AdaIN, we introduce the  AdaPad layer, which is specialized to encoding the bound- ary conditions of each speciﬁc dynamical system. Generally when predicting dynamical systems, error is introduced along the boundaries since it is unknown how the dynam- ics interact with the boundary of the domain, and there may be unknown inﬂows or outﬂows. In our method, the boundary conditions may be inferred by the encoder in z and introduced during prediction by AdaPad as padding immediately outside the spatial domain in each layer.  Denote the spatial index of input x by xi,j for 1 ≤ i ≤ W and 1 ≤ j ≤ H. Letting A and b be trainable weights, the padding is computed  Az + b = (x0,0, . . . , x0,H+1, xW +1,0, . . . , xW +1,H+1, x0,1, . . . , xW,0, xW +1,1, . . . , xW,H+1).  Then  y = AdaPad(x, z) = (xi,j)0≤i≤W +1,0≤j≤H+1.  Thus y is the input x padded with additional values com- puted from the latent variable z.  2.5. DyAd Training  We use a two-stage approach for training. The encoder network gφ is trained ﬁrst separately from the prediction network. To combat the loss of shift invariance from the change from the boundary frames, we train the encoder using a time-shift-invariance loss. Given two training sam- ples (x(i), y(i), c(i)) and (x(j), y(j), c(j)) where x(i) and x(j) come from the same task and thus c(i) = c(j), we have loss  Lenc = L1(ˆc(i), c(i))+αL2(ˆz(i), ˆz(j))+β|(cid:107)ˆz(i)(cid:107)−m| (3)  where estimates ˆz(i) = gφ(x(i)) and ˆz(j) = gφ(x(j)) and ˆc(i) = W ˆz(i) + b is an afﬁne transformation of z.  Figure 4. Illustration of the AdaPad operation.  Meta-Learning Dynamics Forecasting Using Task Inference  The ﬁrst term of the loss L1(ˆc, c(i)) allows weak supervision of the latent variable z with input c(i). While not all time- invariant characteristics of the dynamical system are known, domain knowledge of characteristic parameters which iden- tify the dynamics may be incorporated in the datum c(i). For example, the Reynolds number of the ﬂuid.  The second term L2(ˆz(i), ˆz(j)) is the time-shift invariance loss, which penalizes the changes in latent variables between samples. Since the time-shift invariance of convolution is only approximate, this loss term drives the time-shift error even lower. The third term |(cid:107)ˆz(i)(cid:107)−m| prevents the encoder from generating small ˆz(i) due to time-shift invariance loss. For both L1 and L2, we use mean squared error.  The prediction network is trained afterwards. The kernels of the convolutions and the fully connected mappings of the AdaIN and AdaPad layers are all trained simultaneously as the prediction network is trained. The loss for the prediction network is the forecasting loss for the true label y,  Lpred = L3(ˆy, y).  We use MSE per time step and generate multi-step forecast- ing in an autoregressive fashion.  3. Theoretical Analysis  The high-level idea of our method is to learn a good repre- sentation of the dynamics that generalizes well and adapt this representation to new tasks. Our model achieves this by learning on a heterogeneous domain where it learns to infer the tasks. We provide analysis for this procedure. See Appendix B for a longer treatment with proofs.  Suppose we have K tasks, each of which is sampled from a continuous space {ck}K k=1 ∼ C. For each task ck, we have a collection of series as realizations from the dynamical system Xk = {(xt, . . . , x1; ck)(i)}nk i=1 where ck represents the system behavior in a speciﬁc domain Let X = ∪c∈CXc be the union of samples over all tasks.  Multi-task Learning Error. Our model resembles the multi-task representation learning setting (Maurer et al., 2016) with joint risk (cid:15) = (1/K) (cid:80) k (cid:15)k the mean of risks (cid:15)k of each task deﬁned separately. Denote the corresponding empirical risks ˆ(cid:15) and ˆ(cid:15)k. We bound the true risk (cid:15) using the empirical risk ˆ(cid:15) and Rademacher complexity R(F) of the hypothesis class F. The following theorem restates the main result in (Ando et al., 2005) with simpliﬁed notations. Theorem 3.1. (Ando et al., 2005) Assume loss is bounded l ≤ 1/2. Given n samples each from K different forecast- ing tasks µ1, · · · , µk with probability at least 1 − δ, the following inequality holds for each f ∈ F:  The following inequality compares the performance for multi-task learning to learning the individual tasks. Let Rk(F) be the Rademacher complexity for F over µk. Lemma 3.2. The Rademacher complexity for multi-task learning is bounded R(F) ≤ (1/K) (cid:80)K  k=1 Rk(F).  We can now compare the bound from Theorem 3.1 with the bound obtained by considering each task individually.  Proposition 3.3. Assume the loss is bounded l ≤ 1/2, then the generalization bound given by considering each task individually is  (cid:15)(f ) ≤ ˆ(cid:15)(f ) + 2  (cid:32)  1 K  K (cid:88)  k=1  (cid:33)  (cid:114)  Rk(F)  +  log 1/δ 2n  .  (4)  which is strictly looser than the bound from Theorem 3.1.  This helps explain why our multitask learning framework has better generalization than learning each task indepen- dently. The shared data tightens the generalization bound.  Domain Adaptation Error. Since we test on c ∼ C outside the training set {ck}, we incur error due to domain adapta- tion from the source domains µc1, . . . , µcK to target domain µc with µ being the true distribution. Denote the correspond- ing empirical distributions of n samples per task by ˆµc. For similar domains, the Wasserstein distance W1(µc, µc(cid:48)) is small. The bound from (Redko et al., 2017) applies well to our setting as such:  (cid:16)  (cid:15)c(f ) + 1/K (cid:80)K  Theorem 3.4 ((Redko et al., 2017),Theorem 2). Let λ = . There is N = minf ∈F N (ζ, dim(X )) such that for n > N , for any hypothesis f , with probability at least 1 − δ,  (cid:17) k=1 (cid:15)ck (f )  (cid:15)c(f ) ≤  1 K  K (cid:88)  k=1  (cid:32)  (cid:15)ck (f ) + W1  ˆµc,  (cid:33)  ˆµck  1 K  K (cid:88)  k=1  + (cid:112)2 log(1/δ)  (cid:17) (cid:16)(cid:112)1/n + (cid:112)1/(nK)  + λ.  Encoder versus Prediction Network Error. Error from DyAd may result from either the encoder gφ or the pre- diction network fθ. Our hypothesis space has the form {x (cid:55)→ fθ(x, gφ(x))} where φ and θ are the weights of the encoder and prediction network respectively. Let (cid:15)X be the error over the entire domain X , that is, for all c. Let (cid:15)enc(gφ) = Ex∼X (L1(g(x), gφ(x)) be the encoder error. Proposition 3.5. Assume c (cid:55)→ supθ(fθ(·, c)) is Lipschitz continuous with Lipschitz constant γ. Then we bound  (cid:15)X (fθ(·, gφ(·))) ≤ γ(cid:15)enc(gφ) + Ec∼C [(cid:15)c(fθ(x, c))]  (5)  1 K  (cid:88)  k  (cid:15)k(f ) ≤  1 K  (cid:88)  k  ˆ(cid:15)k(f ) + 2R(F) +  (cid:114)  log 1/δ 2nK  where the ﬁrst term is the error due to the encoder incor- rectly identifying the task and the second term is the error due the prediction network alone.  Meta-Learning Dynamics Forecasting Using Task Inference  4. Related Work  Learning Dynamical Systems. Deep learning models are gaining popularity for learning dynamical systems (Shi et al., 2017; Chen et al., 2018; Kolter & Manek, 2019; Azencot et al., 2020a). An emerging topic is physics-informed deep learning Raissi et al. (2017); Maziar Raissi (2019); Lutter et al. (2018); Azencot et al. (2020b); Wang et al. (2020a) which integrates inductive biases from physical systems to improve learning. For example, (Maziar Raissi, 2019) use deep neural networks to solve PDEs automatically but re- quire explicit input of boundary conditions during inference. (Lutter et al., 2018) encode Euler-Lagrange equation into the deep neural nets but focus on learning low-dimensional trajectories. For high-dimensional ﬂuid ﬂow, (Xie et al., 2018) and (Tompson et al., 2017) developed deep learning models in the context of ﬂuid ﬂow animation, where the pre- diction and physical consistency is less critical. There are also physics-informed deep learning models (de Bezenac et al., 2018; Wang et al., 2020b; Ayed et al., 2019a;b; Li et al., 2021). For instance, Anderson et al. (2019) designed rotationally covariant neural network for learning molecular systems. Morton et al. (2018); Azencot et al. (2020b) in- corporated Koopman theory into the architecture. However, these approaches often focus on a speciﬁc type of system dynamics instead of meta-learning in this work.  Meta-learning and Domain Adaptation. The aim of meta learning, or learning to learn (Thrun & Pratt, 1998), is to ac- quire generic knowledge of different tasks in order for rapid learning on new tasks. Based on how the meta-level knowl- edge is extracted and used, meta-learning methods have been classiﬁed into model-based (Munkhdalai & Yu, 2017; Duan et al., 2017; Santoro et al., 2016; Alet et al., 2018; Ore- shkin et al., 2019; Seo et al., 2020), metric-based (Vinyals et al., 2016; Snell et al., 2017) and gradient-based (Finn et al., 2017; Andrychowicz et al., 2016; Rusu et al., 2019; Grant et al., 2018; Yao et al., 2019). Most meta-learning approaches are outside of the forecasting domain with a few exceptions. Oreshkin et al. (2019) design a residual architec- ture for time series forecasting with a meta-learning parallel. Alet et al. (2018) propose a modular meta-learning approach to combine neural network modules for continues control. But forecasting physical dynamics poses unique challenges to meta-learning as we seek ways to encode physical knowl- edge into our model. DyAd can also be considered as a meta-learning model since the encoder is able to encode different tasks and extract high-level meta representations and the forecaster then performs across different systems without retraining. It is noteworthy that meta-learning for time series forecasting (Lemke & Gabrys, 2010; Talagala et al., 2018) was termed for forecast model selection, which has a different objective from ours.  Style Transfer. Our approach is inspired by the style  transfer techniques. Style transfer initially appear in non- photorealistic rendering (Kyprianidis et al., 2012). Recently, neural style transfer (Jing et al., 2019) has been applied to image synthesis (Gatys et al., 2016), videos generation (Ruder et al., 2016), and language translation (Prabhumoye et al., 2018). For dynamical systems, Sato et al. (2018) adapts texture synthesis to transfer the style of turbulence for animation. Kim & Lee (2019) studies unsupervised gen- erative modeling of turbulent ﬂows but for super-resolution reconstruction rather than forecasting.  5. Experiments  We compare our model with a series of baselines on the multi-step forecasting with different dynamics. We consider two testing scenarios: (1) dynamics with different initial conditions (test-future) and (2) dynamics with different pa- rameters such as external force (test-domain). The ﬁrst scenario evaluates the models’ ability to extrapolate into the future for the same task. The second scenario estimates the capability of the models to generalize across different tasks.  We experiment on synthetic turbulent ﬂows, real-world sea surface temperature and ocean currents data. They are difﬁ- cult to forecast using numerical methods due to unknown external forces and complex dynamics not fully captured by simpliﬁed mathematical models. We defer the details of the datasets and experiments to Appendix A.2.  5.1. Datasets  Turbulent Flow with Varying Buoyancy. We generate a synthetic dataset of turbulent ﬂows with a numerical simula- tor, PhiFlow1. It contains 64×64 velocity ﬁelds of turbulent ﬂows in which we vary the buoyant force acting on the ﬂuid from 1 to 25. Each buoyant force corresponds to a forecast- ing task and there are 25 tasks in total. We use the mean vorticity of each task as partial supervision c as we can di- rectly calculate it from the data. Vorticity can characterize formation and circular motion of turbulent ﬂows.  Sea Surface Temperature. We evaluate on a real-world sea surface temperature data generated by the NEMO ocean engine (Madec et al., 2015)2. We select an area from Paciﬁc ocean range from 01/01/2018 to 12/31/2020. The corre- sponding latitude and longitude are (-174∼-153, 5∼26). This area is then divided into 25 64×64 subregions, each is a task since the mean temperature varies a lot along longi- tude and latitude. For the encoder training, we use season as an additional supervision signal besides the mean temper- ature of each subregion. In other words, the encoder should  1https://github.com/tum-pbs/PhiFlow 2The data are available at https://resources.marine.  copernicus.eu/?option=com_csw&view=details& product_id=GLOBAL_ANALYSIS_FORECAST_PHY_ 001_024  Meta-Learning Dynamics Forecasting Using Task Inference  Table 1. Prediction RMSE on the turbulent ﬂow and sea surface temperature datasets. Prediction RMSE and ESE (energy spectrum errors) on the test-future and test-domain test sets of ocean currents dataset. Sea Temperature  Turbulent Flows  Ocean Currents  Model  test-future  test-domain  test-future  test-domain  test-future  test-domain  ResNet 0.936±0.098 U-Net 0.921±0.020 Mod-ind 1.126±0.050 Mod-attn 0.626±0.022 Mod-wt 0.579±0.033 MetaNet 0.759±0.132 MAML 0.627±0.003 DyAd 0.423±0.011  0.652±0.019 0.675±0.021 ———- 0.919±0.026 0.601±0.071 0.764±0.079 0.677±0.019 0.533±0.017  1.522±0.099 1.106±0.142 2.486±0.167 2.039±0.139 0.834±0.093 2.911±0.219 1.862±0.187 0.741±0.063  1.504±0.078 1.137±0.153 ———- 2.038±0.118 0.721±0.101 1.938±0.171 1.935±0.039 0.689±0.062  1.172±0.091 | 0.942±0.139 1.116±0.103 | 0.756±0.056 1.305±0.156 | 1.160±0.096 1.200±0.177 | 0.804±0.121 1.220±0.092 | 0.835±0.044 1.245±0.086 | 0.971±0.097 1.387±0.191 | 1.077±0.124 1.006±0.067 | 0.486±0.041  1.148±0.036 | 1.136±0.117 1.154±0.118 | 0.876±0.083 ———- | ———- 1.224±0.119 | 1.109±0.084 1.245±0.108 | 1.300±0.073 1.240±0.087 | 1.269±0.040 1.297±0.143 | 1.199±0.086 0.955±0.093 | 0.544±0.113  Figure 5. Target and predictions by ResNet, Modular-wt and DyAd at time 1, 5, 10 for turbulent ﬂows with buoyancy factors 9 (left) and 21 (right) respectively. We can see that DyAd can easily generate predictions for various ﬂows while ResNet and Modular-wt have trouble understanding and disentangling buoyancy factors.  be able to infer the mean temperature of the subregion as well as to classify four seasons given the temperature series.  Ocean Currents. We also experiment with the velocity ﬁelds of ocean currents from the same region and use the same task division as the sea surface temperature data set. Similar to the turbulent ﬂow data set, we use the mean vorticity of each subregion as the weak-supervision signal.  5.2. Baselines  We include several SoTA baselines from meta-learning, as well as common methods for dynamics forecasting.  ResNet (He et al., 2016): A widely adopted video pre- diction model (Oprea et al., 2020; Wang et al., 2020b). • U-net (Ronneberger et al., 2015): Originally devel- oped for biomedical image segmentation, adapted for dynamics forecasting (de Bezenac et al., 2018)  modules through the ﬁnal output.  Mod-wt: A modular meta-learning variant which uses attention weights to combine the parameters of the convolutional kernels for new tasks.  MetaNet (Munkhdalai & Yu, 2017): A model-based meta-learning method which requires a few labels from test tasks as a support set to adapt.  MAML (Finn et al., 2017): A popular gradient-based meta-learning approach. We replaced the classiﬁer in the original model with a ResNet for regression.  Both Mod-attn and Mod-wt have a convolutional en- coder to generate attention weights. MetaNet requires a few samples from test tasks as a support set and MAML needs adaptation retraining on test tasks, while other models do not need any information from the test domains. Details about baselines can be found in Appendix A.2.  Mod-ind: A simple baseline with independent neural  5.3. Experiment Setup  network modules trained for different tasks.  Mod-attn: A modular meta-learning method which combines modules to generalize to new tasks (Alet et al., 2018). Attention mechanism is used to combine  For all datasets, we use a sliding window approach to gener- ate samples of sequences. For test-future, we train and test on the same tasks but different time steps. For test-domain, we train and test on different tasks with a 80-20 split. All  Meta-Learning Dynamics Forecasting Using Task Inference  models are trained to make next step prediction given the previous 20 steps as input. We forecast in an autoregressive manner to generate multi-step ahead predictions. All results are averaged over 3 runs with random initialization.  Apart from root mean square error, we also report the en- ergy spectrum error for ocean current prediction which is the RMSE regarding the log of energy spectrum. ESE can indicate whether the predictions preserve the correct statisti- cal distribution and obey the energy conservation law, which is a critical metric for physical consistency. Details about energy spectrum can be found in Appendix A.3.  Figure 6. The energy spectrum of target and predictions by ResNet, U-net and DyAd on future test set (left) and domain test set (right) of ocean currents.  5.4. Experiment Results  Prediction Performance. Table 1 shows the RMSE of 10- step ahead predictions on two test sets of turbulent ﬂows and sea surface temperature, and DyAd makes the most accurate predictions for both test sets of both datasets. Figure 5 shows the target and predictions by ResNet, Modular-wt and DyAd at time 1, 5, 10 for turbulent ﬂows with buoyancy factors 9 (left) and 21 (right) respectively. We can see that DyAd can generate realistic ﬂows with the corresponding characteristics while the other two models have trouble understanding and disentangling the buoyancy factor.  Table 1 also reports the RMSE and ESE of 20-step ahead ocean currents predictions. DyAd not only has small RMSE but also obtains the smallest ESE, suggesting it captures the statistical distribution of ocean currents well. Figure 6 shows the energy spectrum of target and predictions by ResNet, U-net and DyAd on two test sets of ocean currents, and we can see that DyAd is the closest to the target.  Ablation Study. We also performed an ablation study of DyAd to understand the contribution of each component, shown in Table 2. We ﬁrst remove the encoder from DyAd while keeping the same prediction network (No_enc). The resulting model degrades but still outperforms ResNet. This demonstrates the effectiveness of AdaIN and AdaPad for prediction. Another notable feature of our model is the ability to infer tasks with weakly supervised signal c. It is  Table 2. Ablation study: prediction RMSE of DyAd, DyAd with- out encoder, DyAd with encoder trained by wrong supervision c and DyAd with end to end training.  Model  DyAd(ours) No_enc Wrong_enc End2End  test-future  0.423 ±0.011 0.627 ±0.033 0.658 ±0.019 0.449 ±0.003  test-domain  0.533 ±0.017 0.601 ±0.022 0.617 ±0.031 0.536 ±0.003  important to have a c that is related to the task domain. As an ablative study, we fed the encoder in DyAd a random c, leading to Wrong_enc. We see that having the wrong supervision may hurt the forecasting performance. We also tried to train DyAd end-to-end (End2End) but observed worse performance than the two-stage training approach.  Controllable Forecast. DyAd infers the characteristic la- tent variable from data, which allows direct control of the latent characteristics. We tried to vary the encoder input while keeping the prediction network input ﬁxed. Figure 7 shows the outputs from DyAd when the encoder is fed with ﬂow with different buoyancy factors c = 5, 15, 25. We can see with higher buoyancy factors, the predictions become more turbulent. This demonstrates that the encoder can dis- entangle and control the latent characteristics of predictions.  Figure 7. Outputs from DyAd while we vary encoder input but keep the prediction network input ﬁxed. From left to right, the en- coder is fed with ﬂow with different buoyancy factor c = 5, 15, 25. the prediction network input has ﬁxed buoyancy c = 15.  6. Conclusion  We propose a model-based meta-learning method, DyAd to forecast physical dynamics. DyAd uses an encoder to infer the parameters of the task and a prediction network to adapt and forecast giving the inferred task. Our model can also leverage any weak supervision signals that can help distinguish different tasks, allowing the incorporation of ad- ditional domain knowledge. On challenging turbulent ﬂow prediction and real-world ocean temperature and currents forecasting tasks, we observe superior performance of our model across heterogeneous dynamics. Future work would consider non-grid data such as ﬂows on a graph or a sphere.  Meta-Learning Dynamics Forecasting Using Task Inference  References  Alet, F., Lozano-Perez, T., and Kaelbling, L. Modular meta-  learning. ArXiv, abs/1806.10166, 2018.  Anderson, B., Hy, T.-S., and Kondor, R. Cormorant: Co- variant molecular neural networks. In Advances in neural information processing systems (NeurIPS), 2019.  Ando, R. K., Zhang, T., and Bartlett, P. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(11), 2005.  Andrychowicz, M., Denil, M., Colmenarejo, S. G., Hoff- man, M. W., Pfau, D., Schaul, T., and Freitas, N. D. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, 2016.  Ayed, I., Bézenac, E. D., Pajot, A., and Gallinari, P. Learn- ing partially observed PDE dynamics with neural net- works, 2019a. URL https://openreview.net/ forum?id=HyefgnCqFm.  Ayed, I., de Bézenac, E., Pajot, A., Brajard, J., and Gallinari, P. Learning dynamical systems from partial observations. ArXiv, abs/1902.11136, 2019b.  Azencot, O., Erichson, N., Lin, V., and Mahoney, M. W. Forecasting sequential data using consistent koopman autoencoders. In International Conference on Machine Learning, 2020a.  Azencot, O., Erichson, N. B., Lin, V., and Mahoney, M. Forecasting sequential data using consistent koopman autoencoders. In International Conference on Machine Learning, pp. 475–485. PMLR, 2020b.  Baxter, J. Theoretical models of learning to learn. In Learn-  ing to learn, pp. 71–94. Springer, 1998.  Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, D. Neural ordinary differential equations. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 6572–6583, 2018.  de Bezenac, E., Pajot, A., and Gallinari, P. Deep learn- ing for physical processes: Incorporating prior scientiﬁc In International Conference on Learning knowledge. Representations, 2018. URL https://openreview. net/forum?id=By4HsfWAZ.  Duan, Y., Andrychowicz, M., Stadie, B. C., Ho, J., Schnei- der, J., Sutskever, I., Abbeel, P., and Zaremba, W. One- shot imitation learning. In Advances in Neural Informa- tion Processing Systems, 2017.  Dumoulin, V., Shlens, J., and Kudlur, M. A learned represen- tation for artistic style. arXiv preprint arXiv:1610.07629, 2016.  Dumoulin, V., Perez, E., Schucher, N., Strub, F., Vries, H. d., Courville, A., and Bengio, Y. Feature-wise transforma- tions. Distill, 3(7):e11, 2018.  Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Interna- tional Conference of Machine Learning, 2017.  Gatys, L. A., Ecker, A. S., and Bethge, M. Image style trans- fer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2414–2423, 2016.  Ghiasi, G., Lee, H., Kudlur, M., Dumoulin, V., and Shlens, J. Exploring the structure of a real-time, arbi- trary neural artistic stylization network. arXiv preprint arXiv:1705.06830, 2017.  Grant, E., Finn, C., Levine, S., Darrell, T., and Grifﬁths, T. Recasting gradient-based meta-learning as hierarchical bayes. ArXiv Preprint, abs/1801.08930, 2018.  Gupta, A., Mendonca, R., Liu, Y., Abbeel, P., and Levine, S. Meta-reinforcement learning of structured exploration strategies. In Proceedings of the 32nd International Con- ference on Neural Information Processing Systems, pp. 5307–5316, 2018.  He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.  Huang, X. and Belongie, S. Arbitrary style transfer in real- time with adaptive instance normalization. In Proceed- ings of the IEEE International Conference on Computer Vision, pp. 1501–1510, 2017.  Jing, Y., Yang, Y., Feng, Z., Ye, J., Yu, Y., and Song, M. Neural style transfer: A review. IEEE transactions on visualization and computer graphics, 26(11):3365–3385, 2019.  Karras, T., Laine, S., and Aila, T. A style-based genera- tor architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4401–4410, 2019.  Kim, J. and Lee, C. Deep unsupervised learning of turbu- lence for inﬂow generation at various reynolds numbers. arXiv:1908.10515, 2019.  Kolter, J. Z. and Manek, G. Learning stable deep dynamics models. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, pp. 11128–11136, 2019.  Meta-Learning Dynamics Forecasting Using Task Inference  Kyprianidis, J. E., Collomosse, J., Wang, T., and Isenberg, T. State of the\" art”: A taxonomy of artistic stylization IEEE transactions techniques for images and video. on visualization and computer graphics, 19(5):866–885, 2012.  Oprea, S., Martinez-Gonzalez, P., Garcia-Garcia, A., Castro- Vargas, J. A., Orts-Escolano, S., Garcia-Rodriguez, J., and Argyros, A. A review on deep learning techniques for video prediction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.  Lemke, C. and Gabrys, B. Meta-learning for time series forecasting and forecast combination. Neurocomputing, 73(10-12):2006–2016, 2010.  Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhat- tacharya, K., Stuart, A., and Anandkumar, A. Fourier neu- ral operator for parametric partial differential equations. International Conference on Learning Representations, 2021.  Locatello, F., Bauer, S., Lucic, M., Raetsch, G., Gelly, S., Schölkopf, B., and Bachem, O. Challenging common assumptions in the unsupervised learning of disentangled representations. In international conference on machine learning, pp. 4114–4124. PMLR, 2019.  Lutter, M., Ritter, C., and Peters, J. Deep lagrangian net- works: Using physics as model prior for deep learning. In International Conference on Learning Representations, 2018.  Madec, G. et al. NEMO ocean engine, 2015. Technical Note. Institut Pierre-Simon Laplace (IPSL), France. https://epic.awi.de/id/eprint/39698/ 1/NEMO_book_v6039.pdf.  Massague, A. C., Zhang, C., Feric, Z., Camps, O., and Yu, R. Learning disentangled representations of video with missing data. arXiv preprint arXiv:2006.13391, 2020.  Maurer, A., Pontil, M., and Romera-Paredes, B. The beneﬁt of multitask representation learning. Journal of Machine Learning Research, 17(81):1–32, 2016.  Maziar Raissi, Paris Perdikaris, G. E. K. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686–707, 2019.  Mohri, M., Rostamizadeh, A., and Talwalkar, A. Founda-  tions of machine learning. MIT press, 2018.  Morton, J., Jameson, A., J. Kochenderfer, M., and Wither- den, F. Deep dynamical modeling and control of unsteady ﬂuid ﬂows. In Advances in Neural Information Process- ing Systems (NeurIPS), 2018.  Munkhdalai, T. and Yu, H. Meta networks. Proceedings of  machine learning research, 70:2554–2563, 2017.  Nie, W., Karras, T., Garg, A., Debhath, S., Patney, A., Patel, A. B., and Anandkumar, A. Semi-supervised stylegan for disentanglement learning. In International Conference on Machine Learning, 2020.  Oreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. N-beats: Neural basis expansion analysis for interpretable time series forecasting. In International Conference on Learning Representations, 2019.  Prabhumoye, S., Tsvetkov, Y., Salakhutdinov, R., and Black, A. W. Style transfer through back-translation. In Pro- ceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 866–876, 2018.  Raissi, M., Perdikaris, P., and Karniadakis, G. E. Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations. arXiv preprint arXiv:1711.10561, 2017.  Redko, I., Habrard, A., and Sebban, M. Theoretical analysis of domain adaptation with optimal transport. In Joint Eu- ropean Conference on Machine Learning and Knowledge Discovery in Databases, pp. 737–753. Springer, 2017.  Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolu- tional networks for biomedical image segmentation. In In- ternational Conference on Medical image computing and computer-assisted intervention, pp. 234–241. Springer, 2015.  Ruder, M., Dosovitskiy, A., and Brox, T. Artistic style transfer for videos. In German conference on pattern recognition, pp. 26–36. Springer, 2016.  Rusu, A. A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu, R., Osindero, S., and Hadsell, R. Meta-learning with latent embedding optimization. In International Confer- ence on Learning Representations, 2019. URL https: //openreview.net/forum?id=BJgklhAcK7.  Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T. Meta-learning with memory-augmented neu- ral networks. In ICML, 2016.  Sato, S., Dobashi, Y., Kim, T., and Nishita, T. Example- based turbulence style transfer. ACM Transactions on Graphics (TOG), 37(4):1–9, 2018.  Seo, S., Meng, C., Rambhatla, S., and Liu, Y. Physics- aware spatiotemporal modules with auxiliary tasks for meta-learning. ArXiv, abs/2006.08831, 2020.  Meta-Learning Dynamics Forecasting Using Task Inference  Shi, X., Gao, Z., Lausen, L., Wang, H., Yeung, D., Wong, W., and chun Woo, W. Deep learning for precipitation nowcasting: A benchmark and a new model. In Advances in neural information processing systems, 2017.  Snell, J., Swersky, K., and Zemel, R. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, 2017.  Talagala, T. S., Hyndman, R. J., Athanasopoulos, G., et al. Meta-learning how to forecast time series. Technical report, Monash University, Department of Econometrics and Business Statistics, 2018.  Thrun, S. and Pratt, L. Learning to learn: Introduction and overview. In Learning to learn, pp. 3–17. Springer, 1998.  Tompson, J., Schlachter, K., Sprechmann, P., and Perlin, K. Accelerating Eulerian ﬂuid simulation with convolutional networks. In ICML’17 Proceedings of the 34th Interna- tional Conference on Machine Learning, volume 70, pp. 3424–3433, 2017.  Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D. Matching networks for one shot learning. In NIPS, 2016.  Wang, R., Kashinath, K., Mustafa, M., Albert, A., and Yu, R. Towards physics-informed deep learning for turbulent  ﬂow prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1457–1466, 2020a.  Wang, R., Walters, R., and Yu, R. Incorporating symmetry into deep dynamics models for improved generalization. arXiv preprint arXiv:2002.03061, 2020b.  Xie, Y., Franz, E., Chu, M., and Thuerey, N.  tem- poGAN: A temporally coherent, volumetric GAN for super-resolution ﬂuid ﬂow. ACM Transactions on Graph- ics (TOG), 37(4):95, 2018.  Yao, H., Wei, Y., Huang, J., and Li, Z. Hierarchically structured meta-learning. In International Conference on Machine Learning, pp. 7045–7054. PMLR, 2019.  Yoon, J., Kim, T., Dia, O., Kim, S., Bengio, Y., and Ahn, S. Bayesian model-agnostic meta-learning. In Proceedings of the 32nd International Conference on Neural Informa- tion Processing Systems, pp. 7343–7353, 2018.  Zoltowski, D., Pillow, J., and Linderman, S. A general recur- rent state space framework for modeling neural dynamics during decision-making. In International Conference on Machine Learning, pp. 11680–11691. PMLR, 2020.  Meta-Learning Dynamics Forecasting Using Task Inference  A. Implementation Details  A.1. Model Design  The prediction network ˆy = f (x, z) is composed of 8 blocks. Each block operates on a hidden state h(i) of shape B × H × W × Cin and yields a new hidden state h(i+1) of the shape B × H × W × Cout. The ﬁrst input is h0 = x and the ﬁnal output is computed from the ﬁnal hidden state as ˆy = Conv2D(h(8)). We deﬁne each block as  a(i) = σ(Conv2D(AdaPad(h(i), z))) b(i) = σ(Conv2D(AdaPad(a(i), z))) + h(i) hi+1 = AdaIN(b(i), z)  as illustrated in Figure 4.  A.2. Experiment Details  For fair comparison, we set these models to have equal capacity as DyAd in terms of number of parameters. Hyperparameters including learning rate, input length and the number of steps of accumulated loss for training are tuned on validation sets.  Baselines. Modular-attn has a convolutional encoder f that takes the same input x as each module M to generate attention weights, (cid:80)m k=1 exp[f (x)(k)] Ml(x). Modular-wt also has the same encoder but to generate weights for l=1 combining the convolution parameters of all modules. We use additional samples of up to 20% of the test set from test tasks. MetaNet uses these as a support set. MAML is retrained on these samples for 10 epoch for adaptation.  exp[f (x)(l)]  (cid:80)m  Hyperparameter tuning. We tuned learning rate (1e-3∼1e-5), batch size (16∼64), the number of accumulated errors for backpropogation (2∼5), and hidden size (64∼512) of Modular Networks and Meta-Nets. We ﬁxed the number of historic input frames as 20. When we trained the encoder on turbulent ﬂows and sea surface temperature, we used α = 1 and β = 1. For ocean currents, we used α = 0.2 and β = 0.2.  Model Capacity. Table 3 displays the number of parameters of each tuned model.  ResNet  U-net  Mod-ind  Mod-attn  Mod-wt  MetaNets  20.32  9.69  13.01  13.19  13.19  9.63  MAML  20.32  DyAd  15.60  Table 3. The number of parameters of each model.  A.3. Turbulence kinetic energy spectrum  The turbulence kinetic energy spectrum E(k) is related to the mean turbulence kinetic energy as  (cid:90) ∞  0  E(k)dk = ((u(cid:48))2 + (v(cid:48))2)/2,  (u(cid:48))2 =  1 T  T (cid:88)  t=0  (u(t) − ¯u)2,  where the k is the wavenumber and t is the time step. Figure 8 shows a theoretical turbulence kinetic energy spectrum plot. The spectrum can describe the transfer of energy from large scales of motion to the small scales and provides a representation of the dependence of energy on frequency. Thus, the Energy Spectrum Error can indicate whether the predictions preserve the correct statistical distribution and obey the energy conservation law. A trivial example that can illustrate why we need ESE is that if a model simply outputs moving averages of input frames, the accumulated RMSE of predictions might not be high but the ESE would be really big because all the small or even medium eddies are smoothed out.  B. Theoretical Analysis  The high-level idea of our method is to learn a good representation of the underlying dynamics from multiple tasks, and then transfer this representation to a target task (domain adaptation).  Meta-Learning Dynamics Forecasting Using Task Inference  Figure 8. Spectrum plot  Deﬁnition 1 (Forecasting task). Each forecasting task xt+1 = f (xt, . . . ) is to learn a conditional distribution µ over the system states µ : p(xt+1|xt, . . . ) conditioned on the sequence of previous states where µ is a probability measure.  In our setting, we have K tasks, each of which is sampled from a continuous, ﬁnite space {ck} ∼ C. Let µk be the corresponding conditional probability measure p(xt, . . . , x1|ck). For each task ck, we have a collection of n series as realizations from the dynamical system Xk = {(xt, . . . , x1; ck)(i)}n i=1 sampled from µk. The semicolon here represents the system behavior in a speciﬁc domain ck. Let X = (cid:83)  Xk be the union of samples over all tasks.  k  In practice, we often have some intuition of the variables that dictate the domain. Therefore, we have two possible scenarios for the role of c in dynamical systems:  1. c fully distinguishes the task: the differences in Xk can be completely explained by the differences in ck;  2. c partially distinguishes the task: a more realistic scenario where we only have partial knowledge of the domain. There exist latent variables z(cid:48) that need to be inferred from raw data. Together z = [c, z(cid:48)] can describe the behavior of the system in a domain.  We assume Scenario 1, which resembles the multi-task representation learning setting (Maurer et al., 2016) with joint true risk over all tasks (cid:15) and individual task true risk (cid:15)k deﬁned respectively  (cid:15)(f ) =  1 K  K (cid:88)  k=1  (cid:15)k(f ),  and corresponding empirical risks  (cid:15)k(f ) = E  x(i)  k ∼µk  (cid:16)  (cid:104) l  f  (cid:16)  x(i) k  (cid:17)(cid:17)(cid:105)  (6)  ˆ(cid:15)(f, X) =  1 K  K (cid:88)  k=1  l(f (Xk)),  ˆ(cid:15)k(f, Xk) = l(f (Xk)),  where l is a loss function.  B.1. Multi-Task Learning Error  We want to bound the true loss (cid:15) using the empirical loss ˆ(cid:15) and Rademacher complexity of the hypothesis class F. We can use the classic results from (Ando et al., 2005). Deﬁne empirical Rademacher complexity for samples from all tasks as  ˆRX(F) = Eσ  (cid:34)  (cid:32)  sup f ∈F  1 nK  K (cid:88)  n (cid:88)  k=1  i=1  (cid:33)(cid:35)  σ(i) k l(f (x(i)  k ))  (7)  Meta-Learning Dynamics Forecasting Using Task Inference  where {σ(i) EX( ˆRX(F)).  k } are independent binary variables σ(i)  k ∈ {−1, 1}. The true Rademacher complexity is then deﬁned R(F) =  The following theorem restates the main result from (Ando et al., 2005). We simplify the statement by using the Rademacher complexity rather than the set cover number argument used in the original proof. Theorem B.1. (Ando et al., 2005) Given data from K different forecasting tasks µ1, · · · , µk and f in hypothesis class F, for some constant C with probability at least 1 − δ, the following inequality holds:  1 K  (cid:88)  k  (cid:15)k(f ) ≤  1 K  (cid:88)  k  ˆ(cid:15)k(f ) + 2R(F) + C  (cid:114)  log 1/δ nK  .  (8)  If we assume the loss is bounded l ≤ 1/2, then we may take C = 1/  √  2.  Proof. Consider {x(i)  k } as independent random variables. For a function φ that satisﬁes  |φ(x(1), · · · , x(i), · · · x(n)) − φ(x(1), · · · , ˜x(i), · · · x(n))| ≤ ci  by McDiarmid’s inequality, we have  (cid:16)  p  φ(x(1), · · · , x(n)) − E[φ] ≥ t  (cid:17)  (cid:18)  ≤ exp  −  (cid:19)  .  2t2 i c2 i  (cid:80)  Applying this inequality to the max difference Q(X) = supf ∈F [(cid:15)(f ) − ˆ(cid:15)(f, X)], then with probability at least 1 − δ, we have  Q(X) − EX[Q(X)] ≤ C  (cid:114)  log 1/δ nK  √  where C is a constant depending on the bounds ci. If the loss l ≤ 1/2, then |Q| ≤ 1/2 and so we can take ci = 1 leading to C = 1/ 2. A standard computation (see (Mohri et al., 2018), Theorem 3.3) using the law of total expectation shows EX[Q(X)] ≤ 2R(F), which ﬁnishes the proof.  We can use this to compare the generalization error of multi-task learning versus that of learning the individual tasks. The following inequality compares the Rademacher complexity for multi-task learning to that of individual task learning. Denote ˆRXk and Rk the empirical and true Rademacher complexity for F over µk. Lemma B.2. The Rademacher complexity for multi-task learning is bounded R(F) ≤ (1/K) (cid:80)K  k=1 Rk(F).  Proof. We compute the empirical Rademacher complexity,  ˆRX(F) = Eσ  (cid:34)  (cid:32)  sup f ∈F  1 nK  K (cid:88)  n (cid:88)  k=1  i=1  σ(i) k l  (cid:16)  (cid:16)  f  x(i) k  (cid:17)(cid:17)  (cid:33)(cid:35)  ≤ Eσ  (cid:34) K (cid:88)  k=1  (cid:32)  1 nK  n (cid:88)  i=1  σ(i) k l  (cid:16)  (cid:16)  f  x(i) k  (cid:17)(cid:17)  (cid:33)(cid:35)  (cid:32)  1 n  n (cid:88)  i=1  σ(i) k l  (cid:16)  (cid:16)  f  x(i) k  (cid:17)(cid:17)  (cid:33)(cid:35)  (cid:32)  1 n  n (cid:88)  i=1  σ(i) k l  (cid:16)  (cid:16)  f  x(i) k  (cid:17)(cid:17)  (cid:33)(cid:35)  sup f ∈F (cid:34)  Eσ  sup f ∈F (cid:34)  Eσk  sup f ∈F  ˆRXk (F)  =  =  =  1 K  1 K  1 K  K (cid:88)  k=1  K (cid:88)  k=1  K (cid:88)  k=1  The ﬁrst inequality follows from the sub-additivity of the supremum function. The next equality is due to the fact positive scalars commute with supremum, and by the linearity of expectation. The expectation Eσ reduces to the expectation Eσk over only those Rademacher variables appearing inside the expectation. Rk(F) is the Rademacher complexity of the function on the individual task k. Taking expectation over all samples X gives the result.  Meta-Learning Dynamics Forecasting Using Task Inference  It is instructive to compare the bound from Theorem B.1 with the generalization error bound obtained by considering each task individually.  Proposition B.3. Assume n = nk for all tasks k and the loss l is bounded l ≤ 1/2, then the generalization bound given by considering each task individually is  (cid:15)(f ) ≤ ˆ(cid:15)(f ) + 2  (cid:32)  1 K  K (cid:88)  k=1  (cid:33)  (cid:114)  Rk(F)  +  log 1/δ 2n  .  (9)  which is strictly looser than the bound from Theorem B.1 under the same assumptions.  This result helps to explain why our multitask learning framework has better generalization than learning each task independently. The shared data tightens the generalization bound.  Proof. Applying the classical analog of Theorem B.1 to a single task, we ﬁnd with probability greater than 1 − δ,  (cid:15)k(f ) ≤ ˆ(cid:15)k(f ) + 2Rk(F) + Ck  (cid:114)  log 1/δ n  .  Averaging over all tasks yields  1 K  K (cid:88)  k=1  (cid:15)k(f ) ≤  1 K  K (cid:88)  k=1  ˆ(cid:15)k(f ) + 2  1 K  Since the loss l is bounded l ≤ 1/2, we can take C = Ck = 1/ of Equation 9.  √  √  K (cid:88)  Rk(F) +  (cid:114)  Ck  1 K  K (cid:88)  k=1  log 1/δ n  .  k=1 √  2, giving the generalization upper bound for the joint error  By Lemma B.2 and the fact 1/  2nK ≤ 1/  2n, the bound in Theorem B.1 is strictly tighter.  B.2. Domain Adaptation Error  (cid:80)nc  Since we test on c ∼ C outside the training set {ck}, we incur error due to domain adaptation from the source domains µc1, . . . , µcK to target domain µc with µ being the true distribution. Denote the corresponding empirical distributions of n samples per task by ˆµc = 1 . For different c and c(cid:48), the domains µc and µc(cid:48) may have largely disjoint support, nc leading to very high KL divergence. However, if c and c(cid:48) are close, samples xc ∼ µc and xc(cid:48) ∼ µc(cid:48) may be close in the domain X with respect to the metric (cid:107) · (cid:107)X . For example, if the external forces c and c(cid:48) are close, the distance between the velocity ﬁelds (cid:107)xc − xc(cid:48)(cid:107) may be small. Choosing a measurement between µc and µc(cid:48) which depends on the metric in the space X such as the Wasserstein distance W1(µc, µc(cid:48)) is thus appropriate. The bound from (Redko et al., 2017) applies well to our setting as such:  i=1 δx(i)  c  Theorem B.4 ((Redko et al., 2017),Theorem 2). Let λc = minf ∈F N (dim(X )) such that for n > N , for any hypothesis f , with probability at least 1 − δ,  (cid:16)  (cid:15)c(f ) + 1/K (cid:80)K  k=1 (cid:15)ck (f )  (cid:17)  . There is N =  (cid:15)c(f ) ≤  1 K  K (cid:88)  k=1  (cid:32)  (cid:15)ck (f ) + W1  ˆµc,  (cid:33)  ˆµck  1 K  K (cid:88)  k=1  + (cid:112)2 log(1/δ)  (cid:16)(cid:112)1/n + (cid:112)1/(nK)  (cid:17)  + λc.  Proof. We apply (Redko et al., 2017) Theorem 2 to target domain µT = µc and joint source domain µS = 1/K (cid:80)K with empirical samples ˆµT = ˆµc and ˆµS = 1/K (cid:80)K  k=1 ˆµck .  k=1 µck  B.3. Encoder versus Prediction Network Error  Our goal is to learn a joint hypothesis h over the entire domain X in two steps, ﬁrst inferring the task c and then inferring xt+1 conditioned on c. Error from DyAd may result from either the encoder gφ or the prediction network fθ. Our hypothesis space has the form {x (cid:55)→ fθ(x, gφ(x))} where φ and θ are the weights of the encoder and prediction network respectively.  Meta-Learning Dynamics Forecasting Using Task Inference  Let (cid:15)X be the error over the entire domain X , that is, for all c. Let (cid:15)enc(gφ) = Ex∼X (L1(g(x), gφ(x)) be the encoder error where g : X → C is the ground truth. We state a result that decomposes the ﬁnal error into that attributable to the encoder and that to the prediction network.  Proposition B.5. Assume c (cid:55)→ fθ(·, c) is Lipschitz continuous with Lipschitz constant γ uniformly in θ. Then we bound  (cid:15)X (fθ(·, gφ(·))) ≤ γ(cid:15)enc(gφ) + Ec∼C [(cid:15)c(fθ(x, c))]  (10)  where the ﬁrst term is the error due to the encoder incorrectly identifying the task and the second term is the error due the prediction network alone.  The hypothesis in the second term consists of the prediction network combined with the ground truth task label x (cid:55)→ fθ(x, g(x)).  Proof. By the triangle inequality and linearity of expectation,  (cid:15)X (fθ(·, gφ(·))) = Ec∼C [Ex∼µc [(cid:107)fθ(x, gφ(x)) − f (x)(cid:107)Y ]]  ≤ Ec∼C [Ex∼µc [(cid:107)fθ(x, gφ(x)) − fθ(x, c)(cid:107)Y ]] + Ec∼C [Ex∼µc [(cid:107)fθ(x, c)(cid:107)Y − f (x)(cid:107)Y ]] .  By Lipschitz continuity,  ≤ Ec∼C [Ex∼µc [γ(cid:107)gφ(x) − c(cid:107)C]] + Ec∼C [Ex∼µc [(cid:107)fθ(x, c)(cid:107)Y − f (x)(cid:107)Y ]] ,  which, since g(x) = c and by linearity of expectation,  = γEc∼C [Ex∼µc [(cid:107)gφ(x) − g(x)(cid:107)C]] + Ec∼C [Ex∼µc [(cid:107)fθ(x, c)(cid:107)Y − f (x)(cid:107)Y ]]  and by deﬁnition of (cid:15)enc and (cid:15)c,  as desired.  = γ(cid:15)enc(gφ) + Ec∼C [(cid:15)c(fθ(x, c))]  By combining Theorem B.1, Proposition B.5, and Theorem B.4, we can bound the generalization error in terms of the empirical error of the prediction network on the source domains, the Wasserstein distance between the source and target domains, and the empirical error of the encoder.  Let G = {gφ : X → C} be the task encoder hypothesis space. Denote the empirical risk of the encoder gφ with respect to X by ˆ(cid:15)enc(gφ). Proposition B.6. Assuming the hypotheses of Theorem B.1, Proposition B.5, and Theorem B.4,  (cid:15)X (fθ(·, gφ(·))) ≤ γˆ(cid:15)enc(gφ) +  1 K  K (cid:88)  k=1  ˆ(cid:15)ck (fθ(·, ck)) + 2γR(G) + 2R(F)  (cid:114)  + (γ + 1)  (cid:34)  log(1/δ) 2nK (cid:32)  (cid:16)(cid:112)1/n + (cid:112)1/(nK) + (cid:112)2 log(1/δ) (cid:35) (cid:33)  (cid:17)  + Ec∼C  W1  ˆµc,  ˆµck  + λc  .  1 K  K (cid:88)  k=1  Proof. We start with the bound of Proposition B.5,  By Theorem B.1 or (Mohri et al., 2018), Theorem 3.3, we can bound  (cid:15)X (fθ(·, gφ(·))) ≤ γ(cid:15)enc(gφ) + Ec∼C [(cid:15)c(fθ(x, c))] .  (cid:15)enc(gφ) ≤ ˆ(cid:15)enc(gφ) + 2R(G) +  (cid:114)  log(1/δ) 2nK  .  (11)  (12)  Meta-Learning Dynamics Forecasting Using Task Inference  In order to apply Theorem B.1 to the risk (cid:15)c and relate it to the empirical risk, we need to ﬁrst relate the error on the target domain back to the source domain of our empirical samples. By Theorem B.4,  (cid:15)c(fθ(·, c)) ≤  1 K  K (cid:88)  k=1  (cid:32)  (cid:15)ck (fθ(·, ck)) + W1  ˆµc,  (cid:33)  ˆµck  1 K  K (cid:88)  k=1  + (cid:112)2 log(1/δ)  (cid:16)(cid:112)1/n + (cid:112)1/(nK)  (cid:17)  + λc.  (13)  Applying Theorem B.1, this is  ≤  1 K  K (cid:88)  k=1  ˆ(cid:15)ck (fθ(·, ck))+2R(F)+  (cid:114)  log 1/δ 2nK  (cid:32)  +W1  ˆµc,  (cid:33)  ˆµck  1 K  K (cid:88)  k=1  +(cid:112)2 log(1/δ)  (cid:16)(cid:112)1/n + (cid:112)1/(nK)  (cid:17)  +λc. (14)  Substituting (12) and (14) into (11) gives  (cid:15)X (fθ(·, gφ(·))) ≤ γ  ˆ(cid:15)enc(gφ) + 2R(G) +  (cid:32)  (cid:114)  (cid:33)  log(1/δ) 2nK  + Ec∼C  (cid:34)  1 K  K (cid:88)  k=1  ˆ(cid:15)ck (fθ(·, ck)) + 2R(F) +  (cid:114)  log 1/δ 2nK  (cid:32)  + W1  ˆµc,  (cid:33)  ˆµck  1 K  K (cid:88)  k=1  + (cid:112)2 log(1/δ)  (cid:17) (cid:16)(cid:112)1/n + (cid:112)1/(nK)  (cid:35)  + λc  .  Finally using linearity of the expectation over c ∼ C, removing it where there is no dependence on c, and rearranging terms gives the result.9 1 0 2  p e S 9 2  ]  G L . s c [  1 v 1 7 3 3 1 . 9 0 9 1 : v i X r a  GRADIENT DESCENT: THE ULTIMATE OPTIMIZER  Kartik Chandra 1 Erik Meijer 2 Samantha Andow 2 Emilio Arroyo-Fang 2 Irene Dea 2 Johann George 2 Melissa Grueter 2 Basil Hosmer 2 Stefﬁ Stumpos 2 Alanna Tempest 2 Shannon Yang 2  ABSTRACT Working with any gradient-based machine learning algorithm involves the tedious task of tuning the optimizer’s hyperparameters, such as the learning rate. There exist many techniques for automated hyperparameter optimiza- tion, but they typically introduce even more hyperparameters to control the hyperparameter optimization process. We propose to instead learn the hyperparameters themselves by gradient descent, and furthermore to learn the hyper-hyperparameters by gradient descent as well, and so on ad inﬁnitum. As these towers of gradient-based optimizers grow, they become signiﬁcantly less sensitive to the choice of top-level hyperparameters, hence decreasing the burden on the user to search for optimal values.  1  INTRODUCTION  Usually we think of using gradient descent to optimize weights and other parameters of neural networks. Differen- tiable programming languages promise to make arbitrary code differentiable, allowing us to use gradient descent to optimize any program parameter that would otherwise be hard-coded by a human. Hence, there is no reason we should not be able to use gradient descent to optimize quantities other than the weights of a neural network, for instance hyperparameters like the gradient descent step size. But we don’t need to stop there, and we can just as well learn the hyper-hyperparameters used to optimize those hyperpa- rameters, along with other constants occurring in gradient descent optimizers (Ruder, 2016).  In this paper we show that differentiable programming makes it practical to tune arbitrarily tall recursive towers of optimizers, where each optimizer adjusts the hyperparame- ters of its descendant:  Like Baydin et al. (2017), we independently rediscov- ered the idea of Almeida et al. (1999) to implement efﬁcient on-line hyperparameter optimizers by gradi- ent descent. However, we generalize the approach of Baydin et al. in several dimensions.  In Section 3 we show how to craft the automatic dif- ferentiation (AD) computation graph such that the cal-  1Stanford University, Palo Alto, California, USA 2Facebook, Menlo Park, California, USA. Correspondence to: Kartik Chandra <kach@cs.stanford.edu>, Erik Meijer <erikm@fb.com>.  Preprint under review. Copyright 2019 by the author(s).  Figure 1. The “hyperoptimization surface” described in Section 2. The thin solid traces are of vanilla SGD optimizers with a variety of choices for the hyperparameter α. The thick orange trace is our desired behavior, where the “hyperoptimizer” learns an optimal α over the course of the training, and thus outperforms the vanilla optimizer that begins at the same α.  culations to derive the hyperparameter update formula performed manually by Baydin et al. come “for free” as a result of reverse-mode automatic differentiation, just like the update rule for the weights does. This eliminates the need for certain tedious manual compu- tations.  In Section 3.3 we utilize this newfound power to dif- ferentiate with respect to hyperparameters beyond just the learning rate, such as Adam’s β1, β2, and (cid:15), and in Section 4.2 show empirically that learning these extra hyperparameters improves results.  Furthermore, in Section 3.4, we realize the vision of  Gradient Descent: The Ultimate Optimizer  recursively stacking multiple levels of hyperparame- ter optimizers that was only hypothesized by Baydin et al. Hyperparameter optimizers can themselves be optimized, as can their optimizers, and so on ad in- ﬁnitum. We demonstrate empirically in Section 4.4 that such towers of optimizers are scalable to many recursive levels.  Section 4.3 shows that taller stacks of hyperoptimizers are indeed signiﬁcantly less sensitive to the choice of top-level hyperparameters. This reduces the burden on humans responsible for tuning the hyperparameters — rather than “seeking needles in a haystack,” we can instead simply “shoot ﬁsh in a barrel.”  2 DIFFERENTIATING OPTIMIZERS  What does it mean to optimize an optimizer? Consider Figure 1, which depicts a “hyperoptimization surface” for using stochastic gradient descent (SGD) to optimize some loss function f . Each thin trace is a loss curve of SGD with the given step size hyperparameter α. These loss curves form cross-sections of the surface along the α axis, parallel to the batch-number/loss plane.  Notice that with α < 10−1 SGD performs poorly be- cause the parameters update too slowly to make meaningful progress by the end of the training period. Similarly, for α > 101 SGD also performs poorly because the parameter updates are too noisy to converge to the optimum. The opti- mal hyperparameter is therefore somewhere between 10−1 and 101. If we were training this model, we would have to manually discover this range by experimentation.  Instead, imagine if we could utilize a variant of SGD to climb down this surface no matter where we started, as demonstrated by the thick orange trace. Unlike the thin traces of vanilla SGD, the thick orange trace is not con- ﬁned to a single plane — though it begins at the highly sub-optimal α = 10−4, it gradually “learns” to increase α, and attains a ﬁnal loss function on par with the optimal hy- perparameter for vanilla SGD. In Section 3 we will describe how to achieve this by adjusting α at each step of SGD.  Note that our approach is not limited to tuning just step sizes. The Adam optimizer, for example, already intelli- gently adjusts the step size for each parameter based on past progress (Kingma & Ba, 2014). However, Adam still has its own ﬁxed hyperparameters: the learning rate α, the two moment coefﬁcients β1, β2, and the factor (cid:15) used to avoid division by zero. For instance, the recommended default for (cid:15) is often quoted as 10−8, but as the TensorFlow documenta- tion remarks, sometimes it is better to use 1.0 or 0.1 instead. As we show in Section 3.3, we can tune these additional hyperparameters automatically.  Existing work (Maclaurin et al., 2015; Pedregosa, 2016; Franceschi et al., 2017), attempts to learn a single optimal hyperparameter for the entire training history — by gradient descent on the dashed black “U” in Figure 1. This is inefﬁ- cient because it requires memory to store the entire unrolled run. Our work, along with Baydin et al. (2017) and Almeida et al. (1999), uses a stochastic variant of the above instead: perform incremental updates to the hyperparameter in par- allel with the learning. Since each incremental update only depends on its immediate history, we can “forget” all but a constant amount of information of the unrolled run that non-stochastic approaches have to “remember” and fully differentiate through.  3  IMPLEMENTATION  Consider some stochastic loss function f that we want to minimize using gradient descent, and let wi be the weights at the beginning of step i. Let us ﬁrst recall the standard weight update rule at step i for SGD, using some (ﬁxed) step size α:  wi+1 = wi − α  ∂f (wi) ∂wi  We would like to update α as well at each step, so we will index it with the step number also: let αi be the step size at the beginning of step i. At each step, we will ﬁrst update the step size to αi+1 using some update rule yet to be derived, and then use the updated step size αi+1 to update the weights from wi to wi+1.  αi+1 = αi − adjustment for αi  wi+1 = wi − αi+1  ∂f (wi) ∂wi  What should the adjustment for αi be? By analogy to w, we want to adjust αi in the direction of the gradient of the loss function with respect to αi, scaled by some hyper-step size κ. In other words, the adjustment should be κ(∂f (wi)/∂αi). Section 4.3 addresses the practical matter of the selection of this hyper-hyperparameter — for now, we will take κ as a given ﬁxed constant. Our modiﬁed update rule is therefore:  αi+1 = αi − κ  ∂f (wi) ∂αi  wi+1 = wi − αi+1  ∂f (wi) ∂wi  (1)  (2)  All that remains is to compute ∂f (wi)/∂αi in equation (1).  In the next section, we review how Baydin et al. (2017) compute this derivative by hand, obtaining an elegant and efﬁciently-computable expression. In the section that fol- lows, we show how we can compute the partial derivative for the step size update completely automatically, exactly like the partial derivative for the weights. This makes it possible to generalize our approach in many different ways.  Gradient Descent: The Ultimate Optimizer  3.1 Computing the step-size update rule by hand  One option to compute ∂f (wi)/∂αi, explored by Baydin et al. (2017), is to proceed by direct manual computation of the partial derivative. Applying the chain rule to the derivative in question, we can compute  ∂f (wi) ∂αi  =  =  =  ∂f (wi) ∂wi  ∂f (wi) ∂wi ∂f (wi) ∂wi ∂wi ∂αi (cid:16)  ∂  wi−1 − αi  (cid:17)  ∂f (wi−1) ∂wi−1  (cid:18)  −  ∂αi ∂f (wi−1) ∂wi−1  (cid:19)  (3)  (4)  (5)  where (4) is obtained by substituting the update rule in (2) for wi and (5) is obtained by observing that wi−1 does not depend on αi, and can therefore be treated as a constant. In this particular case, the resulting expression is simple and elegant: the dot product of the preceding two gradients with respect to the weights, which as we see from equation (2) would have already been computed in order to update the weights themselves. We only need to remember the previous derivate, so this is very memory efﬁcient and time efﬁcient.  The direct-computation strategy works well when the update rule is easy to differentiate by hand with respect to the hyper- parameter — in SGD as above, it is simply a multiplication by a constant, whose derivative is trivial. However, this is not always the case. Consider, for example, the update rule for the Adam optimizer, as in Algorithm 1 of Kingma & Ba (2014), which has a much more complicated dependence on the hyperparameters β1 and β2. Differentiating the update rule by hand, we obtain the following results, caveat emptor:  ∂wt ∂αt  ∂wt ∂β1t  ∂wt ∂β2t ∂wt ∂(cid:15)t  = −  = −  ˆmt √ (cid:0)(cid:15)t + (cid:16)  αt  (cid:1)  ˆvt − ∂f (wt−1) ∂wt−1 (cid:0)1 − β1 (cid:18)  √  αt ˆmt  ˆvt  −  =  =  αt ˆmt √ ˆvt  (cid:0)(cid:15)t +  (cid:1)2  (cid:17)  ˆmt  √  (t−1) + mt−1 + tβ1 t (cid:1) (cid:1) (cid:0)(cid:15)t + t t (cid:16) ∂f (wt−1) (cid:17)2 ∂wt−1  ˆvt  + vt−1 + tβ2 √  2vt  (cid:0)(cid:15)t +  (cid:1)2  ˆvt  (cid:19)  (t−1) t  ˆvt  We see how again the derivatives of the loss function with respect to the hyperparameters ∂wt/∂β1t and ∂wt/∂β2t are deﬁned in terms of the previous value of the the deriva- tive for the actual parameters ∂f (wt−1)/∂wt−1, but embed- ded within a much more complex expression than before. Clearly, this manual approach to compute the hyperparam- eter update rules does not scale. However, with a little  bit of care we can actually compute these derivatives au- tomatically by backwards AD, just like we do for regular parameters.  3.2 Computing the step-size update rule  automatically  In order to compute ∂f (wi)/∂αi automatically, let us ﬁrst brieﬂy review the operational mechanics of reverse-mode automatic differentiation. Frameworks that provide reverse- mode AD (Griewank, 2012) to compute ∂f (wi)/∂αi do so by building up a backwards computation graph as the function is computed forwardly. For example, when a user computes the loss function f (wi), the framework internally stores a DAG whose leaves are the weights wi, whose in- ternal nodes are intermediate computations (for a DNN the outputs of each successive layer), and whose root is the con- crete loss function such as LogSoftmax. The framework can then backpropagate from the backwards computation graph created for this root node, depositing gradients in each node as it descends, until the weights wi at the leaf nodes have accumulated the gradient ∂f (wi)/∂wi.  Once the gradient ∂f (wi)/∂wi is computed by the back- wards pass, we can then continue to update the weights wi+1 = wi − α/∂f (wi)∂wi as shown above, and repeat the cycle for the next training batch. However, an important consideration is for the weights to be “detached” from the computation graph before each iteration of this algorithm — that is, for the weights to be forcibly converted to leaves of the graph by removing any inbound edges. The effect of the “detach” operation is depicted in Figure 2a. If this step is skipped, the next backpropagation iteration will continue beyond the current weights into the past. This is problematic in a couple of ways, depending on how the weight update is implemented. If the weight update is implemented as an in-place operation, then this will yield incorrect results as more and more gradients get accumulated onto the same node of the computation graph. If the weight update is implemented by creating a fresh node, then over time the computation graph will grow taller linearly in the number of steps taken; because backpropagation is linear in the size of the graph, the overall training would become quadratic-time and intractable.  Let’s peek inside the implementation of SGD in Py- Torch (Paszke et al., 2017) as of commit bb41e6 to see how this cutting-off is implemented in the actual source code:  # line 91 of sgd . py d p = p . grad.data # ... momentum calculations omitted # line 106 p.data. a d d (−group [ ’ lr ’] , d p )  Here, p represents the parameter being optimized (i.e. wi)  Gradient Descent: The Ultimate Optimizer  (a) Computation graph of SGD with a ﬁxed hyperparameter α.  (b) Computation graph of SGD with a continuously-updated hy- perparameter αi.  Figure 2. Comparing the computation graphs of vanilla SGD and HyperSGD.  and lr is the learning rate (i.e. α). A few more PyTorch- speciﬁc clariﬁcations: p.grad retrieves the gradient of the loss function with respect to p. The call to .add up- dates p in-place with the product of the arguments, that is, with −α · ∂f (w)/∂w. Most importantly, the highlighted calls to .data implement the “detachment” by referring to the datastore of that variable directly, ignoring the associ- ated computation graph information. Note that in vanilla PyTorch the step size is not learned, so no call to .data is required for the learning rate because it is internally stored as a raw Python ﬂoat rather than a differentiable variable tracked by PyTorch.  For the sake of consistency let us rewrite this function, re- naming variables to match the above discussion and promot- ing alpha to a differentiable variable in the form of a rank-0 tensor. In order to keep the computation graph clear, we will also update weights by creating fresh nodes rather than to change them in-place.  def SGD .  i n i t  ( self , alpha ):  self . alpha = tensor ( alpha )  def SGD . adjust ( w ):  d w = w . grad.detach() w = w.detach() − self . alpha.detach() ∗ d w  The highlighted calls to .detach() correspond to detaching the weights and their gradients. Now, in order to have backpropagation deposit the gradient with respect to αi as well as wi, we can simply refrain from detaching αi from the graph, detaching instead its parents. This is depicted in Figure 2b.  Notice in particular that because we want to compute ∂f (wi)/∂αi the edge from αi to wi needs to remain in- tact. To implement this, instead of calling .detach() on alpha directly, we instead call .detach() on its parents  when adjusting it using equation (1). This change yields the following fully-automated hyperoptimization algorithm1:  def HyperSGD . adjust ( w ):  # update alpha using Equation (1) d a l p h a = self . alpha . grad.detach() self . alpha = self . alpha.detach() −  kappa.detach() ∗ d a l p h a  # update w using Equation (2) d w = w . grad.detach() w = w.detach() − self . alpha.detach() ∗ d w  Notice that because we are only extending the computation graph by a little extra amount (corresponding to evaluat- ing the optimizer), the backwards AD pass should not be signiﬁcantly more computationally expensive. Section 4.4 presents an empirical evaluation of the computational cost to hyperoptimization.  3.3 The HyperAdam optimizer  As suggested in previous work (Maclaurin et al., 2015), it should be possible to apply gradient-based methods for tuning hyperparameters of common variations on SGD such as AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012), or Adam (Kingma & Ba, 2014). The above implementation of HyperSGD generalizes quite easily to these optimizers. In this section we demonstrate the HyperAdam optimizer, which mostly follows by analogy to HyperSGD. Unlike previous work, which could only optimize Adam’s learning rate (Baydin et al., 2017), we are able to optimize all four hyperparameters of Adam automatically. Our evaluation in Section 4.2 demonstrates that this indeed useful to do.  1The example code in this section elides some small PyTorch- related details. Appendix A contains the full PyTorch source code for all examples and experiments in this paper.  There are, however, two important subtleties to be aware of.  self . alpha ∗ mhat /  Gradient Descent: The Ultimate Optimizer  First, because the hyperparameters β1 and β2 must be strictly in the domain (0, 1), we clamp the “raw” values to this domain using a scaled sigmoid. Without this step, we might accidentally adjust these values outside their domains, which ultimately leads to arithmetic exceptions.  √  ˆvt, which Second, the Adam optimizer involves the term is continuous but not differentiable at ˆvt = 0. Because Adam normally initializes v0 = 0, backpropagation would fail on the very ﬁrst step due to a division by zero error. We ﬁx this problem by initializing v0 to (cid:15) rather than 0.  These two subtleties reveal a limitation of our automatic approach to hyperparameter optimization: while the domain restrictions are evident in the explicit formulas presented above (notice, for example, the vt in the denominator of the expression for ∂wt/∂β2t derived above, which would im- mediately to signal a user the potential for division-by-zero), they are more difﬁcult to predict and debug if the derivatives are taken automatically. The hyperparameter update rule does not “know” the domains of the hyperparameters, and so it might step too far and lead to a mysterious crash or nan issue. In practice however, this has not been a showstopper.  Implementing these ﬁxes, and remembering to .detach() the Adam intermediate state (i.e. mt−1 and vt−1) in the right place to prevent “leaks” in the backwards AD, we obtain the following implementation:  def HyperAdam . adjust ( w ):  # update Adam hyperparameters by SGD d a l p h a = self . alpha . grad . detach () self . alpha = self . alpha . detach () −  kappa . detach () ∗ d a l p h a  d b e t a 1 = self . beta1 . grad . detach () self . beta1 = self . beta1 . detach () −  kappa . detach () ∗ d b e t a 1  d b e t a 2 = self . beta2 . grad . detach () self . beta2 = self . beta1 . detach () −  kappa . detach () ∗ d b e t a 2 = self . eps . grad . detach ()  d e p s self . eps = self . eps . detach () − kappa . detach () ∗ d e p s  # clamp coefficients to domain (0, 1) b e t a 1 c l a m p = ( tanh ( self . beta1 ) + 1)/2 b e t a 2 c l a m p = ( tanh ( self . beta2 ) + 1)/2  # update w using Adam update rule self . t += 1 g = w . grad . detach () self . m =  b e t a 1 c l a m p ∗ self . m.detach() +  (1 − self . beta1 ) ∗ g  self . v =  b e t a 2 c l a m p ∗ self . v.detach() + (1 − self . beta2 ) ∗ g ∗ g  mhat = self . m / (1 − b e t a 1 c l a m p ∗∗ t ) vhat = self . v / (1 − b e t a 2 c l a m p ∗∗ t ) w = w . detach () −  ( vhat ∗∗ 0.5 + self . eps )  3.4 Stacking Hyperoptimizers Recursively  At this point it is natural to ask whether the hyperopti- mizer can itself be optimized; that is, whether the human- selected hyper-hyperparameter κ to update the hyperparam- eters (e.g. α) can be adjusted by a hyper-hyperoptimizer. The possibility of doing so recursively ad inﬁnitum to ob- tain an optimization algorithm that is highly robust to the top-level human-chosen hypernparameter was hypothesized in Section 5.2 of Baydin et al. (2017). Computing the gra- dients of these higher-order hyperparameters by hand is impossible without knowing the exact sequence of stacked optimizers ahead of time, and as we have shown above, will be extremely tedious and error prone.  However, the ability to compute these gradients automat- ically by backwards AD makes it possible to realize this vision. To do so, let us revisit our previous implementation of HyperSGD. Notice that there is an opportunity for recur- sion lurking here: the adjustment to alpha can be factored out with a call to SGD.adjust, where SGD’s hyperparameter is kappa.  def HyperSGD . adjust ( w ):  SGD(kappa).adjust(self.alpha) d w = w . grad . detach () w = w . detach () − self . alpha ∗ d w  Because SGD is already careful to properly detach its pa- rameter (typically w, but in this case α), this implementation is functionally identical to the one above. Indeed, any op- timizer that observes this protocol would sufﬁce, so let us abstract out the optimizer as a parameter to HyperSGD:  def HyperSGD .  i n i t  ( self , alpha , opt):  self . alpha = tensor ( alpha ) self.optimizer = opt  def HyperSGD . adjust ( w ):  self.optimizer.adjust(self.alpha) d w = w . grad . detach () w = w . detach () − self . alpha ∗ d w  opt = HyperSGD (0.01 , opt=SGD(kappa))  After this refactoring, ﬁnally, we can recursively feed HyperSGD itself as the optimizer, obtaining a level-2 hyper- optimizer HyperSGD(0.01, HyperSGD(0.01, SGD(0.01))). Similarly, we can imagine taller towers, or towers that mix and match multiple different kinds of optimizers, such as Adam-optimized-by-SGD-optimized-by-Adam.  A natural application of this idea is to automatically learn hyperparameters on a per-parameter basis. For example, when hyperoptimizing Adam with SGD as in Section 3.3, it is extremely beneﬁcial to maintain a separate hyper-step size  Gradient Descent: The Ultimate Optimizer  Optimizer SGD(0.01)  Test acc Time 77.48% 16ms  SGD(0.01) / SGD(0.01) SGD(0.145)  88.35% 16ms 88.81% 16ms  SGD(0.01) / Adam(...) SGD(0.096)  86.80% 23ms 87.71% 16ms  refers Table 1. Hyperoptimizing SGD. The symbol Adam(...) Adam with the standard hyperparameters. Each hyperoptimizer experiment is repeated using the ﬁnal hyperparameters learned by the algorithm.  Optimizer Adam(...) — baseline  Time Test acc 91.09% 38ms  Adam(...) / SGD(10−3) / SGD(10−4) Adam(0.0291, 0.8995, 0.999, -8)  92.74% 43ms 93.74% 40ms  Adamα(...) / SGD(10−3) / SGD(10−4) Adamα(0.0284, *) Adam(...) / Adam(...) Adam(0.013, 0.892, 0.998, -8)  92.64% 37ms 93.42% 41ms 94.35% 41ms 94.75% 36ms  Adamα(...) / Adam(...) Adamα(0.013, ...)  94.01% 31ms 94.39% 32ms  Table 2. Hyperoptimizing Adam. The symbol Adam(...) retains its meaning from Table 1.  (i.e. a separate κ) for each of the four hyperparameters, since they typically span many orders of magnitude. Instead of specifying each κ as a separate top-level hyperparameter, however, we can instead apply a second level of SGD that lets the system automatically learn optimal hyper-step sizes for each Adam hyperparameter separately.  A logical concern is whether this process actually exacer- bates the hyperparameter optimization problem by introduc- ing even more hyperparameters. Baydin et al. (2017) pre- dicted that as the towers of hyperoptimizers grow taller, the resulting algorithms would be less sensitive to the human- chosen hyperparameters, and therefore the overall burden on the user will be reduced. This indeed seems to be the case; Section 4.3 presents an empirical evaluation of this hypothesis.  4 EVALUATION  In this section we evaluate the hyperoptimizers made pos- sible by our system, exploring in particular the beneﬁts of being able to optimize hyperparameters beyond just step size and of higher-order optimization, as well as whether or not there is a signiﬁcant computational cost to automatically  computing derivatives with respect to hyperparameters.  Setting Like authors of previous work Maclaurin et al. (2015); Baydin et al. (2017), we conducted all of our experi- ments were conducted on the MNIST dataset (Lecun et al., 1998) using a neural network with one fully-connected hid- den layer of size 128, tanh activations, and a batch size of 300 run for a single epoch. We implemented the system in PyTorch and ran experiments on a 2.4 GHz Intel CPU with 32GB of memory. The full source code for each of these experiments is presented in Appendix A.  Notation We denote species of hyperoptimizers by their sequence of constituent optimizers with their initial hyper- parameters. The leftmost item adjusts the parameters of the model whereas the rightmost item has ﬁxed hyperpa- rameters. For example, the term “SGD(0) / Adam(0.001, 0.9, 0.999, -8)” indicates that the weights of the neural net- work were adjusted by stochastic gradient descent with a step size that, while initially 0, was adjusted by a regular Adam optimizer with hyperparameters α = 0.001, β1 = 0.9, β2 = 0.999, (cid:15) = 10−8. Adamα denotes an Adam optimizer where only α is optimized as in Baydin et al., and the abbreviations Adam(...) and Adamα(...) denote the respective optimizers with the standard hyperparam- eters (α = 0.001, β1 = 0.9, β2 = 0.999, (cid:15) = 10−8), which are recommended by Kingma & Ba (2014) and are used by default almost universally across software packages.  4.1 Hyperoptimization for SGD  Here we seek to answer two questions: (1) whether an SGD hyperoptimizer performs better than an elementary SGD optimizer2, and (2) whether or not the learned step size outperforms the initial step size. We test the latter property by running a fresh elementary SGD optimizer with the ﬁnal learned step size of the hyperoptimizer.  Table 1 summarizes the results of our experiments, run with an initial step size of 0.01 (see Section 4.3 for a discussion of the sensitivity of these results to this initial step size). We ﬁnd that hyperoptimized SGD outperforms the baseline by a signiﬁcant margin (nearly 10%). This holds even if we use an Adam optimizer to adjust the step size of the SGD optimizer.  Furthermore, when we re-ran the elementary optimizers with the new learned hyperparameters, we found that they typically performed incrementally better than the hyper- parameter itself. This is what Luketina et al. (2016, Sec- tion 3.1) refer to as the “hysteresis” effect of hyperparameter optimization: we cannot reap the beneﬁts of the optimized  2Following previous work (Maclaurin et al., 2015; Baydin et al., 2017), we refer to standard, vanilla “non-hyperoptimized” optimizers as “elementary optimizers.”  Gradient Descent: The Ultimate Optimizer  hyperparameters while they are themselves still in the early stages of being optimized — thus, hyperoptimizers should “lag” slightly behind elementary optimizers that start off with the ﬁnal optimized hyperparameters.  4.2 Hyperoptimization for Adam  In Section 3.3, we described how to apply our system to optimizing the Adam optimizer, which maintains ﬁrst- and second-order momentum information for each parameter it optimizes. Altogether, there are four hyperparameters: a learning rate (α), coefﬁcients for the ﬁrst- and second-order momenta (β1, β2), and an epsilon value ((cid:15)). We tune all four simultaneously, ﬁrst using SGD and then by using Adam itself on the top-level. Our Adam / SGD experiments utilize the higher-order design proposed at the end of Section 3.4 to learn a separate hyper-step size for each hyperparameter of Adam; this is not needed for Adam / Adam because Adam by design already maintains separate information for each parameter it optimizes.  We seek to answer three questions: (1) whether hyperop- timized Adam optimizers perform better than elementary Adam optimizers, (2) whether the learned hyperparameters outperform the baseline, and (3) whether there is a beneﬁt to optimizing all four hyperparameters, as opposed to only optimizing the learning rate as Baydin et al. (2017) do.  Table 2 summarizes the results of our experiments. We ﬁnd that indeed the hyperoptimized Adam optimizer out- performs the elementary Adam optimizer on its “default” settings. As with SGD in Section 4.1, the learned hyperpa- rameters perform incrementally better than the hyperopti- mizer due to the hysteresis effect.  Inspecting the learned hyperparameters, we ﬁnd that the al- gorithm signiﬁcantly raises the learning rate α and slightly lowers β1, but does not signiﬁcantly affect either β2 or (cid:15). Nevertheless, learning β1 does provide a noticeable bene- ﬁt: our hyperoptimized Adam outperforms hyperoptimized Adamα, which can only learn α. Both hyperoptimizers learn similar optimized values for α, but Adamα cannot also adapt β1, and therefore does not perform as well.  4.3 Higher-Order Hyperoptimization  In Section 3.4 we developed an interface for building arbi- trarily tall towers of optimizers. Recall that Baydin et al. (2017) hypothesized that taller towers would yield hyperopti- mizers that were more robust to the top-level human-chosen hyperparameters than elementary optimizers are.  To validate this behavior of higher-order hyperoptimizers, we ran the above benchmark with towers of SGD-based hyperoptimizers of increasing heights, where each layer of SGD started with the same initial step size α0. Figure 3 shows the results of this experiment. It is indeed the case  Figure 3. As we stack more and more layers of SGD, the resulting hyperoptimizer is less sensitive to the initial choice of hyperparam- eters.  that the taller the hyperoptimizer stack, the less sensitive the results become for the top-level hypern-parameters; roughly one order of magnitude per step until after 5-6 levels the graphs converge.  Notice also that the sensitivity only decreases for smaller initial step sizes; all hyperoptimizers performed poorly be- yond α0 > 102. We hypothesize that it is difﬁcult to recover from a too-high initial step size because dramatic changes in parameters at each step make the stochastic loss function too noisy. In comparison, if the hyperoptimizer’s initial step size is too low, then the weights do not change very dra- matically at each step, and as a result a “signal” to increase the step size can be extracted from a series of stochastic gradient descent steps.  4.4 Performance  When we stack a new hyperparameter optimizer, we are effectively adding a new layer to the computation graph.  Gradient Descent: The Ultimate Optimizer  of backpropagating through the loss function. This makes sense: the additional work at each level is only the compu- tational cost of backpropagating through the new top-level optimizer, which is typically much simpler than the ma- chine learning model itself. Indeed, the difference in slopes between higher-order SGD in Figure 4a and higher-order Adam in Figure 4b is simply because Adam is a more com- plex optimizer, requiring more computation to differentiate through.  In summary, we ﬁnd that in practice higher-order hyper- optimization is an extremely lightweight addition to any machine learning model with great beneﬁts.  (a) Higher-order hyperoptimization performance with SGD.  5 RELATED WORK  Hyperparameter optimization has a long history, and we refer readers interested in the full story to a recent survey by Feurer & Hutter (2019).  Most existing work on gradient-based hyperparameter op- timization (Bengio, 2000; Domke, 2012; Maclaurin et al., 2015; Pedregosa, 2016; Franceschi et al., 2017) has focused on computing hyperparameter gradients after several iter- ations of training, which is computationally expensive be- cause of the need to backpropagate through much more computation. Baydin et al. (2017), building on a technique ﬁrst published by Almeida et al. (1999), propose instead up- dating hyperparameters at each step. Luketina et al. (2016) apply a similar technique to regularization hyperparameters, though they explicitly note that their proposed method could work in principle for any continuous hyperparameter.  As discussed above, we expand upon this latter line of work in three directions: (1) by optimizing hyperparameters be- yond just the learning rate; (2) by fully automating this pro- cess, rather than requiring manual derivative computations; and (3) by realizing the vision of recursively constructing higher-order hyperoptimizers and evaluating the resulting algorithms.  6 FUTURE WORK  Convergence of hyperparameters Like Baydin et al. (2017), we found that our hyperparameters converge ex- tremely quickly. Further investigation is required to under- stand the dynamics of the higher-order hyperparameters. If there is indeed a compelling theoretical reason for this rapid convergence, it would suggest a form of higher-order “early stopping” where the hyperoptimizer monitors its hy- perparameters’ convergence, and at some point decides to freeze its hyperparameters for the remainder of training. Be- sides the obvious performance improvement, this may allow the system to leverage the existing implicit regularization behavior exhibited by “vanilla” SGD.  (b) Higher-order hyperoptimization performance with Adam.  Figure 4. As the stacks of hyperoptimizers grow taller, each step of SGD takes longer by a small constant factor, corresponding to the extra step of stepping one node further in the backwards AD computation graph.  This corresponds to extending each step of Figure 2b further to the left by yet another node. Thus, with a hyperoptimizer stack of height n we would obtain a computation graph of size O(n) at each step, which means backpropagation takes time O(n). We should therefore expect training with a stack of n hyperoptimizers to take time O(n).  To test this hypothesis, we extended the benchmark from Section 4.3 to much taller stacks, up to height 50. Note that these experiments are meant to stress-test the system with signiﬁcantly taller stacks than would typically be necessary (recall from Section 4.3 that the stacks of hyperoptimizers need not be taller than 3-4 to be highly effective).  As shown in Figure 4, higher-order hyperoptimization is indeed asymptotically linear-time in the height of the opti- mizer stack. Note how the slope of this linear relationship is quite small compared to the ﬁxed computational cost  Gradient Descent: The Ultimate Optimizer  Scaling up to larger models While existing work on gradient-based hyperparameter optimization has primarily been evaluated in small-scale settings such as MNIST, au- tomated hyperparameter tuning is particularly important in large-scale settings where training is computationally expen- sive, limiting the amount of manual hyperparameter tuning that can be done. Nonetheless, the choice of hyperparam- eters is still crucial: for example, a recent study improved signiﬁcantly upon the state of the art in an NLP task sim- ply by (manually) adjusting hyperparameters; indeed, they found that the performance was highly sensitive to Adam’s (cid:15) and β1 hyperparameters (Liu et al., 2019). A natural next step, therefore, is investigating the effectiveness of higher- order hyperoptimization in automatically reproducing such results.  hyperregularization The  Higher-order hyper- regularizer of Luketina et al. (2016) could be combined with the recursive “higher-order” approach described in this paper in order to derive highly robust regularizers. We note that there is a clear connection between hyper-regularizers and hyperpriors in Bayesian inference; we leave further study of this connection to future work.  7 CONCLUSION  In this paper, we presented a technique to enhance optimiz- ers such as SGD and Adam by allowing them to tune their own hyperparameters by gradient descent. Unlike existing work, our proposed hyperoptimizers learn hyperparameters beyond just learning rates, require no manual differentiation by the user, and can be stacked recursively to many levels. We described in detail how to implement hyperoptimizers in a reverse-mode AD system. Finally, we demonstrated empirically three beneﬁts of hyperoptimizers: that they out- perform elementary optimizers, that they are less sensitive to human-chosen hyperparameters than elementary optimizers, and that they are highly scalable.  REFERENCES  Almeida, L. E., Langlois, T., do Amaral, J. F. M., and Plakhov, A. Parameter adaptation in stochastic optimiza- tion. In On-Line Learning in Neural Networks, 1999.  Baydin, A. G., Cornish, R., Mart´ınez-Rubio, D., Schmidt, M., and Wood, F. D. Online learning rate adaptation with hypergradient descent. CoRR, abs/1703.04782, 2017. URL http://arxiv.org/abs/1703.04782.  Bengio, Y. Gradient-based optimization of hyperparame- ters. Neural Computation, 12(8):1889–1900, 2000. doi: 10.1162/089976600300015187. URL https://doi. org/10.1162/089976600300015187.  Domke, J. Generic methods for optimization-based In Lawrence, N. D. and Girolami, M. modeling. (eds.), Proceedings of the Fifteenth International Con- ference on Artiﬁcial Intelligence and Statistics, vol- ume 22 of Proceedings of Machine Learning Research, pp. 318–326, La Palma, Canary Islands, 21–23 Apr 2012. PMLR. URL http://proceedings.mlr. press/v22/domke12.html.  Duchi, J., Hazan, E., and Singer, Y. Adaptive subgra- dient methods for online learning and stochastic opti- mization. J. Mach. Learn. Res., 12:2121–2159, July 2011. ISSN 1532-4435. URL http://dl.acm.org/ citation.cfm?id=1953048.2021068.  Feurer, M. and Hutter, F. Hyperparameter Optimiza- tion, pp. 3–33. Springer International Publishing, Cham, 2019. ISBN 978-3-030-05318-5. doi: 10.1007/ 978-3-030-05318-5 1. URL https://doi.org/10. 1007/978-3-030-05318-5_1.  Franceschi, L., Donini, M., Frasconi, P., and Pontil, M. Forward and reverse gradient-based hyperparam- eter optimization. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Confer- ence on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1165–1173, Interna- tional Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. URL http://proceedings.mlr. press/v70/franceschi17a.html.  Griewank, A. Who invented the reverse mode of differen- tiation? Optimization Stories, Documenta Matematica, Extra Volume ISMP (2012):389–400, 2012.  Kingma, D. and Ba, J. Adam: A method for stochastic optimization. International Conference on Learning Rep- resentations, 12 2014.  Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278–2324, Nov 1998. ISSN 0018-9219. doi: 10.1109/5.726791.  Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/ abs/1907.11692.  Luketina, J., Berglund, M., Greff, K., and Raiko, T. Scal- able gradient-based tuning of continuous regularization In Proceedings of the 33rd Inter- hyperparameters. national Conference on International Conference on Machine Learning - Volume 48, ICML’16, pp. 2952– 2960. JMLR.org, 2016. URL http://dl.acm.org/ citation.cfm?id=3045390.3045701.  Gradient Descent: The Ultimate Optimizer  Maclaurin, D., Duvenaud, D., and Adams, R. P. Gradient- based hyperparameter optimization through reversible learning. In Proceedings of the 32Nd International Con- ference on International Conference on Machine Learn- ing - Volume 37, ICML’15, pp. 2113–2122. JMLR.org, URL http://dl.acm.org/citation. 2015. cfm?id=3045118.3045343.  Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. Automatic differentiation in PyTorch. In NIPS Autodiff Workshop, 2017.  Pedregosa, F. Hyperparameter optimization with ap- In Proceedings of the 33rd Inter- proximate gradient. national Conference on International Conference on Machine Learning - Volume 48, ICML’16, pp. 737– 746. JMLR.org, 2016. URL http://dl.acm.org/ citation.cfm?id=3045390.3045469.  Ruder, S. An overview of gradient descent optimization algorithms. CoRR, abs/1609.04747, 2016. URL http: //arxiv.org/abs/1609.04747.  Zeiler, M. D.  ADADELTA: An adaptive learning rate method. CoRR, abs/1212.5701, 2012. URL http://dblp.uni-trier.de/db/journals/ corr/corr1212.html#abs-1212-5701.  A CODE LISTING  Gradient Descent: The Ultimate Optimizer  Below is the full source code for the implementation described in Section 3 and all the experiments reported in Section 4.  # h y p e r o p t . py import math  import import  t o r c h t o r c h v i s i o n  import import import  t o r c h . nn a s nn t o r c h . nn . f u n c t i o n a l t o r c h . o p t i m a s o p t i m  a s F  c l a s s O p t i m i z a b l e :  i s  t h e  ’ ’ ’ T h i s o p t i m i z e d , h y p e r o p t i m i z a b i l i t y . i n t e r f a c e w h i c h d o e s n o t g i v e u s enough c o n t r o l a b o u t  f o r a n y t h i n g t h a t h a s p a r a m e t e r s t o r c h . nn . Model b u t w i t h t h e  ( S p e c i f i c a l l y ,  t o r c h . nn . Model u s e s  i n t e r f a c e  somewhat  l i k e  t h a t n e e d t o be r i g h t p l u m b i n g f o r t h e P a r a m e t e r t h e d e t a c h m e n t s . )  Nominal o p e r a t i o n o f an O p t i m i z a b l e a t  t h e  l o w e s t  l e v e l  i s a s  f o l l o w s :  o = M y O p t i m i z a b l e ( . . . ) o . i n i t i a l i z e ( ) l o o p {  o . b e g i n ( ) o . z e r o g r a d ( ) l o s s = −−c o m p u t e l o s s l o s s . backward ( ) o . a d j u s t ( )  f u n c t i o n f r o m p a r a m e t e r s −−  }  O p t i m i z a b l e s ’ ’ ’ d e f  i n i t  r e c u r s i v e l y h a n d l e u p d a t e s  t o t h e i r o p t i m i z ∗ e r s ∗ .  ( s e l f , p a r a m e t e r s , o p t i m i z e r ) :  s e l f . p a r a m e t e r s = p a r a m e t e r s # a d i c t mapping names s e l f . o p t i m i z e r = o p t i m i z e r s e l f . a l l p a r a m s w i t h g r a d i e n t s = [ ]  # w h i c h m u s t  t o t e n s o r s  i t s e l f be O p t i m i z a b l e !  d e f  i n i t i a l i z e ( s e l f ) : ’ ’ ’ p a s s  I n i t i a l i z e p a r a m e t e r s , e . g . w i t h a Kaiming i n i t i a l i z e r .  ’ ’ ’  d e f b e g i n ( s e l f ) :  ’ ’ ’ E n a b l e g r a d i e n t f o r name , param i n s e l f . p a r a m e t e r s . i t e m s ( ) :  t r a c k i n g on c u r r e n t p a r a m e t e r s .  ’ ’ ’  param . r e q u i r e s g r a d ( ) # k e e p g r a d i e n t param . r e t a i n g r a d ( ) s e l f . a l l p a r a m s w i t h g r a d i e n t s . a p p e n d ( param )  # e v e n i f n o t a l e a f . . .  i n f o r m a t i o n . . .  s e l f . o p t i m i z e r . b e g i n ( )  d e f z e r o g r a d ( s e l f ) :  ’ ’ ’ S e t a l l g r a d i e n t s f o r param i n s e l f . a l l p a r a m s w i t h g r a d i e n t s : param . g r a d = t o r c h . z e r o s ( param . s h a p e )  t o z e r o .  ’ ’ ’  s e l f . o p t i m i z e r . z e r o g r a d ( )  ’ ’ ’ N o t e : a t f u n c t i o n .  ’ ’ ’  t h i s p o i n t you would p r o b a b l y  c a l l  . b a c k w a r d s ( ) on t h e  l o s s  d e f  a d j u s t ( s e l f ) : ’ ’ ’ Update p a r a m e t e r s p a s s  ’ ’ ’  c l a s s M N I S T F u l l y C o n n e c t e d ( O p t i m i z a b l e ) :  ’ ’ ’  Gradient Descent: The Ultimate Optimizer  t h e MNIST t a s k . T h i s  i s O p t i m i z a b l e b u t n o t  i t s e l f  A f u l l y −c o n n e c t e d NN f o r an o p t i m i z e r . ’ ’ ’ d e f  i n i t  ( s e l f , num inp , num hid , num out , o p t i m i z e r ) :  p a r a m e t e r s = { ’w1 ’ : ’ b1 ’ : ’w2 ’ : ’ b2 ’ :  t o r c h . z e r o s ( num inp , num hid ) . t ( ) , t o r c h . z e r o s ( num hid ) . t ( ) , t o r c h . z e r o s ( num hid , num out ) . t ( ) , t o r c h . z e r o s ( num out ) . t ( )  } s u p e r ( ) .  i n i t  ( p a r a m e t e r s , o p t i m i z e r )  i n i t i a l i z e ( s e l f ) : nn . i n i t . k a i m i n g u n i f o r m ( s e l f . p a r a m e t e r s [ ’w1 ’ ] , a=math . s q r t ( 5 ) ) nn . i n i t . k a i m i n g u n i f o r m ( s e l f . p a r a m e t e r s [ ’w2 ’ ] , a=math . s q r t ( 5 ) ) s e l f . o p t i m i z e r . i n i t i a l i z e ( )  f o r w a r d ( s e l f , x ) : ””” Compute a p r e d i c t i o n . ””” x = F . l i n e a r ( x , x = t o r c h . t a n h ( x ) x = F . l i n e a r ( x , x = t o r c h . t a n h ( x ) x = F . l o g s o f t m a x ( x , dim = 1) r e t u r n x  s e l f . p a r a m e t e r s [ ’w1 ’ ] ,  s e l f . p a r a m e t e r s [ ’w2 ’ ] ,  s e l f . p a r a m e t e r s [ ’ b1 ’ ] )  s e l f . p a r a m e t e r s [ ’ b2 ’ ] )  d e f  d e f  d e f  a d j u s t ( s e l f ) : s e l f . o p t i m i z e r . a d j u s t ( s e l f . p a r a m e t e r s )  d e f  s t r  ( s e l f ) :  r e t u r n ’ m n i s t  /  ’ + s t r ( s e l f . o p t i m i z e r )  c l a s s NoOpOptimizer ( O p t i m i z a b l e ) :  ’ ’ ’ N o Op O p t im i z er ’ ’ ’ d e f  i n i t  ( s e l f ) :  s i t s on t o p o f a s t a c k , and d o e s n o t a f f e c t what  l i e s b e l o w .  p a s s  d e f  i n i t i a l i z e ( s e l f ) : p a s s  d e f b e g i n ( s e l f ) :  p a s s  d e f z e r o g r a d ( s e l f ) :  p a s s  d e f  a d j u s t ( s e l f , p a r a m s ) : p a s s  d e f  s t r  ( s e l f ) :  r e t u r n ’ s t a t i c ’  c l a s s SGD( O p t i m i z a b l e ) :  ’ ’ ’ A h y p e r o p t i m i z a b l e SGD ’ ’ ’ d e f  i n i t  ( s e l f , p a r a m e t e r s = { ’ a l p h a ’ : s u p e r ( ) .  i n i t  a l p h a = 0 . 0 1 , o p t i m i z e r = NoOpOptimizer ( ) ) :  t o r c h . t e n s o r ( a l p h a ) }  ( p a r a m e t e r s , o p t i m i z e r )  d e f  a d j u s t ( s e l f , p a r a m s ) : s e l f . o p t i m i z e r . a d j u s t ( s e l f . p a r a m e t e r s ) f o r name , param i n p a r a m s . i t e m s ( ) :  Gradient Descent: The Ultimate Optimizer  g = param . g r a d . d e t a c h ( ) p a r a m s [ name ] = param . d e t a c h ( ) − g ∗ s e l f . p a r a m e t e r s [ ’ a l p h a ’ ]  d e f  s t r  ( s e l f ) :  r e t u r n ’ s g d (% f )  /  ’%s e l f . p a r a m e t e r s [ ’ a l p h a ’ ] + s t r ( s e l f . o p t i m i z e r )  c l a s s SGDPerParam ( O p t i m i z a b l e ) :  ’ ’ ’ L i k e above , b u t can be t a u g h t a s e p a r a t e t u n e s . ’ ’ ’ d e f  ( s e l f ,  i n i t  p a r a m e t e r s = {name + ’ a l p h a ’ : s u p e r ( ) .  i n i t  ( p a r a m e t e r s , o p t i m i z e r )  a l p h a = 0 . 0 1 , p a r a m s = [ ] , o p t i m i z e r = NoOpOptimizer ( ) ) : t o r c h . t e n s o r ( a l p h a )  f o r name i n p a r a m s }  s t e p s i z e  f o r e a c h p a r a m e t e r  i t  d e f  a d j u s t ( s e l f , p a r a m s ) : s e l f . o p t i m i z e r . a d j u s t ( s e l f . p a r a m e t e r s ) f o r name , param i n p a r a m s . i t e m s ( ) :  g = param . g r a d . d e t a c h ( ) p a r a m s [ name ] = param . d e t a c h ( ) − g ∗ s e l f . p a r a m e t e r s [ name+ ’ a l p h a ’ ]  d e f  s t r  ( s e l f ) :  r e t u r n ’ s g d (% s )  /  ’ %\\  s t r ( { k : t . i t e m ( ) s t r ( s e l f . o p t i m i z e r )  f o r k , t  i n s e l f . p a r a m e t e r s . i t e m s ( ) } ) +\\  c l a s s Adam ( O p t i m i z a b l e ) :  ’ ’ ’ A f u l l y h y p e r o p t i m i z a b l e Adam o p t i m i z e r ’ ’ ’ d e f clamp ( x ) :  r e t u r n ( x . t a n h ( ) + 1 . )  / 2 .  d e f unclamp ( y ) :  z = y ∗ 2 . − 1 . r e t u r n ( ( 1 . + z )  d e f  i n i t  (  /  ( 1 . − z ) ) . l o g ( )  / 2 .  s e l f , a l p h a = 0 . 0 0 1 , b e t a 1 = 0 . 9 , b e t a 2 = 0 . 9 9 9 , o p t i m i z e r = NoOpOptimizer ( )  l o g e p s = −8. ,  ) :  p a r a m e t e r s = {  t o r c h . t e n s o r ( a l p h a ) ,  ’ a l p h a ’ : ’ b e t a 1 ’ : Adam . unclamp ( t o r c h . t e n s o r ( b e t a 1 ) ) , ’ b e t a 2 ’ : Adam . unclamp ( t o r c h . t e n s o r ( b e t a 2 ) ) , t o r c h . t e n s o r ( l o g e p s ) ’ l o g e p s ’ :  } s u p e r ( ) . i n i t s e l f . n u m a d j u s t m e n t s = 0 s e l f . c a c h e = {}  ( p a r a m e t e r s , o p t i m i z e r )  d e f  a d j u s t ( s e l f , p a r a m s ) : s e l f . n u m a d j u s t m e n t s += 1 s e l f . o p t i m i z e r . a d j u s t ( s e l f . p a r a m e t e r s ) t = s e l f . n u m a d j u s t m e n t s b e t a 1 = Adam . clamp ( s e l f . p a r a m e t e r s [ ’ b e t a 1 ’ ] ) b e t a 2 = Adam . clamp ( s e l f . p a r a m e t e r s [ ’ b e t a 2 ’ ] ) f o r name , param i n p a r a m s . i t e m s ( ) :  i f name n o t  i n s e l f . c a c h e :  s e l f . c a c h e [ name ] = {  ’m’ : ’ v ’ :  t o r c h . z e r o s ( param . s h a p e ) , t o r c h . z e r o s ( param . s h a p e ) +\\  # NOTE t h a t we add a l i t t l e  1 0 . ∗ ∗ s e l f . p a r a m e t e r s [ ’ l o g e p s ’ ] . d a t a ‘ f u d g e  f a c t o r ’ h e r e b e c a u s e  s q r t  i s n o t  Gradient Descent: The Ultimate Optimizer  # d i f f e r e n t i a b l e a t  e x a c t l y  z e r o  }  g = param . g r a d . d e t a c h ( ) s e l f . c a c h e [ name ] [ ’m’ ] = m =\\  b e t a 1 ∗ s e l f . c a c h e [ name ] [ ’m’ ] . d e t a c h ( ) + ( 1 . − b e t a 1 ) ∗ g  s e l f . c a c h e [ name ] [ ’ v ’ ] = v =\\  b e t a 2 ∗ s e l f . c a c h e [ name ] [ ’ v ’ ] . d e t a c h ( ) + ( 1 . − b e t a 2 ) ∗ g ∗ g  s e l f . a l l p a r a m s w i t h g r a d i e n t s . a p p e n d (m) s e l f . a l l p a r a m s w i t h g r a d i e n t s . a p p e n d ( v )  m h a t = m / v h a t = v /  ( 1 . − b e t a 1 ∗∗ f l o a t ( t ) ) ( 1 . − b e t a 2 ∗∗ f l o a t ( t ) )  dparam = m h a t p a r a m s [ name ] = param . d e t a c h ( ) − s e l f . p a r a m e t e r s [ ’ a l p h a ’ ] ∗ dparam  ( v h a t ∗∗ 0 . 5 + 1 0 . ∗∗ s e l f . p a r a m e t e r s [ ’ l o g e p s ’ ] )  /  d e f  s t r  ( s e l f ) :  r e t u r n ’ adam ( ’ + s t r ( s e l f . p a r a m e t e r s ) + ’ )  /  ’ + s t r ( s e l f . o p t i m i z e r )  c l a s s AdamBaydin ( O p t i m i z a b l e ) :  ’ ’ ’ Same a s above , b u t o n l y o p t i m i z e s r e m a i n i n g h y p e r p a r a m e t e r s a s c o n s t a n t s .  ’ ’ ’  t h e  l e a r n i n g r a t e ,  t r e a t i n g t h e  d e f  i n i t  (  s e l f , a l p h a = 0 . 0 0 1 , b e t a 1 = 0 . 9 , b e t a 2 = 0 . 9 9 9 , o p t i m i z e r = NoOpOptimizer ( )  l o g e p s = −8. ,  ) :  p a r a m e t e r s = {  ’ a l p h a ’ :  t o r c h . t e n s o r ( a l p h a ) ,  } s e l f . b e t a 1 = b e t a 1 s e l f . b e t a 2 = b e t a 2 s e l f . l o g e p s = l o g e p s s u p e r ( ) . i n i t s e l f . n u m a d j u s t m e n t s = 0 s e l f . c a c h e = {}  ( p a r a m e t e r s , o p t i m i z e r )  d e f  a d j u s t ( s e l f , p a r a m s ) : s e l f . n u m a d j u s t m e n t s += 1 s e l f . o p t i m i z e r . a d j u s t ( s e l f . p a r a m e t e r s ) t = s e l f . n u m a d j u s t m e n t s b e t a 1 = s e l f . b e t a 1 b e t a 2 = s e l f . b e t a 2 f o r name , param i n p a r a m s . i t e m s ( ) :  i f name n o t  i n s e l f . c a c h e :  s e l f . c a c h e [ name ] = {  ’m’ : ’ v ’ :  t o r c h . z e r o s ( param . s h a p e ) , t o r c h . z e r o s ( param . s h a p e ) + 1 0 . ∗ ∗ s e l f . l o g e p s  }  g = param . g r a d . d e t a c h ( ) s e l f . c a c h e [ name ] [ ’m’ ] = m =\\  b e t a 1 ∗ s e l f . c a c h e [ name ] [ ’m’ ] . d e t a c h ( ) + ( 1 . − b e t a 1 ) ∗ g  s e l f . c a c h e [ name ] [ ’ v ’ ] = v =\\  b e t a 2 ∗ s e l f . c a c h e [ name ] [ ’ v ’ ] . d e t a c h ( ) + ( 1 . − b e t a 2 ) ∗ g ∗ g  s e l f . a l l p a r a m s w i t h g r a d i e n t s . a p p e n d (m) s e l f . a l l p a r a m s w i t h g r a d i e n t s . a p p e n d ( v )  m h a t = m / v h a t = v /  ( 1 . − b e t a 1 ∗∗ f l o a t ( t ) ) ( 1 . − b e t a 2 ∗∗ f l o a t ( t ) )  Gradient Descent: The Ultimate Optimizer  dparam = m h a t p a r a m s [ name ] = param . d e t a c h ( ) − s e l f . p a r a m e t e r s [ ’ a l p h a ’ ] ∗ dparam  ( v h a t ∗∗ 0 . 5 + 1 0 . ∗∗ s e l f . l o g e p s )  /  d e f  s t r  ( s e l f ) :  r e t u r n ’ adam ( ’ + s t r ( s e l f . p a r a m e t e r s ) + ’ )  /  ’ + s t r ( s e l f . o p t i m i z e r )  # main . py import numpy a s np import j s o n , math , from h y p e r o p t import gc  t i m e  import ∗  BATCH SIZE = 300  m n i s t  t r a i n = t o r c h v i s i o n . d a t a s e t s . MNIST (  ’ . / d a t a ’ , t r a i n = True , download = True , t r a n s f o r m = t o r c h v i s i o n . t r a n s f o r m s . T o T e n s o r ( )  )  m n i s t  t e s t = t o r c h v i s i o n . d a t a s e t s . MNIST (  ’ . / d a t a ’ , t r a i n = F a l s e , download = True , t r a n s f o r m = t o r c h v i s i o n . t r a n s f o r m s . T o T e n s o r ( )  )  d l  t r a i n = t o r c h . u t i l s . d a t a . D a t a L o a d e r (  t r a i n ,  m n i s t b a t c h s i z e =BATCH SIZE , s h u f f l e = F a l s e  )  d l  t e s t = t o r c h . u t i l s . d a t a . D a t a L o a d e r (  m n i s t t e s t , b a t c h s i z e =10000 , s h u f f l e = F a l s e  )  d e f  t e s t ( model ) : f o r i ,  ( f e a t u r e s  ,  l a b e l s  )  i n enumerate ( d l  l a b e l s = t o r c h . r e s h a p e ( f e a t u r e s  f e a t u r e s , p r e d = model . f o r w a r d ( f e a t u r e s ) r e t u r n p r e d . argmax ( dim = 1 ) . eq ( l a b e l s ) . sum ( ) . i t e m ( )  / 10000 ∗ 100  t e s t ) : ,  ( 1 0 0 0 0 , 28 ∗ 2 8 ) ) ,  l a b e l s  d e f  t r a i n ( model , s t a t s = [ ] f o r e p o c h i n range ( e p o c h s ) :  e p o c h s =3 , h e i g h t = 1 ) :  f o r i ,  ( f e a t u r e s  ,  l a b e l s  )  i n enumerate ( d l  t r a i n ) :  t 0 = t i m e . p r o c e s s t i m e ( ) model . b e g i n ( ) f e a t u r e s ,  l a b e l s =\\ t o r c h . r e s h a p e ( f e a t u r e s p r e d = model . f o r w a r d ( f e a t u r e s ) l o s s = F . n l l model . z e r o g r a d ( ) l o s s . b a c k w a r d ( c r e a t e g r a p h = T r u e ) model . a d j u s t ( ) t f = t i m e . p r o c e s s t i m e ( ) d a t a = {  l o s s ( p r e d ,  l a b e l s )  ,  ( BATCH SIZE , 28 ∗ 2 8 ) ) ,  l a b e l s  t f − t 0 ,  ’ t i m e ’ : ’ i t e r ’ : e p o c h ∗ l e n ( d l ’ l o s s ’ : ’ p a r a m s ’ : {  l o s s . i t e m ( ) ,  t r a i n ) + i ,  Gradient Descent: The Ultimate Optimizer  k : v . i t e m ( ) i f  ’ . ’ n o t  }  } s t a t s . a p p e n d ( d a t a )  r e t u r n s t a t s  f o r k , v i n model . o p t i m i z e r . p a r a m e t e r s . i t e m s ( )  i n k  d e f  r u n ( o p t , name= ’ o u t ’ , u s r ={} , e p o c h s =3 , h e i g h t = 1 ) : t o r c h . m a n u a l s e e d ( 0 x42 ) model = M N I S T F u l l y C o n n e c t e d ( 28 ∗ 2 8 , 1 2 8 , 1 0 , o p t  s t r ( model ) )  ) p r i n t ( ’ Running . . . ’ , model . i n i t i a l i z e ( ) l o g = t r a i n ( model , e p o c h s , h e i g h t ) a c c = t e s t ( model ) o u t = { ’ a c c ’ : acc , w i t h open ( ’ l o g /% s . j s o n ’ % name ,  ’ l o g ’ :  l o g ,  ’ u s r ’ : u s r } f :  ’w ’ ) a s  i n d e n t = T r u e )  j s o n . dump ( o u t , t i m e s = [ x [ ’ t i m e ’ ] p r i n t ( ’ Times ( ms ) : p r i n t ( ’ F i n a l a c c u r a c y : ’ , a c c ) r e t u r n o u t  f , f o r x i n l o g ] ’ , np . mean ( t i m e s ) ,  ’ +/− ’ , np . s t d ( t i m e s ) )  d e f  ’ s g d ’ , e p o c h s = 1 )  s g d e x p e r i m e n t s ( ) : r u n (SGD ( 0 . 0 1 ) , o u t = r u n (SGD ( 0 . 0 1 , o p t i m i z e r =SGD ( 0 . 0 1 ) ) , a l p h a = o u t [ ’ l o g ’ ] [ − 1 ] [ ’ p a r a m s ’ ] [ ’ a l p h a ’ ] p r i n t ( a l p h a ) r u n (SGD( a l p h a ) ,  ’ sgd−f i n a l ’ , e p o c h s = 1 )  ’ s g d + s g d ’ , e p o c h s =1 )  d e f a d a m e x p e r i m e n t s ( ) :  r u n ( Adam ( ) ,  ’ adam ’ , e p o c h s = 1 )  p r i n t ( ) mo = SGDPerParam (  0 . 0 0 1 ,  [ ’ a l p h a ’ ,  ’ b e t a 1 ’ ,  ’ b e t a 2 ’ ,  ’ l o g e p s ’ ] , o p t i m i z e r =SGD ( 0 . 0 0 0 1 )  ’ adam+ s g d ’ , e p o c h s =1 )  ) o u t = r u n ( Adam ( o p t i m i z e r =mo ) , p = o u t [ ’ l o g ’ ] [ − 1 ] [ ’ p a r a m s ’ ] a l p h a = p [ ’ a l p h a ’ ] b e t a 1 = Adam . clamp ( t o r c h . t e n s o r ( p [ ’ b e t a 1 ’ ] b e t a 2 = Adam . clamp ( t o r c h . t e n s o r ( p [ ’ b e t a 2 ’ ] l o g e p s = p [ ’ l o g e p s ’ ] p r i n t ( a l p h a , b e t a 1 , b e t a 2 , p r i n t ( mo ) r u n (  l o g e p s )  ) ) . i t e m ( ) ) ) . i t e m ( )  Adam ( a l p h a =p [ ’ a l p h a ’ ] , b e t a 1 = b e t a 1 , b e t a 2 = b e t a 2 , ’ adam+ sgd−f i n a l ’ , e p o c h s =1  l o g e p s = l o g e p s ) ,  )  p r i n t ( ) o u t = r u n ( Adam ( o p t i m i z e r =Adam ( ) ) , p = o u t [ ’ l o g ’ ] [ − 1 ] [ ’ p a r a m s ’ ] a l p h a = p [ ’ a l p h a ’ ] b e t a 1 = Adam . clamp ( t o r c h . t e n s o r ( p [ ’ b e t a 1 ’ ] b e t a 2 = Adam . clamp ( t o r c h . t e n s o r ( p [ ’ b e t a 2 ’ ] l o g e p s = p [ ’ l o g e p s ’ ] p r i n t ( a l p h a , b e t a 1 , b e t a 2 , r u n (  l o g e p s )  ’ adam2 ’ , e p o c h s =1 )  ) ) . i t e m ( ) ) ) . i t e m ( )  Adam ( a l p h a =p [ ’ a l p h a ’ ] , b e t a 1 = b e t a 1 , b e t a 2 = b e t a 2 , ’ adam2−f i n a l ’ , e p o c h s =1  l o g e p s = l o g e p s ) ,  )  Gradient Descent: The Ultimate Optimizer  [ ’ a l p h a ’ ] , o p t i m i z e r =SGD ( 0 . 0 0 0 1 ) )  ’ a d a m b a y d i n + s g d ’ , e p o c h s =1 )  ’ a d a m b a y d i n + sgd−f i n a l ’ , e p o c h s = 1 )  ’ a d a m b a y d i n 2 ’ , e p o c h s =1 )  p r i n t ( ) mo = SGDPerParam ( 0 . 0 0 1 , o u t = r u n ( AdamBaydin ( o p t i m i z e r =mo ) , p = o u t [ ’ l o g ’ ] [ − 1 ] [ ’ p a r a m s ’ ] a l p h a = p [ ’ a l p h a ’ ] p r i n t ( a l p h a ) p r i n t ( mo ) r u n ( Adam ( a l p h a =p [ ’ a l p h a ’ ] ) ,  p r i n t ( ) o u t = r u n ( AdamBaydin ( o p t i m i z e r =Adam ( ) ) , p = o u t [ ’ l o g ’ ] [ − 1 ] [ ’ p a r a m s ’ ] a l p h a = p [ ’ a l p h a ’ ] p r i n t ( a l p h a ) r u n ( Adam ( a l p h a =p [ ’ a l p h a ’ ] ) ,  ’ adambaydin2−f i n a l ’ , e p o c h s = 1 )  d e f  s u r f a c e ( ) : r u n (SGD( 1 0 ∗∗ −3, o p t i m i z e r =SGD( 1 0 ∗ ∗ − 1 ) ) , f o r  l o g a l p h a i n np . l i n s p a c e ( −3 , 2 , 1 0 ) : r u n (  ’ t s t ’ , e p o c h s =1 )  SGD( 1 0 ∗∗ l o g a l p h a ) , ’ sgd@1e%+.2 f ’ % l o g a l p h a , e p o c h s =1  )  d e f m a k e s g d s t a c k ( h e i g h t ,  t o p ) :  i f h e i g h t == 0 :  r e t u r n SGD( a l p h a = t o p )  r e t u r n SGD( a l p h a = t o p , o p t i m i z e r = m a k e s g d s t a c k ( h e i g h t − 1 ,  t o p ) )  d e f m a k e a d a m s t a c k ( h e i g h t ,  t o p = 0 . 0 0 0 0 0 0 1 ) :  i f h e i g h t == 0 :  r e t u r n Adam ( a l p h a = t o p )  r e t u r n Adam ( a l p h a = t o p , o p t i m i z e r = m a k e a d a m s t a c k ( h e i g h t − 1 ) )  d e f  s t a c k t e s t ( ) : f o r t o p i n np . l i n s p a c e ( −7 , 3 , 2 0 ) :  f o r h e i g h t  i n range ( 6 ) : p r i n t ( ’ h e i g h t = ’ , h e i g h t , o p t = m a k e s g d s t a c k ( h e i g h t , 10∗∗ t o p ) r u n (  ’ t o p = ’ ,  t o p )  ’ metasgd3−%d@%+.2 f ’ % ( h e i g h t ,  t o p ) ,  o p t , { ’ h e i g h t ’ : h e i g h t ,  ’ t o p ’ :  t o p } , e p o c h s =1 , h e i g h t = h e i g h t  ) gc . c o l l e c t ( )  d e f p e r f  t e s t ( ) :  f o r h i n range ( 5 1 ) :  p r i n t ( ’ h e i g h t : ’ , h ) # o p t = m a k e s g d s t a c k ( h , 0 . 0 1 ) o p t = m a k e a d a m s t a c k ( h ) r u n ( o p t , gc . c o l l e c t ( )  ’ a d a m p e r f−%d ’ % h , { ’ h e i g h t ’ : h } , e p o c h s = 1)  i f  ’ :  == ’ m a i n  n a m e s u r f a c e ( ) s g d e x p e r i m e n t s ( ) a d a m e x p e r i m e n t s ( ) s t a c k t e s t ( ) p e r f  t e s t ( ) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 88953, but `max_length` is set to 10000. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_2742/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1258299458.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 2&gt;</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_2742/1258299458.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_2742/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1331552400.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">display_answer</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_2742/1331552400.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/autodl-tmp/langchain-ChatGLM/chains/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">local_doc_qa.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">201</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_knowledge_based_answer</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 │   │   </span>prompt = generate_prompt(related_docs, query)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> streaming:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>201 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> result, history <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.llm._call(prompt=prompt,                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   │   │   │   │   │   │   │   │   │   │   │     </span>history=chat_history):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 │   │   │   │   </span>history[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>] = query                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">204 │   │   │   │   </span>response = {<span style=\"color: #808000; text-decoration-color: #808000\">\"query\"</span>: query,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/autodl-tmp/langchain-ChatGLM/models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">chatglm_llm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">78</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call</span>                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 75 │   │   │     </span>stop: Optional[List[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>]] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>:                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 76 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(prompt)                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 77 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.streaming:                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 78 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> inum, (stream_resp, _) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model.stream_chat(                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 79 │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.tokenizer,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 80 │   │   │   │   │   </span>prompt,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 81 │   │   │   │   │   </span>history=history[-<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.history_len:-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.history_len &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> [   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/miniconda3/lib/python3.8/site-packages/torch/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_contextlib.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">35</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generator_context</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 32 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 33 │   │   │   # Issuing `None` to a generator fires it up</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 34 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ctx_factory():                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 35 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>response = gen.send(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 36 │   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 37 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 38 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1311</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">stream_chat</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1308 │   │   │   </span>prompt += <span style=\"color: #808000; text-decoration-color: #808000\">\"[Round {}]\\n问：{}\\n答：\"</span>.format(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(history), query)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1309 │   │   </span>inputs = tokenizer([prompt], return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>)                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1310 │   │   </span>inputs = inputs.to(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1311 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> outputs <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.stream_generate(**inputs, **gen_kwargs):                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1312 │   │   │   </span>outputs = outputs.tolist()[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>][<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(inputs[<span style=\"color: #808000; text-decoration-color: #808000\">\"input_ids\"</span>][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]):]                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1313 │   │   │   </span>response = tokenizer.decode(outputs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1314 │   │   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.process_response(response)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/miniconda3/lib/python3.8/site-packages/torch/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_contextlib.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">35</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generator_context</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 32 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 33 │   │   │   # Issuing `None` to a generator fires it up</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 34 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ctx_factory():                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 35 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>response = gen.send(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>)                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 36 │   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 37 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 38 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1386</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">stream_generate</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1383 │   │   </span>unfinished_sequences = input_ids.new(input_ids.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]).fill_(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1384 │   │   </span>scores = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1385 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1386 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>model_inputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prepare_inputs_for_generation(input_ids, **model_kwargs)  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1387 │   │   │   # forward pass to get next token</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1388 │   │   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>(                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1389 │   │   │   │   </span>**model_inputs,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1155</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">prepare_inputs_for_generation</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1152 │   │   │   │   </span>logger.warning_once(<span style=\"color: #808000; text-decoration-color: #808000\">f\"The dtype of attention mask ({</span>attention_mask.dtype  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1153 │   │   │   │   </span>attention_mask = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1154 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> attention_mask <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1155 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>attention_mask = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.get_masks(                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1156 │   │   │   │   │   </span>input_ids,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1157 │   │   │   │   │   </span>device=input_ids.device                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1158 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">683</span> in      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_masks</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 680 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_masks</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, input_ids, device):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 681 │   │   </span>batch_size, seq_length = input_ids.shape                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 682 │   │   </span>context_lengths = [seq.tolist().index(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.bos_token_id) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> seq <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> input  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 683 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attention_mask = torch.ones((batch_size, seq_length, seq_length), device=device)  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 684 │   │   </span>attention_mask.tril_()                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 685 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i, context_length <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(context_lengths):                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 686 │   │   │   </span>attention_mask[i, :, :context_length] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29.48</span> GiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.70</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.72</span> GiB \n",
       "already allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.80</span> GiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.74</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated memory\n",
       "try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_2742/\u001b[0m\u001b[1;33m1258299458.py\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<cell line: 2>\u001b[0m                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_2742/1258299458.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_2742/\u001b[0m\u001b[1;33m1331552400.py\u001b[0m:\u001b[94m5\u001b[0m in \u001b[92mdisplay_answer\u001b[0m                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_2742/1331552400.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/autodl-tmp/langchain-ChatGLM/chains/\u001b[0m\u001b[1;33mlocal_doc_qa.py\u001b[0m:\u001b[94m201\u001b[0m in \u001b[92mget_knowledge_based_answer\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   │   \u001b[0mprompt = generate_prompt(related_docs, query)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m streaming:                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m201 \u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m result, history \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.llm._call(prompt=prompt,                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │   │   │   │     \u001b[0mhistory=chat_history):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mhistory[-\u001b[94m1\u001b[0m][\u001b[94m0\u001b[0m] = query                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m204 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mresponse = {\u001b[33m\"\u001b[0m\u001b[33mquery\u001b[0m\u001b[33m\"\u001b[0m: query,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/autodl-tmp/langchain-ChatGLM/models/\u001b[0m\u001b[1;33mchatglm_llm.py\u001b[0m:\u001b[94m78\u001b[0m in \u001b[92m_call\u001b[0m                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 75 \u001b[0m\u001b[2m│   │   │     \u001b[0mstop: Optional[List[\u001b[96mstr\u001b[0m]] = \u001b[94mNone\u001b[0m) -> \u001b[96mstr\u001b[0m:                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 76 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(prompt)                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 77 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.streaming:                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 78 \u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m inum, (stream_resp, _) \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(\u001b[96mself\u001b[0m.model.stream_chat(                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 79 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.tokenizer,                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 80 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mprompt,                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 81 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhistory=history[-\u001b[96mself\u001b[0m.history_len:-\u001b[94m1\u001b[0m] \u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.history_len > \u001b[94m0\u001b[0m \u001b[94melse\u001b[0m [   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/lib/python3.8/site-packages/torch/utils/\u001b[0m\u001b[1;33m_contextlib.py\u001b[0m:\u001b[94m35\u001b[0m in \u001b[92mgenerator_context\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 32 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 33 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Issuing `None` to a generator fires it up\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 34 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m ctx_factory():                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 35 \u001b[2m│   │   │   │   \u001b[0mresponse = gen.send(\u001b[94mNone\u001b[0m)                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 36 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 37 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 38 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m1311\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mstream_chat\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1308 \u001b[0m\u001b[2m│   │   │   \u001b[0mprompt += \u001b[33m\"\u001b[0m\u001b[33m[Round \u001b[0m\u001b[33m{}\u001b[0m\u001b[33m]\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m问：\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m答：\u001b[0m\u001b[33m\"\u001b[0m.format(\u001b[96mlen\u001b[0m(history), query)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1309 \u001b[0m\u001b[2m│   │   \u001b[0minputs = tokenizer([prompt], return_tensors=\u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m)                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1310 \u001b[0m\u001b[2m│   │   \u001b[0minputs = inputs.to(\u001b[96mself\u001b[0m.device)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1311 \u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m outputs \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.stream_generate(**inputs, **gen_kwargs):                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1312 \u001b[0m\u001b[2m│   │   │   \u001b[0moutputs = outputs.tolist()[\u001b[94m0\u001b[0m][\u001b[96mlen\u001b[0m(inputs[\u001b[33m\"\u001b[0m\u001b[33minput_ids\u001b[0m\u001b[33m\"\u001b[0m][\u001b[94m0\u001b[0m]):]                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1313 \u001b[0m\u001b[2m│   │   │   \u001b[0mresponse = tokenizer.decode(outputs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1314 \u001b[0m\u001b[2m│   │   │   \u001b[0mresponse = \u001b[96mself\u001b[0m.process_response(response)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/miniconda3/lib/python3.8/site-packages/torch/utils/\u001b[0m\u001b[1;33m_contextlib.py\u001b[0m:\u001b[94m35\u001b[0m in \u001b[92mgenerator_context\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 32 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 33 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Issuing `None` to a generator fires it up\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 34 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m ctx_factory():                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 35 \u001b[2m│   │   │   │   \u001b[0mresponse = gen.send(\u001b[94mNone\u001b[0m)                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 36 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 37 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 38 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m1386\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mstream_generate\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1383 \u001b[0m\u001b[2m│   │   \u001b[0munfinished_sequences = input_ids.new(input_ids.shape[\u001b[94m0\u001b[0m]).fill_(\u001b[94m1\u001b[0m)                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1384 \u001b[0m\u001b[2m│   │   \u001b[0mscores = \u001b[94mNone\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1385 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1386 \u001b[2m│   │   │   \u001b[0mmodel_inputs = \u001b[96mself\u001b[0m.prepare_inputs_for_generation(input_ids, **model_kwargs)  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1387 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# forward pass to get next token\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1388 \u001b[0m\u001b[2m│   │   │   \u001b[0moutputs = \u001b[96mself\u001b[0m(                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1389 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m**model_inputs,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m1155\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mprepare_inputs_for_generation\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1152 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogger.warning_once(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mThe dtype of attention mask (\u001b[0m\u001b[33m{\u001b[0mattention_mask.dtype  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1153 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mattention_mask = \u001b[94mNone\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1154 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m attention_mask \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1155 \u001b[2m│   │   │   │   \u001b[0mattention_mask = \u001b[96mself\u001b[0m.get_masks(                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1156 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0minput_ids,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1157 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mdevice=input_ids.device                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1158 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/chatglm-6b/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m683\u001b[0m in      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mget_masks\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 680 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mget_masks\u001b[0m(\u001b[96mself\u001b[0m, input_ids, device):                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 681 \u001b[0m\u001b[2m│   │   \u001b[0mbatch_size, seq_length = input_ids.shape                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 682 \u001b[0m\u001b[2m│   │   \u001b[0mcontext_lengths = [seq.tolist().index(\u001b[96mself\u001b[0m.config.bos_token_id) \u001b[94mfor\u001b[0m seq \u001b[95min\u001b[0m input  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 683 \u001b[2m│   │   \u001b[0mattention_mask = torch.ones((batch_size, seq_length, seq_length), device=device)  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 684 \u001b[0m\u001b[2m│   │   \u001b[0mattention_mask.tril_()                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 685 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m i, context_length \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(context_lengths):                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 686 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask[i, :, :context_length] = \u001b[94m1\u001b[0m                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m29.48\u001b[0m GiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m23.70\u001b[0m GiB total capacity; \u001b[1;36m12.72\u001b[0m GiB \n",
       "already allocated; \u001b[1;36m9.80\u001b[0m GiB free; \u001b[1;36m12.74\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory\n",
       "try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 多轮对话\n",
    "resp, history = display_answer(local_doc_qa, query=\"它与ResNet的关键区别是什么？\", vs_path=vs_path, history=history)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_2742/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">914167168.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_2742/914167168.py'</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'resp'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_2742/\u001b[0m\u001b[1;33m914167168.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_2742/914167168.py'\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'resp'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resp[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "B53529438F034BE783FA1D2B71E70DBD",
    "jupyter": {
     "outputs_hidden": false
    },
    "notebookId": "644b7c4749a32aea7f079534",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
